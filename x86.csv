Instruction,Opcode,Valid 64-bit,Valid 32-bit,Valid 16-bit,Feature Flags,Operand 1,Operand 2,Operand 3,Operand 4,Tuple Type,Description
"AAA","37","Invalid","Valid","Valid","","NA","NA","NA","NA","","ASCII adjust AL after addition."
"AAD","D5 0A","Invalid","Valid","Valid","","NA","NA","NA","NA","","ASCII adjust AX before division."
"AAD imm8","D5 ib","Invalid","Valid","Valid","","NA","NA","NA","NA","","Adjust AX before division to number base imm8."
"AAM","D4 0A","Invalid","Valid","Valid","","NA","NA","NA","NA","","ASCII adjust AX after multiply."
"AAM imm8","D4 ib","Invalid","Valid","Valid","","NA","NA","NA","NA","","Adjust AX after multiply to number base imm8."
"AAS","3F","Invalid","Valid","Valid","","NA","NA","NA","NA","","ASCII adjust AL after subtraction."
"ADC AL, imm8","14 ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add with carry imm8 to AL."
"ADC AX, imm16","15 iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add with carry imm16 to AX."
"ADC EAX, imm32","15 id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add with carry imm32 to EAX."
"ADC RAX, imm32","REX.W + 15 id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add with carry imm32 sign extended to 64- bits to RAX."
"ADC r/m8, imm8","80 /2 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with carry imm8 to r/m8."
"ADC r/m8 , imm8","REX + 80 /2 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with carry imm8 to r/m8."
"ADC r/m16, imm16","81 /2 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with carry imm16 to r/m16."
"ADC r/m32, imm32","81 /2 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with CF imm32 to r/m32."
"ADC r/m64, imm32","REX.W + 81 /2 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with CF imm32 sign extended to 64-bits to r/m64."
"ADC r/m16, imm8","83 /2 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with CF sign-extended imm8 to r/m16."
"ADC r/m32, imm8","83 /2 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with CF sign-extended imm8 into r/m32."
"ADC r/m64, imm8","REX.W + 83 /2 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add with CF sign-extended imm8 into r/m64."
"ADC r/m8, r8","10 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add with carry byte register to r/m8."
"ADC r/m8 , r8","REX + 10 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add with carry byte register to r/m64."
"ADC r/m16, r16","11 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add with carry r16 to r/m16."
"ADC r/m32, r32","11 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add with CF r32 to r/m32."
"ADC r/m64, r64","REX.W + 11 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add with CF r64 to r/m64."
"ADC r8, r/m8","12 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add with carry r/m8 to byte register."
"ADC r8 , r/m8","REX + 12 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add with carry r/m64 to byte register."
"ADC r16, r/m16","13 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add with carry r/m16 to r16."
"ADC r32, r/m32","13 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add with CF r/m32 to r32."
"ADC r64, r/m64","REX.W + 13 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add with CF r/m64 to r64."
"ADCX r32, r/m32","66 0F 38 F6 /r","Valid","Valid","Invalid","ADX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Unsigned addition of r32 with CF, r/m32 to r32, writes CF."
"ADCX r64, r/m64","66 REX.w 0F 38 F6 /r","Valid","Invalid","Invalid","ADX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Unsigned addition of r64 with CF, r/m64 to r64, writes CF."
"ADD AL, imm8","04 ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add imm8 to AL."
"ADD AX, imm16","05 iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add imm16 to AX."
"ADD EAX, imm32","05 id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add imm32 to EAX."
"ADD RAX, imm32","REX.W + 05 id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8","NA","NA","","Add imm32 sign-extended to 64-bits to RAX."
"ADD r/m8, imm8","80 /0 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add imm8 to r/m8."
"ADD r/m8 , imm8","REX + 80 /0 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add sign-extended imm8 to r/m8."
"ADD r/m16, imm16","81 /0 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add imm16 to r/m16."
"ADD r/m32, imm32","81 /0 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add imm32 to r/m32."
"ADD r/m64, imm32","REX.W + 81 /0 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add imm32 sign-extended to 64-bits to r/m64."
"ADD r/m16, imm8","83 /0 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add sign-extended imm8 to r/m16."
"ADD r/m32, imm8","83 /0 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add sign-extended imm8 to r/m32."
"ADD r/m64, imm8","REX.W + 83 /0 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Add sign-extended imm8 to r/m64."
"ADD r/m8, r8","00 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add r8 to r/m8."
"ADD r/m8 , r8","REX + 00 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add r8 to r/m8."
"ADD r/m16, r16","01 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add r16 to r/m16."
"ADD r/m32, r32","01 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add r32 to r/m32."
"ADD r/m64, r64","REX.W + 01 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Add r64 to r/m64."
"ADD r8, r/m8","02 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add r/m8 to r8."
"ADD r8 , r/m8","REX + 02 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add r/m8 to r8."
"ADD r16, r/m16","03 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add r/m16 to r16."
"ADD r32, r/m32","03 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add r/m32 to r32."
"ADD r64, r/m64","REX.W + 03 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add r/m64 to r64."
"ADDPD xmm1, xmm2/m128","66 0F 58 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed double-precision floating-point values from xmm2/mem to xmm1 and store result in xmm1."
"VADDPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add packed double-precision floating-point values from xmm3/mem to xmm2 and store result in xmm1."
"VADDPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add packed double-precision floating-point values from ymm3/mem to ymm2 and store result in ymm1."
"VADDPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F.W1 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"ADDPS xmm1, xmm2/m128","NP 0F 58 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed single-precision floating-point values from xmm2/m128 to xmm1 and store result in xmm1."
"VADDPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add packed single-precision floating-point values from xmm3/m128 to xmm2 and store result in xmm1."
"VADDPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add packed single-precision floating-point values from ymm3/m256 to ymm2 and store result in ymm1."
"VADDPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1 with writemask k1."
"VADDPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 58 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1 with writemask k1."
"VADDPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","EVEX.NDS.512.0F.W0 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Add packed single-precision floating-point values from zmm3/m512/m32bcst to zmm2 and store result in zmm1 with writemask k1."
"ADDSD xmm1, xmm2/m64","F2 0F 58 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add the low double-precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1."
"VADDSD xmm1, xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add the low double-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.F2.0F.W1 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Add the low double-precision floating-point value from xmm3/m64 to xmm2 and store the result in xmm1 with writemask k1."
"ADDSS xmm1, xmm2/m32","F3 0F 58 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add the low single-precision floating-point value from xmm2/mem to xmm1 and store the result in xmm1."
"VADDSS xmm1,xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 58 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Add the low single-precision floating-point value from xmm3/mem to xmm2 and store the result in xmm1."
"VADDSS xmm1{k1}{z}, xmm2, xmm3/m32{er}","EVEX.NDS.LIG.F3.0F.W0 58 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Add the low single-precision floating-point value from xmm3/m32 to xmm2 and store the result in xmm1with writemask k1."
"ADDSUBPD xmm1, xmm2/m128","66 0F D0 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add/subtract double-precision floating-point values from xmm2/m128 to xmm1."
"VADDSUBPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add/subtract packed double-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add / subtract packed double-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"ADDSUBPS xmm1, xmm2/m128","F2 0F D0 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add/subtract single-precision floating-point values from xmm2/m128 to xmm1."
"VADDSUBPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.F2.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add/subtract single-precision floating-point values from xmm3/mem to xmm2 and stores result in xmm1."
"VADDSUBPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.F2.0F.WIG D0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add / subtract single-precision floating-point values from ymm3/mem to ymm2 and stores result in ymm1."
"ADOX r32, r/m32","F3 0F 38 F6 /r","Valid","Valid","Invalid","ADX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Unsigned addition of r32 with OF, r/m32 to r32, writes OF."
"ADOX r64, r/m64","F3 REX.w 0F 38 F6 /r","Valid","Invalid","Invalid","ADX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Unsigned addition of r64 with OF, r/m64 to r64, writes OF."
"AESDEC xmm1, xmm2/m128","66 0F 38 DE /r","Valid","Valid","Invalid","AES","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128."
"VAESDEC xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG DE /r","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Perform one round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1."
"AESDECLAST xmm1, xmm2/m128","66 0F 38 DF /r","Valid","Valid","Invalid","AES","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128."
"VAESDECLAST xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG DF /r","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Perform the last round of an AES decryption flow, using the Equivalent Inverse Cipher, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from xmm3/m128; store the result in xmm1."
"AESENC xmm1, xmm2/m128","66 0F 38 DC /r","Valid","Valid","Invalid","AES","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128."
"VAESENC xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG DC /r","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Perform one round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128-bit round key from the xmm3/m128; store the result in xmm1."
"AESENCLAST xmm1, xmm2/m128","66 0F 38 DD /r","Valid","Valid","Invalid","AES","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm1 with a 128-bit round key from xmm2/m128."
"VAESENCLAST xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG DD /r","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Perform the last round of an AES encryption flow, operating on a 128-bit data (state) from xmm2 with a 128 bit round key from xmm3/m128; store the result in xmm1."
"AESIMC xmm1, xmm2/m128","66 0F 38 DB /r","Valid","Valid","Invalid","AES","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1."
"VAESIMC xmm1, xmm2/m128","VEX.128.66.0F38.WIG DB /r","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Perform the InvMixColumn transformation on a 128-bit round key from xmm2/m128 and store the result in xmm1."
"AESKEYGENASSIST xmm1, xmm2/m128, imm8","66 0F 3A DF /r ib","Valid","Valid","Invalid","AES","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Assist in AES round key generation using an 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1."
"VAESKEYGENASSIST xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.WIG DF /r ib","Valid","Valid","Invalid","AES AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Assist in AES round key generation using 8 bits Round Constant (RCON) specified in the immediate byte, operating on 128 bits of data specified in xmm2/m128 and stores the result in xmm1."
"AND AL, imm8","24 ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","AL AND imm8."
"AND AX, imm16","25 iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","AX AND imm16."
"AND EAX, imm32","25 id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8","NA","NA","","EAX AND imm32."
"AND RAX, imm32","REX.W + 25 id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8","NA","NA","","RAX AND imm32 sign-extended to 64-bits."
"AND r/m8, imm8","80 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m8 AND imm8."
"AND r/m8 , imm8","REX + 80 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m8 AND imm8."
"AND r/m16, imm16","81 /4 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m16 AND imm16."
"AND r/m32, imm32","81 /4 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m32 AND imm32."
"AND r/m64, imm32","REX.W + 81 /4 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m64 AND imm32 sign extended to 64-bits."
"AND r/m16, imm8","83 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m16 AND imm8 (sign-extended)."
"AND r/m32, imm8","83 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m32 AND imm8 (sign-extended)."
"AND r/m64, imm8","REX.W + 83 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","r/m64 AND imm8 (sign-extended)."
"AND r/m8, r8","20 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m8 AND r8."
"AND r/m8 , r8","REX + 20 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m64 AND r8 (sign-extended)."
"AND r/m16, r16","21 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m16 AND r16."
"AND r/m32, r32","21 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m32 AND r32."
"AND r/m64, r64","REX.W + 21 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m64 AND r32."
"AND r8, r/m8","22 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r8 AND r/m8."
"AND r8 , r/m8","REX + 22 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r/m64 AND r8 (sign-extended)."
"AND r16, r/m16","23 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r16 AND r/m16."
"AND r32, r/m32","23 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r32 AND r/m32."
"AND r64, r/m64","REX.W + 23 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r64 AND r/m64."
"ANDN r32a, r32b, r/m32","VEX.NDS.LZ.0F38.W0 F2 /r","Valid","Valid","Invalid","BMI1","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Bitwise AND of inverted r32b with r/m32, store result in r32a."
"ANDN r64a, r64b, r/m64","VEX.NDS.LZ.0F38.W1 F2 /r","Valid","Invalid","Invalid","BMI1","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Bitwise AND of inverted r64b with r/m64, store result in r64a."
"ANDNPD xmm1, xmm2/m128","66 0F 55 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm1 and xmm2/mem."
"VANDNPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VANDNPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F 55/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VANDNPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 55 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND NOT of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VANDNPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 55 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND NOT of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VANDNPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 55 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND NOT of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"ANDNPS xmm1, xmm2/m128","NP 0F 55 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm1 and xmm2/mem."
"VANDNPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.0F 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND NOT of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VANDNPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F 55 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND NOT of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VANDNPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 55 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VANDNPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 55 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VANDNPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 55 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"ANDPD xmm1, xmm2/m128","66 0F 54 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical AND of packed double-precision floating-point values in xmm1 and xmm2/mem."
"VANDPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VANDPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VANDPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 54 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VANDPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 54 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VANDPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 54 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"ANDPS xmm1, xmm2/m128","NP 0F 54 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical AND of packed single-precision floating-point values in xmm1 and xmm2/mem."
"VANDPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VANDPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F 54 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VANDPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 54 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VANDPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 54 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VANDPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 54 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical AND of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"ARPL r/m16, r16","63 /r","Invalid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Valid Adjust RPL of r/m16 to not less than RPL of r16."
"BEXTR r32a, r/m32, r32b","VEX.NDS.LZ.0F38.W0 F7 /r","Valid","Valid","Invalid","BMI1","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Contiguous bitwise extract from r/m32 using r32b as control; store result in r32a."
"BEXTR r64a, r/m64, r64b","VEX.NDS.LZ.0F38.W1 F7 /r","Valid","Invalid","Invalid","BMI1","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Contiguous bitwise extract from r/m64 using r64b as control; store result in r64a"
"BLENDPD xmm1, xmm2/m128, imm8","66 0F 3A 0D /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Select packed DP-FP values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"VBLENDPD xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 0D /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[3:0]","","Select packed double-precision floating-point Values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1."
"VBLENDPD ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 0D /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[3:0]","","Select packed double-precision floating-point Values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1."
"BLENDPS xmm1, xmm2/m128, imm8","66 0F 3A 0C /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"VBLENDPS xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 0C /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Select packed single-precision floating-point values from xmm2 and xmm3/m128 from mask in imm8 and store the values in xmm1."
"VBLENDPS ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 0C /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Select packed single-precision floating-point values from ymm2 and ymm3/m256 from mask in imm8 and store the values in ymm1."
"BLENDVPD xmm1, xmm2/m128 , <XMM0>","66 0F 38 15 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","implicit XMM0","NA","","Select packed DP FP values from xmm1 and xmm2 from mask specified in XMM0 and store the values in xmm1."
"VBLENDVPD xmm1, xmm2, xmm3/m128, xmm4","VEX.NDS.128.66.0F3A.W0 4B /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Conditionally copy double-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the mask operand, xmm4."
"VBLENDVPD ymm1, ymm2, ymm3/m256, ymm4","VEX.NDS.256.66.0F3A.W0 4B /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Conditionally copy double-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the mask operand, ymm4."
"BLENDVPS xmm1, xmm2/m128, <XMM0>","66 0F 38 14 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","implicit XMM0","NA","","Select packed single precision floating-point values from xmm1 and xmm2/m128 from mask specified in XMM0 and store the values into xmm1."
"VBLENDVPS xmm1, xmm2, xmm3/m128, xmm4","VEX.NDS.128.66.0F3A.W0 4A /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Conditionally copy single-precision floating-point values from xmm2 or xmm3/m128 to xmm1, based on mask bits in the specified mask operand, xmm4."
"VBLENDVPS ymm1, ymm2, ymm3/m256, ymm4","VEX.NDS.256.66.0F3A.W0 4A /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Conditionally copy single-precision floating-point values from ymm2 or ymm3/m256 to ymm1, based on mask bits in the specified mask register, ymm4."
"BLSI r32, r/m32","VEX.NDD.LZ.0F38.W0 F3 /3","Valid","Valid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Extract lowest set bit from r/m32 and set that bit in r32."
"BLSI r64, r/m64","VEX.NDD.LZ.0F38.W1 F3 /3","Valid","Invalid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Extract lowest set bit from r/m64, and set that bit in r64."
"BLSMSK r32, r/m32","VEX.NDD.LZ.0F38.W0 F3 /2","Valid","Valid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Set all lower bits in r32 to â€œ1â€ starting from bit 0 to lowest set bit in r/m32."
"BLSMSK r64, r/m64","VEX.NDD.LZ.0F38.W1 F3 /2","Valid","Invalid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Set all lower bits in r64 to â€œ1â€ starting from bit 0 to lowest set bit in r/m64."
"BLSR r32, r/m32","VEX.NDD.LZ.0F38.W0 F3 /1","Valid","Valid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Reset lowest set bit of r/m32, keep all other bits of r/m32 and write result to r32."
"BLSR r64, r/m64","VEX.NDD.LZ.0F38.W1 F3 /1","Valid","Invalid","Invalid","BMI1","VEX.vvvv (w)","ModRM:r/m (r)","NA","NA","","Reset lowest set bit of r/m64, keep all other bits of r/m64 and write result to r64."
"BNDCL bnd, r/m32","F3 0F 1A /r","Invalid","Valid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m32 is lower than the lower bound in bnd.LB."
"BNDCL bnd, r/m64","F3 0F 1A /r","Valid","Invalid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m64 is lower than the lower bound in bnd.LB."
"BNDCU bnd, r/m32","F2 0F 1A /r","Invalid","Valid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form)."
"BNDCU bnd, r/m64","F2 0F 1A /r","Valid","Invalid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB in 1's complement form)."
"BNDCN bnd, r/m32","F2 0F 1B /r","Invalid","Valid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m32 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form)."
"BNDCN bnd, r/m64","F2 0F 1B /r","Valid","Invalid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Generate a #BR if the address in r/m64 is higher than the upper bound in bnd.UB (bnb.UB not in 1's complement form)."
"BNDLDX bnd, mib","NP 0F 1A /r","Valid","Valid","Invalid","MPX","ModRM:reg (w)","SIB.base (r): Address of pointer SIB.index (r)","NA","","","Load the bounds stored in a bound table entry (BTE) into bnd with address translation using the base of mib and conditional on the index of mib matching the pointer value in the BTE."
"BNDMK bnd, m32","F3 0F 1B /r","Invalid","Valid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Make lower and upper bounds from m32 and store them in bnd."
"BNDMK bnd, m64","F3 0F 1B /r","Valid","Invalid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Make lower and upper bounds from m64 and store them in bnd."
"BNDMOV bnd1, bnd2/m64","66 0F 1A /r","Invalid","Valid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Move lower and upper bound from bnd2/m64 to bound register bnd1."
"BNDMOV bnd1, bnd2/m128","66 0F 1A /r","Valid","Invalid","Invalid","MPX","ModRM:reg (w)","ModRM:r/m (r)","NA","","","Move lower and upper bound from bnd2/m128 to bound register bnd1."
"BNDMOV bnd1/m64, bnd2","66 0F 1B /r","Invalid","Valid","Invalid","MPX","ModRM:r/m (w)","ModRM:reg (r)","NA","","","Move lower and upper bound from bnd2 to bnd1/m64."
"BNDMOV bnd1/m128, bnd2","66 0F 1B /r","Valid","Invalid","Invalid","MPX","ModRM:r/m (w)","ModRM:reg (r)","NA","","","Move lower and upper bound from bnd2 to bound register bnd1/m128."
"BNDSTX mib, bnd","NP 0F 1B /r","Valid","Valid","Invalid","MPX","SIB.base (r): Address of pointer SIB.index (r)","ModRM:reg (r)","NA","","","Store the bounds in bnd and the pointer value in the index regis-ter of mib to a bound table entry (BTE) with address translation using the base of mib."
"BOUND r16, m16&16","62 /r","Invalid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Check if r16 (array index) is within bounds specified by m16&16."
"BOUND r32, m32&32","62 /r","Invalid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Check if r32 (array index) is within bounds specified by m32&32."
"BSF r16, r/m16","0F BC /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan forward on r/m16."
"BSF r32, r/m32","0F BC /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan forward on r/m32."
"BSF r64, r/m64","REX.W + 0F BC /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan forward on r/m64."
"BSR r16, r/m16","0F BD /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan reverse on r/m16."
"BSR r32, r/m32","0F BD /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan reverse on r/m32."
"BSR r64, r/m64","REX.W + 0F BD /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Bit scan reverse on r/m64."
"BSWAP r32","0F C8+rd","Valid","Valid","Valid","","opcode +rd (r, w)","NA","NA","NA","","Reverses the byte order of a 32-bit register."
"BSWAP r64","REX.W + 0F C8+rd","Valid","Invalid","Invalid","","opcode +rd (r, w)","NA","NA","NA","","Reverses the byte order of a 64-bit register."
"BT r/m16, r16","0F A3 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag."
"BT r/m32, r32","0F A3 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag."
"BT r/m64, r64","REX.W + 0F A3 /r","Valid","Invalid","Invalid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag."
"BT r/m16, imm8","0F BA /4 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Store selected bit in CF flag."
"BT r/m32, imm8","0F BA /4 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Store selected bit in CF flag."
"BT r/m64, imm8","REX.W + 0F BA /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8","NA","NA","","Store selected bit in CF flag."
"BTC r/m16, r16","0F BB /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and complement."
"BTC r/m32, r32","0F BB /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and complement."
"BTC r/m64, r64","REX.W + 0F BB /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and complement."
"BTC r/m16, imm8","0F BA /7 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and complement."
"BTC r/m32, imm8","0F BA /7 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and complement."
"BTC r/m64, imm8","REX.W + 0F BA /7 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and complement."
"BTR r/m16, r16","0F B3 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and clear."
"BTR r/m32, r32","0F B3 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and clear."
"BTR r/m64, r64","REX.W + 0F B3 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and clear."
"BTR r/m16, imm8","0F BA /6 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and clear."
"BTR r/m32, imm8","0F BA /6 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and clear."
"BTR r/m64, imm8","REX.W + 0F BA /6 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and clear."
"BTS r/m16, r16","0F AB /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and set."
"BTS r/m32, r32","0F AB /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and set."
"BTS r/m64, r64","REX.W + 0F AB /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Store selected bit in CF flag and set."
"BTS r/m16, imm8","0F BA /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and set."
"BTS r/m32, imm8","0F BA /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and set."
"BTS r/m64, imm8","REX.W + 0F BA /5 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Store selected bit in CF flag and set."
"BZHI r32a, r/m32, r32b","VEX.NDS.LZ.0F38.W0 F5 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Zero bits in r/m32 starting with the position in r32b, write result to r32a."
"BZHI r64a, r/m64, r64b","VEX.NDS.LZ.0F38.W1 F5 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Zero bits in r/m64 starting with the position in r64b, write result to r64a."
"CALL rel16","E8 cw","Invalid","Valid","Valid","","","","","","","Call near, relative, displacement relative to next instruction."
"CALL rel32","E8 cd","Valid","Valid","Invalid","","","","","","","Call near, relative, displacement relative to next instruction. 32-bit displacement sign extended to 64-bits in 64-bit mode."
"CALL r/m16","FF /2","Invalid","Valid","Valid","","","","","","","Call near, absolute indirect, address given in r/m16."
"CALL r/m32","FF /2","Invalid","Valid","Invalid","","","","","","","Call near, absolute indirect, address given in r/m32."
"CALL r/m64","FF /2","Valid","Invalid","Invalid","","","","","","","Call near, absolute indirect, address given in r/m64."
"CALL ptr16:16","9A cd","Invalid","Valid","Valid","","","","","","","Call far, absolute, address given in operand."
"CALL ptr16:32","9A cp","Invalid","Valid","Invalid","","","","","","","Call far, absolute, address given in operand."
"CALL m16:16","FF /3","Valid","Valid","Valid","","","","","","","Call far, absolute indirect address given in m16:16. In 32-bit mode: if selector points to a gate, then RIP = 32-bit zero extended displacement taken from gate; else RIP = zero extended 16-bit offset from far pointer referenced in the instruction."
"CALL m16:32","FF /3","Valid","Valid","Invalid","","","","","","","In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = zero extended 32-bit offset from far pointer referenced in the instruction."
"CALL m16:64","REX.W + FF /3","Valid","Invalid","Invalid","","","","","","","In 64-bit mode: If selector points to a gate, then RIP = 64-bit displacement taken from gate; else RIP = 64-bit offset from far pointer referenced in the instruction."
"CBW","98","Valid","Valid","Valid","","NA","NA","NA","NA","","AX â† sign-extend of AL."
"CWDE","98","Valid","Valid","Valid","","NA","NA","NA","NA","","EAX â† sign-extend of AX."
"CDQE","REX.W + 98","Valid","Invalid","Invalid","","NA","NA","NA","NA","","RAX â† sign-extend of EAX."
"CLAC","NP 0F 01 CA","Valid","Valid","Invalid","SMAP","NA","NA","NA","NA","","Clear the AC flag in the EFLAGS register."
"CLC","F8","Valid","Valid","Valid","","NA","NA","NA","NA","","Clear CF flag."
"CLD","FC","Valid","Valid","Valid","","NA","NA","NA","NA","","Clear DF flag."
"CLFLUSH m8","NP 0F AE /7","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Flushes cache line containing m8."
"CLFLUSHOPT m8","66 0F AE /7","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Flushes cache line containing m8."
"CLI","FA","Valid","Valid","Valid","","NA","NA","NA","NA","","Clear interrupt flag; interrupts disabled when interrupt flag cleared."
"CLTS","0F 06","Valid","Valid","Valid","","NA","NA","NA","NA","","Clears TS flag in CR0."
"CLWB m8","66 0F AE /6","Valid","Valid","Invalid","CLWB","","","","","","Writes back modified cache line containing m8, and may retain the line in cache hierarchy in non-modified state."
"CMC","F5","Valid","Valid","Valid","","NA","NA","NA","NA","","Complement CF flag."
"CMOVA r16, r/m16","0F 47 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above (CF=0 and ZF=0)."
"CMOVA r32, r/m32","0F 47 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above (CF=0 and ZF=0)."
"CMOVA r64, r/m64","REX.W + 0F 47 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above (CF=0 and ZF=0)."
"CMOVAE r16, r/m16","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above or equal (CF=0)."
"CMOVAE r32, r/m32","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above or equal (CF=0)."
"CMOVAE r64, r/m64","REX.W + 0F 43 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if above or equal (CF=0)."
"CMOVB r16, r/m16","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below (CF=1)."
"CMOVB r32, r/m32","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below (CF=1)."
"CMOVB r64, r/m64","REX.W + 0F 42 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below (CF=1)."
"CMOVBE r16, r/m16","0F 46 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below or equal (CF=1 or ZF=1)."
"CMOVBE r32, r/m32","0F 46 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below or equal (CF=1 or ZF=1)."
"CMOVBE r64, r/m64","REX.W + 0F 46 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if below or equal (CF=1 or ZF=1)."
"CMOVC r16, r/m16","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if carry (CF=1)."
"CMOVC r32, r/m32","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if carry (CF=1)."
"CMOVC r64, r/m64","REX.W + 0F 42 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if carry (CF=1)."
"CMOVE r16, r/m16","0F 44 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if equal (ZF=1)."
"CMOVE r32, r/m32","0F 44 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if equal (ZF=1)."
"CMOVE r64, r/m64","REX.W + 0F 44 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if equal (ZF=1)."
"CMOVG r16, r/m16","0F 4F /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater (ZF=0 and SF=OF)."
"CMOVG r32, r/m32","0F 4F /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater (ZF=0 and SF=OF)."
"CMOVG r64, r/m64","REX.W + 0F 4F /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater (ZF=0 and SF=OF)."
"CMOVGE r16, r/m16","0F 4D /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater or equal (SF=OF)."
"CMOVGE r32, r/m32","0F 4D /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater or equal (SF=OF)."
"CMOVGE r64, r/m64","REX.W + 0F 4D /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if greater or equal (SF=OF)."
"CMOVL r16, r/m16","0F 4C /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less (SFâ‰  OF)."
"CMOVL r32, r/m32","0F 4C /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less (SFâ‰  OF)."
"CMOVL r64, r/m64","REX.W + 0F 4C /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less (SFâ‰  OF)."
"CMOVLE r16, r/m16","0F 4E /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less or equal (ZF=1 or SFâ‰  OF)."
"CMOVLE r32, r/m32","0F 4E /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less or equal (ZF=1 or SFâ‰  OF)."
"CMOVLE r64, r/m64","REX.W + 0F 4E /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if less or equal (ZF=1 or SFâ‰  OF)."
"CMOVNA r16, r/m16","0F 46 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above (CF=1 or ZF=1)."
"CMOVNA r32, r/m32","0F 46 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above (CF=1 or ZF=1)."
"CMOVNA r64, r/m64","REX.W + 0F 46 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above (CF=1 or ZF=1)."
"CMOVNAE r16, r/m16","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above or equal (CF=1)."
"CMOVNAE r32, r/m32","0F 42 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above or equal (CF=1)."
"CMOVNAE r64, r/m64","REX.W + 0F 42 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not above or equal (CF=1)."
"CMOVNB r16, r/m16","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below (CF=0)."
"CMOVNB r32, r/m32","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below (CF=0)."
"CMOVNB r64, r/m64","REX.W + 0F 43 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below (CF=0)."
"CMOVNBE r16, r/m16","0F 47 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below or equal (CF=0 and ZF=0).CMOVccâ€”Conditional Move"
"CMOVNBE r32, r/m32","0F 47 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below or equal (CF=0 and ZF=0)."
"CMOVNBE r64, r/m64","REX.W + 0F 47 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not below or equal (CF=0 and ZF=0)."
"CMOVNC r16, r/m16","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not carry (CF=0)."
"CMOVNC r32, r/m32","0F 43 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not carry (CF=0)."
"CMOVNC r64, r/m64","REX.W + 0F 43 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not carry (CF=0)."
"CMOVNE r16, r/m16","0F 45 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not equal (ZF=0)."
"CMOVNE r32, r/m32","0F 45 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not equal (ZF=0)."
"CMOVNE r64, r/m64","REX.W + 0F 45 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not equal (ZF=0)."
"CMOVNG r16, r/m16","0F 4E /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater (ZF=1 or SFâ‰  OF)."
"CMOVNG r32, r/m32","0F 4E /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater (ZF=1 or SFâ‰  OF)."
"CMOVNG r64, r/m64","REX.W + 0F 4E /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater (ZF=1 or SFâ‰  OF)."
"CMOVNGE r16, r/m16","0F 4C /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater or equal (SFâ‰  OF)."
"CMOVNGE r32, r/m32","0F 4C /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater or equal (SFâ‰  OF)."
"CMOVNGE r64, r/m64","REX.W + 0F 4C /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not greater or equal (SFâ‰  OF)."
"CMOVNL r16, r/m16","0F 4D /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less (SF=OF)."
"CMOVNL r32, r/m32","0F 4D /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less (SF=OF)."
"CMOVNL r64, r/m64","REX.W + 0F 4D /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less (SF=OF)."
"CMOVNLE r16, r/m16","0F 4F /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less or equal (ZF=0 and SF=OF)."
"CMOVNLE r32, r/m32","0F 4F /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less or equal (ZF=0 and SF=OF)."
"CMOVNLE r64, r/m64","REX.W + 0F 4F /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not less or equal (ZF=0 and SF=OF)."
"CMOVNO r16, r/m16","0F 41 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not overflow (OF=0)."
"CMOVNO r32, r/m32","0F 41 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not overflow (OF=0)."
"CMOVNO r64, r/m64","REX.W + 0F 41 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not overflow (OF=0)."
"CMOVNP r16, r/m16","0F 4B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not parity (PF=0)."
"CMOVNP r32, r/m32","0F 4B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not parity (PF=0)."
"CMOVNP r64, r/m64","REX.W + 0F 4B /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not parity (PF=0)."
"CMOVNS r16, r/m16","0F 49 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not sign (SF=0)."
"CMOVNS r32, r/m32","0F 49 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not sign (SF=0)."
"CMOVNS r64, r/m64","REX.W + 0F 49 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not sign (SF=0)."
"CMOVNZ r16, r/m16","0F 45 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not zero (ZF=0)."
"CMOVNZ r32, r/m32","0F 45 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not zero (ZF=0)."
"CMOVNZ r64, r/m64","REX.W + 0F 45 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if not zero (ZF=0)."
"CMOVO r16, r/m16","0F 40 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if overflow (OF=1)."
"CMOVO r32, r/m32","0F 40 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if overflow (OF=1)."
"CMOVO r64, r/m64","REX.W + 0F 40 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if overflow (OF=1)."
"CMOVP r16, r/m16","0F 4A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity (PF=1)."
"CMOVP r32, r/m32","0F 4A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity (PF=1)."
"CMOVP r64, r/m64","REX.W + 0F 4A /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity (PF=1)."
"CMOVPE r16, r/m16","0F 4A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity even (PF=1)."
"CMOVPE r32, r/m32","0F 4A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity even (PF=1)."
"CMOVPE r64, r/m64","REX.W + 0F 4A /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Move if parity even (PF=1).CMOVccâ€”Conditional Move"
"CMP AL, imm8","3C ib","Valid","Valid","Valid","","AL/AX/EAX/RAX (r)","imm8","NA","NA","","Compare imm8 with AL."
"CMP AX, imm16","3D iw","Valid","Valid","Valid","","AL/AX/EAX/RAX (r)","imm8","NA","NA","","Compare imm16 with AX."
"CMP EAX, imm32","3D id","Valid","Valid","Valid","","AL/AX/EAX/RAX (r)","imm8","NA","NA","","Compare imm32 with EAX."
"CMP RAX, imm32","REX.W + 3D id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX (r)","imm8","NA","NA","","Compare imm32 sign-extended to 64-bits with RAX."
"CMP r/m8, imm8","80 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm8 with r/m8."
"CMP r/m8 , imm8","REX + 80 /7 ib","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm8 with r/m8."
"CMP r/m16, imm16","81 /7 iw","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm16 with r/m16."
"CMP r/m32, imm32","81 /7 id","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm32 with r/m32."
"CMP r/m64, imm32","REX.W + 81 /7 id","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm32 sign-extended to 64-bits with r/m64."
"CMP r/m16, imm8","83 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm8 with r/m16."
"CMP r/m32, imm8","83 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm8 with r/m32."
"CMP r/m64, imm8","REX.W + 83 /7 ib","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8","NA","NA","","Compare imm8 with r/m64."
"CMP r/m8, r8","38 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Compare r8 with r/m8."
"CMP r/m8 , r8","REX + 38 /r","Valid","Invalid","Invalid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Compare r8 with r/m8."
"CMP r/m16, r16","39 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Compare r16 with r/m16."
"CMP r/m32, r32","39 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Compare r32 with r/m32."
"CMP r/m64,r64","REX.W + 39 /r","Valid","Invalid","Invalid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","Compare r64 with r/m64."
"CMP r8, r/m8","3A /r","Valid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Compare r/m8 with r8."
"CMP r8 , r/m8","REX + 3A /r","Valid","Invalid","Invalid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Compare r/m8 with r8."
"CMP r16, r/m16","3B /r","Valid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Compare r/m16 with r16."
"CMP r32, r/m32","3B /r","Valid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Compare r/m32 with r32."
"CMP r64, r/m64","REX.W + 3B /r","Valid","Invalid","Invalid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Compare r/m64 with r64."
"CMPPD xmm1, xmm2/m128, imm8","66 0F C2 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Compare packed double-precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate."
"VCMPPD xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare packed double-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPD ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare packed double-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPD k1 {k2}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed double-precision floating-point values in xmm3/m128/m64bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed double-precision floating-point values in ymm3/m256/m64bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPD k1 {k2}, zmm2, zmm3/m512/m64bcst{sae}, imm8","EVEX.NDS.512.66.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed double-precision floating-point values in zmm3/m512/m64bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"CMPPS xmm1, xmm2/m128, imm8","NP 0F C2 /r ib","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Compare packed single-precision floating-point values in xmm2/m128 and xmm1 using bits 2:0 of imm8 as a comparison predicate."
"VCMPPS xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare packed single-precision floating-point values in xmm3/m128 and xmm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPS ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare packed single-precision floating-point values in ymm3/m256 and ymm2 using bits 4:0 of imm8 as a comparison predicate."
"VCMPPS k1 {k2}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed single-precision floating-point values in xmm3/m128/m32bcst and xmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed single-precision floating-point values in ymm3/m256/m32bcst and ymm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VCMPPS k1 {k2}, zmm2, zmm3/m512/m32bcst{sae}, imm8","EVEX.NDS.512.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Compare packed single-precision floating-point values in zmm3/m512/m32bcst and zmm2 using bits 4:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"CMPS m8, m8","A6","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI to byte at address (R|E)DI. The status flags are set accordingly."
"CMPS m16, m16","A7","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly."
"CMPS m32, m32","A7","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare dword at address DS:(E)SI at dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI at dword at address (R|E)DI. The status flags are set accordingly."
"CMPS m64, m64","REX.W + A7","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly."
"CMPSB","A6","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare byte at address DS:(E)SI with byte at address ES:(E)DI; For 64-bit mode compare byte at address (R|E)SI with byte at address (R|E)DI. The status flags are set accordingly."
"CMPSW","A7","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare word at address DS:(E)SI with word at address ES:(E)DI; For 64-bit mode compare word at address (R|E)SI with word at address (R|E)DI. The status flags are set accordingly."
"CMPSD","A7","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, compare dword at address DS:(E)SI with dword at address ES:(E)DI; For 64-bit mode compare dword at address (R|E)SI with dword at address (R|E)DI. The status flags are set accordingly."
"CMPSQ","REX.W + A7","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Compares quadword at address (R|E)SI with quadword at address (R|E)DI and sets the status flags accordingly."
"CMPSD xmm1, xmm2/m64, imm8","F2 0F C2 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Compare low double-precision floating-point value in xmm2/m64 and xmm1 using bits 2:0 of imm8 as comparison predicate."
"VCMPSD xmm1, xmm2, xmm3/m64, imm8","VEX.NDS.LIG.F2.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate."
"VCMPSD k1 {k2}, xmm2, xmm3/m64{sae}, imm8","EVEX.NDS.LIG.F2.0F.W1 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Compare low double-precision floating-point value in xmm3/m64 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1."
"CMPSS xmm1, xmm2/m32, imm8","F3 0F C2 /r ib","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Compare low single-precision floating-point value in xmm2/m32 and xmm1 using bits 2:0 of imm8 as comparison predicate."
"VCMPSS xmm1, xmm2, xmm3/m32, imm8","VEX.NDS.LIG.F3.0F.WIG C2 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate."
"VCMPSS k1 {k2}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.F3.0F.W0 C2 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Compare low single-precision floating-point value in xmm3/m32 and xmm2 using bits 4:0 of imm8 as comparison predicate with writemask k2 and leave the result in mask register k1."
"CMPXCHG r/m8, r8","0F B0/r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL."
"CMPXCHG r/m8,r8","REX + 0F B0/r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Compare AL with r/m8. If equal, ZF is set and r8 is loaded into r/m8. Else, clear ZF and load r/m8 into AL."
"CMPXCHG r/m16, r16","0F B1/r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Compare AX with r/m16. If equal, ZF is set and r16 is loaded into r/m16. Else, clear ZF and load r/m16 into AX."
"CMPXCHG r/m32, r32","0F B1/r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Compare EAX with r/m32. If equal, ZF is set and r32 is loaded into r/m32. Else, clear ZF and load r/m32 into EAX."
"CMPXCHG r/m64, r64","REX.W + 0F B1/r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Compare RAX with r/m64. If equal, ZF is set and r64 is loaded into r/m64. Else, clear ZF and load r/m64 into RAX."
"CMPXCHG8B m64","0F C7 /1 m64","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Compare EDX:EAX with m64. If equal, set ZF and load ECX:EBX into m64. Else, clear ZF and load m64 into EDX:EAX."
"CMPXCHG16B m128","REX.W + 0F C7 /1 m128","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Compare RDX:RAX with m128. If equal, set ZF and load RCX:RBX into m128. Else, clear ZF and load m128 into RDX:RAX."
"COMISD xmm1, xmm2/m64","66 0F 2F /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISD xmm1, xmm2/m64","VEX.LIG.66.0F.WIG 2F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VCOMISD xmm1, xmm2/m64{sae}","EVEX.LIG.66.0F.W1 2F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"COMISS xmm1, xmm2/m32","NP 0F 2F /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32","VEX.LIG.0F.WIG 2F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VCOMISS xmm1, xmm2/m32{sae}","EVEX.LIG.0F.W0 2F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"CPUID","0F A2","Valid","Valid","Valid","","NA","NA","NA","NA","","Returns processor identification and feature information to the EAX, EBX, ECX, and EDX registers, as determined by input entered in EAX (in some cases, ECX as well)."
"CRC32 r32, r/m8","F2 0F 38 F0 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m8."
"CRC32 r32, r/m8","F2 REX 0F 38 F0 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m8."
"CRC32 r32, r/m16","F2 0F 38 F1 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m16."
"CRC32 r32, r/m32","F2 0F 38 F1 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m32."
"CRC32 r64, r/m8","F2 REX.W 0F 38 F0 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m8."
"CRC32 r64, r/m64","F2 REX.W 0F 38 F1 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Accumulate CRC32 on r/m64."
"CVTDQ2PD xmm1, xmm2/m64","F3 0F E6 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1."
"VCVTDQ2PD xmm1, xmm2/m64","VEX.128.F3.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed signed doubleword integers from xmm2/mem to two packed double-precision floating-point values in xmm1."
"VCVTDQ2PD ymm1, xmm2/m128","VEX.256.F3.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed signed doubleword integers from xmm2/mem to four packed double-precision floating-point values in ymm1."
"VCVTDQ2PD xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert 2 packed signed doubleword integers from xmm2/m128/m32bcst to eight packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTDQ2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert 4 packed signed doubleword integers from xmm2/m128/m32bcst to 4 packed double-precision floating-point values in ymm1 with writemask k1."
"VCVTDQ2PD zmm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.512.F3.0F.W0 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"CVTDQ2PS xmm1, xmm2/m128","NP 0F 5B /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1."
"VCVTDQ2PS xmm1, xmm2/m128","VEX.128.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed signed doubleword integers from xmm2/mem to four packed single-precision floating-point values in xmm1."
"VCVTDQ2PS ymm1, ymm2/m256","VEX.256.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert eight packed signed doubleword integers from ymm2/mem to eight packed single-precision floating-point values in ymm1."
"VCVTDQ2PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed signed doubleword integers from xmm2/m128/m32bcst to four packed single-precision floating-point values in xmm1with writemask k1."
"VCVTDQ2PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed signed doubleword integers from ymm2/m256/m32bcst to eight packed single-precision floating-point values in ymm1with writemask k1."
"VCVTDQ2PS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed signed doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1with writemask k1."
"CVTPD2DQ xmm1, xmm2/m128","F2 0F E6 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1."
"VCVTPD2DQ xmm1, xmm2/m128","VEX.128.F2.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1."
"VCVTPD2DQ xmm1, ymm2/m256","VEX.256.F2.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1."
"VCVTPD2DQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2DQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2DQ ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.F2.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 subject to writemask k1."
"CVTPD2PI mm, xmm/m128","66 0F 2D /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two packed double-precision floating-point values from xmm/m128 to two packed signed doubleword integers in mm."
"CVTPD2PS xmm1, xmm2/m128","66 0F 5A /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two single-precision floating-point values in xmm1."
"VCVTPD2PS xmm1, xmm2/m128","VEX.128.66.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two single-precision floating-point values in xmm1."
"VCVTPD2PS xmm1, ymm2/m256","VEX.256.66.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed double-precision floating-point values in ymm2/mem to four single-precision floating-point values in xmm1."
"VCVTPD2PS xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two single-precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four single-precision floating-point values in xmm1with writemask k1."
"VCVTPD2PS ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight single-precision floating-point values in ymm1with writemask k1."
"CVTPI2PD xmm, mm/m64","66 0F 2A /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two packed signed doubleword integers from mm/mem64 to two packed double-precision floating-point values in xmm."
"CVTPI2PS xmm, mm/m64","NP 0F 2A /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two signed doubleword integers from mm/m64 to two single-precision floating-point values in xmm."
"CVTPS2DQ xmm1, xmm2/m128","66 0F 5B /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1."
"VCVTPS2DQ xmm1, xmm2/m128","VEX.128.66.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1."
"VCVTPS2DQ ymm1, ymm2/m256","VEX.256.66.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1."
"VCVTPS2DQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 subject to writemask k1."
"VCVTPS2DQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 subject to writemask k1."
"VCVTPS2DQ zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.66.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 subject to writemask k1."
"CVTPS2PD xmm1, xmm2/m64","NP 0F 5A /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed single-precision floating-point values in xmm2/m64 to two packed double-precision floating-point values in xmm1."
"VCVTPS2PD xmm1, xmm2/m64","VEX.128.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed single-precision floating-point values in xmm2/m64 to two packed double-precision floating-point values in xmm1."
"VCVTPS2PD ymm1, xmm2/m128","VEX.256.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed single-precision floating-point values in xmm2/m128 to four packed double-precision floating-point values in ymm1."
"VCVTPS2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.0F.W0 5A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed single-precision floating-point values in xmm2/m64/m32bcst to packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTPS2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.0F.W0 5A /r","Valid","Valid","Invalid","AVX512VL","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed single-precision floating-point values in xmm2/m128/m32bcst to packed double-precision floating-point values in ymm1 with writemask k1."
"VCVTPS2PD zmm1 {k1}{z}, ymm2/m256/m32bcst{sae}","EVEX.512.0F.W0 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed single-precision floating-point values in ymm2/m256/b32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"CVTPS2PI mm, xmm/m64","NP 0F 2D /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two packed single-precision floating-point values from xmm/m64 to two packed signed doubleword integers in mm."
"CVTSD2SI r32,xmm1/m64",F2 0F 2D /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
"CVTSD2SI r64,xmm1/m64",F2 REX.W 0F 2D /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer signextended into r64.
"VCVTSD2SI r32,xmm1/m64",VEX.LIG.F2.0F.W0 2D /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
"VCVTSD2SI r64,xmm1/m64",VEX.LIG.F2.0F.W1 2D /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer signextended into r64.
"VCVTSD2SI r32,xmm1/m64{er}",EVEX.LIG.F2.0F.W0 2D /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer r32.
"VCVTSD2SI r64,xmm1/m64{er}",EVEX.LIG.F2.0F.W1 2D /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer signextended into r64.
"CVTSD2SS xmm1, xmm2/m64","F2 0F 5A /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert one double-precision floating-point value in xmm2/m64 to one single-precision floating-point value in xmm1."
"VCVTSD2SS xmm1,xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2."
"VCVTSD2SS xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.F2.0F.W1 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Convert one double-precision floating-point value in xmm3/m64 to one single-precision floating-point value and merge with high bits in xmm2 under writemask k1."
"CVTSI2SD xmm1,r32/m32",F2 0F 2A /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,SSE2,Convert one signed doubleword integer from r32/m32 to one double-precision floating-point value in xmm1.
"CVTSI2SD xmm1,r/m64",F2 REX.W 0F 2A /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,SSE2,Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
"VCVTSI2SD xmm1,xmm2,r/m32",VEX.NDS.LIG.F2.0F.W0 2A /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,AVX,Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
"VCVTSI2SD xmm1,xmm2,r/m64",VEX.NDS.LIG.F2.0F.W1 2A /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,AVX,Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
"VCVTSI2SD xmm1,xmm2,r/m32",EVEX.NDS.LIG.F2.0F.W0 2A /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,AVX512F,Convert one signed doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
"VCVTSI2SD xmm1,xmm2,r/m64{er}",EVEX.NDS.LIG.F2.0F.W1 2A /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,AVX512F,Convert one signed quadword integer from r/m64 to one double-precision floating-point value in xmm1.
"CVTSI2SS xmm1,r/m32",F3 0F 2A /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
"CVTSI2SS xmm1,r/m64",F3 REX.W 0F 2A /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
"VCVTSI2SS xmm1,xmm2,r/m32",VEX.NDS.LIG.F3.0F.W0 2A /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,,Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
"VCVTSI2SS xmm1,xmm2,r/m64",VEX.NDS.LIG.F3.0F.W1 2A /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,,Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
"VCVTSI2SS xmm1,xmm2,r/m32{er}",EVEX.NDS.LIG.F3.0F.W0 2A /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,,Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
"VCVTSI2SS xmm1,xmm2,r/m64{er}",EVEX.NDS.LIG.F3.0F.W1 2A /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
"CVTSS2SD xmm1, xmm2/m32","F3 0F 5A /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert one single-precision floating-point value in xmm2/m32 to one double-precision floating-point value in xmm1."
"VCVTSS2SD xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 5A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2."
"VCVTSS2SD xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.NDS.LIG.F3.0F.W0 5A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Convert one single-precision floating-point value in xmm3/m32 to one double-precision floating-point value and merge with high bits of xmm2 under writemask k1."
"CVTSS2SI r32,xmm1/m32",F3 0F 2D /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
"CVTSS2SI r64,xmm1/m32",F3 REX.W 0F 2D /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
"VCVTSS2SI r32,xmm1/m32",VEX.LIG.F3.0F.W0 2D /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
"VCVTSS2SI r64,xmm1/m32",VEX.LIG.F3.0F.W1 2D /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
"VCVTSS2SI r32,xmm1/m32{er}",EVEX.LIG.F3.0F.W0 2D /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32.
"VCVTSS2SI r64,xmm1/m32{er}",EVEX.LIG.F3.0F.W1 2D /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64.
"CVTTPD2DQ xmm1, xmm2/m128","66 0F E6 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ xmm1, xmm2/m128","VEX.128.66.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert two packed double-precision floating-point values in xmm2/mem to two signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ xmm1, ymm2/m256","VEX.256.66.0F.WIG E6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed double-precision floating-point values in ymm2/mem to four signed doubleword integers in xmm1 using truncation."
"VCVTTPD2DQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four signed doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2DQ ymm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight signed doubleword integers in ymm1 using truncation subject to writemask k1."
"CVTTPD2PI mm, xmm/m128","66 0F 2C /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two packer double-precision floating-point values from xmm/m128 to two packed signed doubleword integers in mm using truncation."
"CVTTPS2DQ xmm1, xmm2/m128","F3 0F 5B /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation."
"VCVTTPS2DQ xmm1, xmm2/m128","VEX.128.F3.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed single-precision floating-point values from xmm2/mem to four packed signed doubleword values in xmm1 using truncation."
"VCVTTPS2DQ ymm1, ymm2/m256","VEX.256.F3.0F.WIG 5B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert eight packed single-precision floating-point values from ymm2/mem to eight packed signed doubleword values in ymm1 using truncation."
"VCVTTPS2DQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed doubleword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2DQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed doubleword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2DQ zmm1 {k1}{z}, zmm2/m512/m32bcst {sae}","EVEX.512.F3.0F.W0 5B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed signed doubleword values in zmm1 using truncation subject to writemask k1."
"CVTTPS2PI mm, xmm/m64","NP 0F 2C /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Convert two single-precision floating-point values from xmm/m64 to two signed doubleword signed integers in mm using truncation."
"CVTTSD2SI r32,xmm1/m64",F2 0F 2C /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
"CVTTSD2SI r64,xmm1/m64",F2 REX.W 0F 2C /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
"VCVTTSD2SI r32,xmm1/m64",VEX.LIG.F2.0F.W0 2C /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
"VCVTTSD2SI r64,xmm1/m64",VEX.LIG.F2.0F.W1 2C /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
"VCVTTSD2SI r32,xmm1/m64{sae}",EVEX.LIG.F2.0F.W0 2C,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one signed doubleword integer in r32 using truncation.
"VCVTTSD2SI r64,xmm1/m64{sae}",EVEX.LIG.F2.0F.W1 2C,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one signed quadword integer in r64 using truncation.
"CVTTSS2SI r32,xmm1/m32",F3 0F 2C /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
"CVTTSS2SI r64,xmm1/m32",F3 REX.W 0F 2C /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
"VCVTTSS2SI r32,xmm1/m32",VEX.LIG.F3.0F.W0 2C /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
"VCVTTSS2SI r64,xmm1/m32",VEX.LIG.F3.0F.W1 2C /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
"VCVTTSS2SI r32,xmm1/m32{sae}",EVEX.LIG.F3.0F.W0 2C,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one signed doubleword integer in r32 using truncation.
"VCVTTSS2SI r64,xmm1/m32{sae}",EVEX.LIG.F3.0F.W1 2C,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one signed quadword integer in r64 using truncation.
"CWD","99","Valid","Valid","Valid","","NA","NA","NA","NA","","DX:AX â† sign-extend of AX."
"CDQ","99","Valid","Valid","Valid","","NA","NA","NA","NA","","EDX:EAX â† sign-extend of EAX."
"CQO","REX.W + 99","Valid","Invalid","Invalid","","NA","NA","NA","NA","","RDX:RAXâ† sign-extend of RAX."
"DAA","27","Invalid","Valid","Valid","","NA","NA","NA","NA","","Decimal adjust AL after addition."
"DAS","2F","Invalid","Valid","Valid","","NA","NA","NA","NA","","Decimal adjust AL after subtraction."
"DEC r/m8","FE /1","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Decrement r/m8 by 1."
"DEC r/m8","REX + FE /1","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Decrement r/m8 by 1."
"DEC r/m16","FF /1","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Decrement r/m16 by 1."
"DEC r/m32","FF /1","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Decrement r/m32 by 1."
"DEC r/m64","REX.W + FF /1","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Decrement r/m64 by 1."
"DEC r16","48+rw","Invalid","Valid","Valid","","opcode +rd (r, w)","NA","NA","NA","","Decrement r16 by 1."
"DEC r32","48+rd","Invalid","Valid","Valid","","opcode +rd (r, w)","NA","NA","NA","","Decrement r32 by 1."
"DIV r/m8","F6 /6","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Unsigned divide AX by r/m8, with result stored in AL â† Quotient, AH â† Remainder."
"DIV r/m8","REX + F6 /6","Valid","Invalid","Invalid","","ModRM:r/m (w)","NA","NA","NA","","Unsigned divide AX by r/m8, with result stored in AL â† Quotient, AH â† Remainder."
"DIV r/m16","F7 /6","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Unsigned divide DX:AX by r/m16, with result stored in AX â† Quotient, DX â† Remainder."
"DIV r/m32","F7 /6","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Unsigned divide EDX:EAX by r/m32, with result stored in EAX â† Quotient, EDX â† Remainder."
"DIV r/m64","REX.W + F7 /6","Valid","Invalid","Invalid","","ModRM:r/m (w)","NA","NA","NA","","Unsigned divide RDX:RAX by r/m64, with result stored in RAX â† Quotient, RDX â† Remainder."
"DIVPD xmm1, xmm2/m128","66 0F 5E /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Divide packed double-precision floating-point values in xmm1 by packed double-precision floating-point values in xmm2/mem."
"VDIVPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/mem."
"VDIVPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/mem."
"VDIVPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed double-precision floating-point values in xmm2 by packed double-precision floating-point values in xmm3/m128/m64bcst and write results to xmm1 subject to writemask k1."
"VDIVPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed double-precision floating-point values in ymm2 by packed double-precision floating-point values in ymm3/m256/m64bcst and write results to ymm1 subject to writemask k1."
"VDIVPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F.W1 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed double-precision floating-point values in zmm2 by packed double-precision FP values in zmm3/m512/m64bcst and write results to zmm1 subject to writemask k1."
"DIVPS xmm1, xmm2/m128","NP 0F 5E /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Divide packed single-precision floating-point values in xmm1 by packed single-precision floating-point values in xmm2/mem."
"VDIVPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/mem."
"VDIVPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/mem."
"VDIVPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed single-precision floating-point values in xmm2 by packed single-precision floating-point values in xmm3/m128/m32bcst and write results to xmm1 subject to writemask k1."
"VDIVPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 5E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed single-precision floating-point values in ymm2 by packed single-precision floating-point values in ymm3/m256/m32bcst and write results to ymm1 subject to writemask k1."
"VDIVPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.0F.W0 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Divide packed single-precision floating-point values in zmm2 by packed single-precision floating-point values in zmm3/m512/m32bcst and write results to zmm1 subject to writemask k1."
"DIVSD xmm1, xmm2/m64","F2 0F 5E /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Divide low double-precision floating-point value in xmm1 by low double-precision floating-point value in xmm2/m64."
"VDIVSD xmm1, xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64."
"VDIVSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.F2.0F.W1 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Divide low double-precision floating-point value in xmm2 by low double-precision floating-point value in xmm3/m64."
"DIVSS xmm1, xmm2/m32","F3 0F 5E /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Divide low single-precision floating-point value in xmm1 by low single-precision floating-point value in xmm2/m32."
"VDIVSS xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 5E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32."
"VDIVSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.NDS.LIG.F3.0F.W0 5E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Divide low single-precision floating-point value in xmm2 by low single-precision floating-point value in xmm3/m32."
"DPPD xmm1, xmm2/m128, imm8","66 0F 3A 41 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Selectively multiply packed DP floating-point values from xmm1 with packed DP floating-point values from xmm2, add and selectively store the packed DP floating-point values to xmm1."
"VDPPD xmm1,xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 41 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Selectively multiply packed DP floating-point values from xmm2 with packed DP floating-point values from xmm3, add and selectively store the packed DP floating-point values to xmm1."
"DPPS xmm1, xmm2/m128, imm8","66 0F 3A 40 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Selectively multiply packed SP floating-point values from xmm1 with packed SP floating-point values from xmm2, add and selectively store the packed SP floating-point values or zero values to xmm1."
"VDPPS xmm1,xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 40 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Multiply packed SP floating point values from xmm1 with packed SP floating point values from xmm2/mem selectively add and store to xmm1."
"VDPPS ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 40 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Multiply packed single-precision floating-point values from ymm2 with packed SP floating point values from ymm3/mem, selectively add pairs of elements and store to ymm1."
"EMMS","NP 0F 77","Valid","Valid","Valid","","NA","NA","NA","NA","","Set the x87 FPU tag word to empty."
"ENTER imm16, 0","C8 iw 00","Valid","Valid","Valid","","iw","imm8","NA","NA","","Create a stack frame for a procedure."
"ENTER imm16,1","C8 iw 01","Valid","Valid","Valid","","iw","imm8","NA","NA","","Create a stack frame with a nested pointer for a procedure."
"ENTER imm16, imm8","C8 iw ib","Valid","Valid","Valid","","iw","imm8","NA","NA","","Create a stack frame with nested pointers for a procedure."
"EXTRACTPS reg/m32, xmm1, imm8","66 0F 3A 17 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable."
"VEXTRACTPS reg/m32, xmm1, imm8","VEX.128.66.0F3A.WIG 17 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable."
"VEXTRACTPS reg/m32, xmm1, imm8","EVEX.128.66.0F3A.WIG 17 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Tuple1 Scalar","Extract one single-precision floating-point value from xmm1 at the offset specified by imm8 and store the result in reg or m32. Zero extend the results in 64-bit register if applicable."
"F2XM1","D9 F0","Valid","Valid","Valid","","","","","","","Replace ST(0) with (2 ST(0) â€“ 1)."
"FABS","D9 E1","Valid","Valid","Valid","","","","","","","Replace ST with its absolute value."
"FADD m32fp","D8 /0","Valid","Valid","Valid","","","","","","","Add m32fp to ST(0) and store result in ST(0)."
"FADD m64fp","DC /0","Valid","Valid","Valid","","","","","","","Add m64fp to ST(0) and store result in ST(0)."
"FADD ST(0), ST(i)","D8 C0+i","Valid","Valid","Valid","","","","","","","Add ST(0) to ST(i) and store result in ST(0)."
"FADD ST(i), ST(0)","DC C0+i","Valid","Valid","Valid","","","","","","","Add ST(i) to ST(0) and store result in ST(i)."
"FADDP ST(i), ST(0)","DE C0+i","Valid","Valid","Valid","","","","","","","Add ST(0) to ST(i), store result in ST(i), and pop the register stack."
"FADDP","DE C1","Valid","Valid","Valid","","","","","","","Add ST(0) to ST(1), store result in ST(1), and pop the register stack."
"FIADD m32int","DA /0","Valid","Valid","Valid","","","","","","","Add m32int to ST(0) and store result in ST(0)."
"FIADD m16int","DE /0","Valid","Valid","Valid","","","","","","","Add m16int to ST(0) and store result in ST(0).FADD/FADDP/FIADDâ€”Add This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
FBLD m80dec,DF /4,Valid,Valid,Valid,,,,,,,Convert BCD value to floating-point and push onto the FPU stack.
"FBSTP m80bcd","DF /6","Valid","Valid","Valid","","","","","","","Store ST(0) in m80bcd and pop ST(0)."
"FCHS","D9 E0","Valid","Valid","Valid","","","","","","","Complements sign of ST(0)."
"FCLEX","9B DB E2","Valid","Valid","Valid","","","","","","","Clear floating-point exception flags after checking for pending unmasked floating-point exceptions."
"FNCLEX","DB E2","Valid","Valid","Valid","","","","","","","Clear floating-point exception flags without checking for pending unmasked floating-point exceptions."
"FCMOVB ST(0), ST(i)","DA C0+i","Valid","Valid","Valid","","","","","","","Move if below (CF=1)."
"FCMOVE ST(0), ST(i)","DA C8+i","Valid","Valid","Valid","","","","","","","Move if equal (ZF=1)."
"FCMOVBE ST(0), ST(i)","DA D0+i","Valid","Valid","Valid","","","","","","","Move if below or equal (CF=1 or ZF=1)."
"FCMOVU ST(0), ST(i)","DA D8+i","Valid","Valid","Valid","","","","","","","Move if unordered (PF=1)."
"FCMOVNB ST(0), ST(i)","DB C0+i","Valid","Valid","Valid","","","","","","","Move if not below (CF=0)."
"FCMOVNE ST(0), ST(i)","DB C8+i","Valid","Valid","Valid","","","","","","","Move if not equal (ZF=0)."
"FCMOVNBE ST(0), ST(i)","DB D0+i","Valid","Valid","Valid","","","","","","","Move if not below or equal (CF=0 and ZF=0)."
"FCMOVNU ST(0), ST(i)","DB D8+i","Valid","Valid","Valid","","","","","","","Move if not unordered (PF=0)."
"FCOM m32fp","D8 /2","Valid","Valid","Valid","","","","","","","Compare ST(0) with m32fp."
"FCOM m64fp","DC /2","Valid","Valid","Valid","","","","","","","Compare ST(0) with m64fp."
"FCOM ST(i)","D8 D0+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i)."
"FCOM","D8 D1","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1)."
"FCOMP m32fp","D8 /3","Valid","Valid","Valid","","","","","","","Compare ST(0) with m32fp and pop register stack."
"FCOMP m64fp","DC /3","Valid","Valid","Valid","","","","","","","Compare ST(0) with m64fp and pop register stack."
"FCOMP ST(i)","D8 D8+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i) and pop register stack."
"FCOMP","D8 D9","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1) and pop register stack."
"FCOMPP","DE D9","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1) and pop register stack twice."
"FCOMI ST, ST(i)","DB F0+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i) and set status flags accordingly."
"FCOMIP ST, ST(i)","DF F0+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i), set status flags accordingly, and pop register stack."
"FUCOMI ST, ST(i)","DB E8+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i), check for ordered values, and set status flags accordingly."
"FUCOMIP ST, ST(i)","DF E8+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i), check for ordered values, set status flags accordingly, and pop register stack."
"FCOS","D9 FF","Valid","Valid","Valid","","","","","","","Replace ST(0) with its approximate cosine."
"FDECSTP","D9 F6","Valid","Valid","Valid","","","","","","","Decrement TOP field in FPU status word."
"FDIV m32fp","D8 /6","Valid","Valid","Valid","","","","","","","Divide ST(0) by m32fp and store result in ST(0)."
"FDIV m64fp","DC /6","Valid","Valid","Valid","","","","","","","Divide ST(0) by m64fp and store result in ST(0)."
"FDIV ST(0), ST(i)","D8 F0+i","Valid","Valid","Valid","","","","","","","Divide ST(0) by ST(i) and store result in ST(0)."
"FDIV ST(i), ST(0)","DC F8+i","Valid","Valid","Valid","","","","","","","Divide ST(i) by ST(0) and store result in ST(i)."
"FDIVP ST(i), ST(0)","DE F8+i","Valid","Valid","Valid","","","","","","","Divide ST(i) by ST(0), store result in ST(i), and pop the register stack."
"FDIVP","DE F9","Valid","Valid","Valid","","","","","","","Divide ST(1) by ST(0), store result in ST(1), and pop the register stack."
"FIDIV m32int","DA /6","Valid","Valid","Valid","","","","","","","Divide ST(0) by m32int and store result in ST(0)."
"FIDIV m16int","DE /6","Valid","Valid","Valid","","","","","","","Divide ST(0) by m16int and store result in ST(0).FDIV/FDIVP/FIDIVâ€”Divide This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
"FDIVR m32fp","D8 /7","Valid","Valid","Valid","","","","","","","Divide m32fp by ST(0) and store result in ST(0)."
"FDIVR m64fp","DC /7","Valid","Valid","Valid","","","","","","","Divide m64fp by ST(0) and store result in ST(0)."
"FDIVR ST(0), ST(i)","D8 F8+i","Valid","Valid","Valid","","","","","","","Divide ST(i) by ST(0) and store result in ST(0)."
"FDIVR ST(i), ST(0)","DC F0+i","Valid","Valid","Valid","","","","","","","Divide ST(0) by ST(i) and store result in ST(i)."
"FDIVRP ST(i), ST(0)","DE F0+i","Valid","Valid","Valid","","","","","","","Divide ST(0) by ST(i), store result in ST(i), and pop the register stack."
"FDIVRP","DE F1","Valid","Valid","Valid","","","","","","","Divide ST(0) by ST(1), store result in ST(1), and pop the register stack."
"FIDIVR m32int","DA /7","Valid","Valid","Valid","","","","","","","Divide m32int by ST(0) and store result in ST(0)."
"FIDIVR m16int","DE /7","Valid","Valid","Valid","","","","","","","Divide m16int by ST(0) and store result in ST(0).FDIVR/FDIVRP/FIDIVRâ€”Reverse Divide When the source operand is an integer 0, it is treated as a +0. This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
"FFREE ST(i)","DD C0+i","Valid","Valid","Valid","","","","","","","Sets tag for ST(i) to empty."
"FICOM m16int","DE /2","Valid","Valid","Valid","","","","","","","Compare ST(0) with m16int."
"FICOM m32int","DA /2","Valid","Valid","Valid","","","","","","","Compare ST(0) with m32int."
"FICOMP m16int","DE /3","Valid","Valid","Valid","","","","","","","Compare ST(0) with m16int and pop stack register."
"FICOMP m32int","DA /3","Valid","Valid","Valid","","","","","","","Compare ST(0) with m32int and pop stack register."
"FILD m16int","DF /0","Valid","Valid","Valid","","","","","","","Push m16int onto the FPU register stack."
"FILD m32int","DB /0","Valid","Valid","Valid","","","","","","","Push m32int onto the FPU register stack."
"FILD m64int","DF /5","Valid","Valid","Valid","","","","","","","Push m64int onto the FPU register stack.FILDâ€”Load Integer Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #MF If there is a pending x87 FPU exception. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FINCSTP","D9 F7","Valid","Valid","Valid","","","","","","","Increment the TOP field in the FPU status register."
"FINIT","9B DB E3","Valid","Valid","Valid","","","","","","","Initialize FPU after checking for pending unmasked floating-point exceptions."
"FNINIT","DB E3","Valid","Valid","Valid","","","","","","","Initialize FPU without checking for pending unmasked floating-point exceptions."
"FIST m16int","DF /2","Valid","Valid","Valid","","","","","","","Store ST(0) in m16int."
"FIST m32int","DB /2","Valid","Valid","Valid","","","","","","","Store ST(0) in m32int."
"FISTP m16int","DF /3","Valid","Valid","Valid","","","","","","","Store ST(0) in m16int and pop register stack."
"FISTP m32int","DB /3","Valid","Valid","Valid","","","","","","","Store ST(0) in m32int and pop register stack."
"FISTP m64int","DF /7","Valid","Valid","Valid","","","","","","","Store ST(0) in m64int and pop register stack."
"FISTTP m16int","DF /1","Valid","Valid","Valid","","","","","","","Store ST(0) in m16int with truncation."
"FISTTP m32int","DB /1","Valid","Valid","Valid","","","","","","","Store ST(0) in m32int with truncation."
"FISTTP m64int","DD /1","Valid","Valid","Valid","","","","","","","Store ST(0) in m64int with truncation."
"FLD m32fp","D9 /0","Valid","Valid","Valid","","","","","","","Push m32fp onto the FPU register stack."
"FLD m64fp","DD /0","Valid","Valid","Valid","","","","","","","Push m64fp onto the FPU register stack."
"FLD m80fp","DB /5","Valid","Valid","Valid","","","","","","","Push m80fp onto the FPU register stack."
"FLD ST(i)","D9 C0+i","Valid","Valid","Valid","","","","","","","Push ST(i) onto the FPU register stack.FLDâ€”Load Floating Point Value Protected Mode Exceptions #GP(0) If destination is located in a non-writable segment. If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. If the DS, ES, FS, or GS register is used to access memory and it contains a NULL segment selector. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used. Real-Address Mode Exceptions #GP If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #UD If the LOCK prefix is used. Virtual-8086 Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made. #UD If the LOCK prefix is used. Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #MF If there is a pending x87 FPU exception. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FLD1","D9 E8","Valid","Valid","Valid","","","","","","","Push +1.0 onto the FPU register stack."
"FLDL2T","D9 E9","Valid","Valid","Valid","","","","","","","Push log 10 onto the FPU register stack."
"FLDL2E","D9 EA","Valid","Valid","Valid","","","","","","","Push log e onto the FPU register stack."
"FLDPI","D9 EB","Valid","Valid","Valid","","","","","","","Push Ï€ onto the FPU register stack."
"FLDLG2","D9 EC","Valid","Valid","Valid","","","","","","","Push log 10 2 onto the FPU register stack."
"FLDLN2","D9 ED","Valid","Valid","Valid","","","","","","","Push log 2 onto the FPU register stack."
"FLDZ","D9 EE","Valid","Valid","Valid","","","","","","","Push +0.0 onto the FPU register stack."
"FLDCW m2byte","D9 /5","Valid","Valid","Valid","","","","","","","Load FPU control word from m2byte.FLDCWâ€”Load x87 FPU Control Word Virtual-8086 Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made. #UD If the LOCK prefix is used. Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #MF If there is a pending x87 FPU exception. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FLDENV m14/28byte","D9 /4","Valid","Valid","Valid","","","","","","","Load FPU environment from m14byte or m28byte.FLDENVâ€”Load x87 FPU Environment Protected Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. If the DS, ES, FS, or GS register is used to access memory and it contains a NULL segment selector. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used. Real-Address Mode Exceptions #GP If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #UD If the LOCK prefix is used. Virtual-8086 Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made. #UD If the LOCK prefix is used. Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #MF If there is a pending x87 FPU exception. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FMUL m32fp","D8 /1","Valid","Valid","Valid","","","","","","","Multiply ST(0) by m32fp and store result in ST(0)."
"FMUL m64fp","DC /1","Valid","Valid","Valid","","","","","","","Multiply ST(0) by m64fp and store result in ST(0)."
"FMUL ST(0), ST(i)","D8 C8+i","Valid","Valid","Valid","","","","","","","Multiply ST(0) by ST(i) and store result in ST(0)."
"FMUL ST(i), ST(0)","DC C8+i","Valid","Valid","Valid","","","","","","","Multiply ST(i) by ST(0) and store result in ST(i)."
"FMULP ST(i), ST(0)","DE C8+i","Valid","Valid","Valid","","","","","","","Multiply ST(i) by ST(0), store result in ST(i), and pop the register stack."
"FMULP","DE C9","Valid","Valid","Valid","","","","","","","Multiply ST(1) by ST(0), store result in ST(1), and pop the register stack."
"FIMUL m32int","DA /1","Valid","Valid","Valid","","","","","","","Multiply ST(0) by m32int and store result in ST(0)."
"FIMUL m16int","DE /1","Valid","Valid","Valid","","","","","","","Multiply ST(0) by m16int and store result in ST(0).FMUL/FMULP/FIMULâ€”Multiply This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
"FNOP","D9 D0","Valid","Valid","Valid","","","","","","","No operation is performed."
"FPATAN","D9 F3","Valid","Valid","Valid","","","","","","","Replace ST(1) with arctan(ST(1)/ST(0)) and pop the register stack."
"FPREM","D9 F8","Valid","Valid","Valid","","","","","","","Replace ST(0) with the remainder obtained from dividing ST(0) by ST(1)."
"FPREM1","D9 F5","Valid","Valid","Valid","","","","","","","Replace ST(0) with the IEEE remainder obtained from dividing ST(0) by ST(1)."
"FPTAN","D9 F2","Valid","Valid","Valid","","","","","","","Replace ST(0) with its approximate tangent and push 1 onto the FPU stack."
"FRNDINT","D9 FC","Valid","Valid","Valid","","","","","","","Round ST(0) to an integer."
"FRSTOR m94/108byte","DD /4","Valid","Valid","Valid","","","","","","","Load FPU state from m94byte or m108byte.FRSTORâ€”Restore x87 FPU State Protected Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. If the DS, ES, FS, or GS register is used to access memory and it contains a NULL segment selector. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used. Real-Address Mode Exceptions #GP If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #UD If the LOCK prefix is used. Virtual-8086 Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made. #UD If the LOCK prefix is used. Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FSAVE m94/108byte","9B DD /6","Valid","Valid","Valid","","","","","","","Store FPU state to m94byte or m108byte after checking for pending unmasked floating-point exceptions. Then re-initialize the FPU."
"FNSAVE m94/108byte","DD /6","Valid","Valid","Valid","","","","","","","Store FPU environment to m94byte or m108byte without checking for pending unmasked floating- point exceptions. Then re-initialize the FPU."
"FSCALE","D9 FD","Valid","Valid","Valid","","","","","","","Scale ST(0) by ST(1)."
"FSIN","D9 FE","Valid","Valid","Valid","","","","","","","Replace ST(0) with the approximate of its sine."
"FSINCOS","D9 FB","Valid","Valid","Valid","","","","","","","Compute the sine and cosine of ST(0); replace ST(0) with the approximate sine, and push the approximate cosine onto the register stack."
"FSQRT","D9 FA","Valid","Valid","Valid","","","","","","","Computes square root of ST(0) and stores the result in ST(0)."
"FST m32fp","D9 /2","Valid","Valid","Valid","","","","","","","Copy ST(0) to m32fp."
"FST m64fp","DD /2","Valid","Valid","Valid","","","","","","","Copy ST(0) to m64fp."
"FST ST(i)","DD D0+i","Valid","Valid","Valid","","","","","","","Copy ST(0) to ST(i)."
"FSTP m32fp","D9 /3","Valid","Valid","Valid","","","","","","","Copy ST(0) to m32fp and pop register stack."
"FSTP m64fp","DD /3","Valid","Valid","Valid","","","","","","","Copy ST(0) to m64fp and pop register stack."
"FSTP m80fp","DB /7","Valid","Valid","Valid","","","","","","","Copy ST(0) to m80fp and pop register stack."
"FSTP ST(i)","DD D8+i","Valid","Valid","Valid","","","","","","","Copy ST(0) to ST(i) and pop register stack.FST/FSTPâ€”Store Floating Point Value Floating-Point Exceptions #IS Stack underflow occurred. #IA If destination result is an SNaN value or unsupported format, except when the destination format is in double extended-precision floating-point format. #U Result is too small for the destination format. #O Result is too large for the destination format. #P Value cannot be represented exactly in destination format. Protected Mode Exceptions #GP(0) If the destination is located in a non-writable segment. If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. If the DS, ES, FS, or GS register is used to access memory and it contains a NULL segment selector. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used. Real-Address Mode Exceptions #GP If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #UD If the LOCK prefix is used. Virtual-8086 Mode Exceptions #GP(0) If a memory operand effective address is outside the CS, DS, ES, FS, or GS segment limit. #SS(0) If a memory operand effective address is outside the SS segment limit. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made. #UD If the LOCK prefix is used. Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions #SS(0) If a memory address referencing the SS segment is in a non-canonical form. #GP(0) If the memory address is in a non-canonical form. #NM CR0.EM[bit 2] or CR0.TS[bit 3] = 1. #MF If there is a pending x87 FPU exception. #PF(fault-code) If a page fault occurs. #AC(0) If alignment checking is enabled and an unaligned memory reference is made while the current privilege level is 3. #UD If the LOCK prefix is used."
"FSTCW m2byte","9B D9 /7","Valid","Valid","Valid","","","","","","","Store FPU control word to m2byte after checking for pending unmasked floating-point exceptions."
"FNSTCW m2byte","D9 /7","Valid","Valid","Valid","","","","","","","Store FPU control word to m2byte without checking for pending unmasked floating-point exceptions."
"FSTENV m14/28byte","9B D9 /6","Valid","Valid","Valid","","","","","","","Store FPU environment to m14byte or m28byte after checking for pending unmasked floating-point exceptions. Then mask all floating-point exceptions."
"FNSTENV m14/28byte","D9 /6","Valid","Valid","Valid","","","","","","","Store FPU environment to m14byte or m28byte without checking for pending unmasked floating- point exceptions. Then mask all floating- point exceptions."
"FSTSW m2byte","9B DD /7","Valid","Valid","Valid","","","","","","","Store FPU status word at m2byte after checking for pending unmasked floating-point exceptions."
"FSTSW AX","9B DF E0","Valid","Valid","Valid","","","","","","","Store FPU status word in AX register after checking for pending unmasked floating-point exceptions."
"FNSTSW m2byte","DD /7","Valid","Valid","Valid","","","","","","","Store FPU status word at m2byte without checking for pending unmasked floating-point exceptions."
"FNSTSW AX","DF E0","Valid","Valid","Valid","","","","","","","Store FPU status word in AX register without checking for pending unmasked floating-point exceptions."
"FSUB m32fp","D8 /4","Valid","Valid","Valid","","","","","","","Subtract m32fp from ST(0) and store result in ST(0)."
"FSUB m64fp","DC /4","Valid","Valid","Valid","","","","","","","Subtract m64fp from ST(0) and store result in ST(0)."
"FSUB ST(0), ST(i)","D8 E0+i","Valid","Valid","Valid","","","","","","","Subtract ST(i) from ST(0) and store result in ST(0)."
"FSUB ST(i), ST(0)","DC E8+i","Valid","Valid","Valid","","","","","","","Subtract ST(0) from ST(i) and store result in ST(i)."
"FSUBP ST(i), ST(0)","DE E8+i","Valid","Valid","Valid","","","","","","","Subtract ST(0) from ST(i), store result in ST(i), and pop register stack."
"FSUBP","DE E9","Valid","Valid","Valid","","","","","","","Subtract ST(0) from ST(1), store result in ST(1), and pop register stack."
"FISUB m32int","DA /4","Valid","Valid","Valid","","","","","","","Subtract m32int from ST(0) and store result in ST(0)."
"FISUB m16int","DE /4","Valid","Valid","Valid","","","","","","","Subtract m16int from ST(0) and store result in ST(0).FSUB/FSUBP/FISUBâ€”Subtract This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
"FSUBR m32fp","D8 /5","Valid","Valid","Valid","","","","","","","Subtract ST(0) from m32fp and store result in ST(0)."
"FSUBR m64fp","DC /5","Valid","Valid","Valid","","","","","","","Subtract ST(0) from m64fp and store result in ST(0)."
"FSUBR ST(0), ST(i)","D8 E8+i","Valid","Valid","Valid","","","","","","","Subtract ST(0) from ST(i) and store result in ST(0)."
"FSUBR ST(i), ST(0)","DC E0+i","Valid","Valid","Valid","","","","","","","Subtract ST(i) from ST(0) and store result in ST(i)."
"FSUBRP ST(i), ST(0)","DE E0+i","Valid","Valid","Valid","","","","","","","Subtract ST(i) from ST(0), store result in ST(i), and pop register stack."
"FSUBRP","DE E1","Valid","Valid","Valid","","","","","","","Subtract ST(1) from ST(0), store result in ST(1), and pop register stack."
"FISUBR m32int","DA /5","Valid","Valid","Valid","","","","","","","Subtract ST(0) from m32int and store result in ST(0)."
"FISUBR m16int","DE /5","Valid","Valid","Valid","","","","","","","Subtract ST(0) from m16int and store result in ST(0).FSUBR/FSUBRP/FISUBRâ€”Reverse Subtract This instructionâ€™s operation is the same in non-64-bit modes and 64-bit mode."
"FTST","D9 E4","Valid","Valid","Valid","","","","","","","Compare ST(0) with 0.0."
"FUCOM ST(i)","DD E0+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i)."
"FUCOM","DD E1","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1)."
"FUCOMP ST(i)","DD E8+i","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(i) and pop register stack."
"FUCOMP","DD E9","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1) and pop register stack."
"FUCOMPP","DA E9","Valid","Valid","Valid","","","","","","","Compare ST(0) with ST(1) and pop register stack twice."
"FXAM","D9 E5","Valid","Valid","Valid","","","","","","","Classify value or number in ST(0)."
"FXCH ST(i)","D9 C8+i","Valid","Valid","Valid","","","","","","","Exchange the contents of ST(0) and ST(i)."
"FXCH","D9 C9","Valid","Valid","Valid","","","","","","","Exchange the contents of ST(0) and ST(1).FXCHâ€”Exchange Register Contents Compatibility Mode Exceptions Same exceptions as in protected mode. 64-Bit Mode Exceptions Same exceptions as in protected mode."
"FXRSTOR m512byte","NP 0F AE /1","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte."
"FXRSTOR64 m512byte","NP REX.W + 0F AE /1","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Restore the x87 FPU, MMX, XMM, and MXCSR register state from m512byte."
"FXSAVE m512byte","NP 0F AE /0","Valid","Valid","Valid","","","","","","","Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte."
"FXSAVE64 m512byte","NP REX.W + 0F AE /0","Valid","Invalid","Invalid","","","","","","","Save the x87 FPU, MMX, XMM, and MXCSR register state to m512byte."
FXTRACT,D9 F4,Valid,Valid,Valid,,,,,,,"Separate value in ST(0) into exponent and significand, store exponent in ST(0), and push the significand onto the register stack."
"FYL2X","D9 F1","Valid","Valid","Valid","","","","","","","Replace ST(1) with (ST(1) âˆ— log ST(0)) and pop the register stack."
"FYL2XP1","D9 F9","Valid","Valid","Valid","","","","","","","Replace ST(1) with ST(1) âˆ— log (ST(0) + 1.0) and pop the register stack."
"HADDPD xmm1, xmm2/m128","66 0F 7C /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Horizontal add packed double-precision floating-point values from xmm2/m128 to xmm1."
"VHADDPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal add packed double-precision floating-point values from xmm2 and xmm3/mem."
"VHADDPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal add packed double-precision floating-point values from ymm2 and ymm3/mem."
"HADDPS xmm1, xmm2/m128","F2 0F 7C /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Horizontal add packed single-precision floating-point values from xmm2/m128 to xmm1."
"VHADDPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.F2.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal add packed single-precision floating-point values from xmm2 and xmm3/mem."
"VHADDPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.F2.0F.WIG 7C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal add packed single-precision floating-point values from ymm2 and ymm3/mem."
"HLT","F4","Valid","Valid","Valid","","NA","NA","NA","NA","","Halt"
"HSUBPD xmm1, xmm2/m128","66 0F 7D /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Horizontal subtract packed double-precision floating-point values from xmm2/m128 to xmm1."
"VHSUBPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal subtract packed double-precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal subtract packed double-precision floating-point values from ymm2 and ymm3/mem."
"HSUBPS xmm1, xmm2/m128","F2 0F 7D /r","Valid","Valid","Invalid","SSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Horizontal subtract packed single-precision floating-point values from xmm2/m128 to xmm1."
"VHSUBPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.F2.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal subtract packed single-precision floating-point values from xmm2 and xmm3/mem."
"VHSUBPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.F2.0F.WIG 7D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Horizontal subtract packed single-precision floating-point values from ymm2 and ymm3/mem."
"IDIV r/m8","F6 /7","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Signed divide AX by r/m8, with result stored in: AL â† Quotient, AH â† Remainder."
"IDIV r/m8","REX + F6 /7","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Signed divide AX by r/m8, with result stored in AL â† Quotient, AH â† Remainder."
"IDIV r/m16","F7 /7","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Signed divide DX:AX by r/m16, with result stored in AX â† Quotient, DX â† Remainder."
"IDIV r/m32","F7 /7","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Signed divide EDX:EAX by r/m32, with result stored in EAX â† Quotient, EDX â† Remainder."
"IDIV r/m64","REX.W + F7 /7","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Signed divide RDX:RAX by r/m64, with result stored in RAX â† Quotient, RDX â† Remainder."
"IMUL r/m8","F6 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","AXâ† AL âˆ— r/m byte."
"IMUL r/m16","F7 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","DX:AX â† AX âˆ— r/m word."
"IMUL r/m32","F7 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","EDX:EAX â† EAX âˆ— r/m32."
"IMUL r/m64","REX.W + F7 /5","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","RDX:RAX â† RAX âˆ— r/m64."
"IMUL r16, r/m16","0F AF /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","word register â† word register âˆ— r/m16."
"IMUL r32, r/m32","0F AF /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","doubleword register â† doubleword register âˆ— r/m32."
"IMUL r64, r/m64","REX.W + 0F AF /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Quadword register â† Quadword register âˆ— r/m64."
"IMUL r16, r/m16, imm8","6B /r ib","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","word register â† r/m16 âˆ— sign-extended immediate byte."
"IMUL r32, r/m32, imm8","6B /r ib","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","doubleword register â† r/m32 âˆ— sign- extended immediate byte."
"IMUL r64, r/m64, imm8","REX.W + 6B /r ib","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","Quadword register â† r/m64 âˆ— sign-extended immediate byte."
"IMUL r16, r/m16, imm16","69 /r iw","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","word register â† r/m16 âˆ— immediate word."
"IMUL r32, r/m32, imm32","69 /r id","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","doubleword register â† r/m32 âˆ— immediate doubleword."
"IMUL r64, r/m64, imm32","REX.W + 69 /r id","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","imm8/16/32","NA","","Quadword register â† r/m64 âˆ— immediate doubleword."
"IN AL, imm8","E4 ib","Valid","Valid","Valid","","","","","","","Input byte from imm8 I/O port address into AL."
"IN AX, imm8","E5 ib","Valid","Valid","Valid","","","","","","","Input word from imm8 I/O port address into AX."
"IN EAX, imm8","E5 ib","Valid","Valid","Valid","","","","","","","Input dword from imm8 I/O port address into EAX."
"IN AL,DX","EC","Valid","Valid","Valid","","","","","","","Input byte from I/O port in DX into AL."
"IN AX,DX","ED","Valid","Valid","Valid","","","","","","","Input word from I/O port in DX into AX."
"IN EAX,DX","ED","Valid","Valid","Valid","","","","","","","Input doubleword from I/O port in DX into EAX."
"INC r/m8","FE /0","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Increment r/m byte by 1."
"INC r/m8","REX + FE /0","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Increment r/m byte by 1."
"INC r/m16","FF /0","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Increment r/m word by 1."
"INC r/m32","FF /0","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Increment r/m doubleword by 1."
"INC r/m64","REX.W + FF /0","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Increment r/m quadword by 1."
"INC r16","40 +rw","Invalid","Valid","Valid","","opcode +rd (r, w)","NA","NA","NA","","Increment word register by 1."
"INC r32","40 +rd","Invalid","Valid","Valid","","opcode +rd (r, w)","NA","NA","NA","","Increment doubleword register by 1."
"INS m8, DX","6C","Valid","Valid","Valid","","NA","NA","NA","NA","","Input byte from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INS m16, DX","6D","Valid","Valid","Valid","","NA","NA","NA","NA","","Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INS m32, DX","6D","Valid","Valid","Valid","","NA","NA","NA","NA","","Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INSB","6C","Valid","Valid","Valid","","NA","NA","NA","NA","","Input byte from I/O port specified in DX into memory location specified with ES:(E)DI or RDI."
"INSW","6D","Valid","Valid","Valid","","NA","NA","NA","NA","","Input word from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INSD","6D","Valid","Valid","Valid","","NA","NA","NA","NA","","Input doubleword from I/O port specified in DX into memory location specified in ES:(E)DI or RDI."
"INSERTPS xmm1, xmm2/m32, imm8","66 0F 3A 21 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Insert a single-precision floating-point value selected by imm8 from xmm2/m32 into xmm1 at the specified destination element specified by imm8 and zero out destination elements in xmm1 as indicated in imm8."
"VINSERTPS xmm1, xmm2, xmm3/m32, imm8","VEX.NDS.128.66.0F3A.WIG 21 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Insert a single-precision floating-point value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8."
"VINSERTPS xmm1, xmm2, xmm3/m32, imm8","EVEX.NDS.128.66.0F3A.W0 21 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Insert a single-precision floating-point value selected by imm8 from xmm3/m32 and merge with values in xmm2 at the specified destination element specified by imm8 and write out the result and zero out destination elements in xmm1 as indicated in imm8."
"INT 3","CC","Valid","Valid","Valid","","NA","NA","NA","NA","","Interrupt 3â€”trap to debugger. CD ib INT imm8 I Valid Valid Interrupt vector specified by immediate byte."
"INTO","CE","Invalid","Valid","Valid","","NA","NA","NA","NA","","Interrupt 4â€”if overflow flag is 1."
"INVD","0F 08","Valid","Valid","Valid","","NA","NA","NA","NA","","Flush internal caches; initiate flushing of external caches."
"INVLPG m","0F 01/7","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Invalidate TLB entries for page containing m."
"INVPCID r32, m128","66 0F 38 82 /r","Invalid","Valid","Invalid","INVPCID","ModRM:reg (R)","ModRM:r/m (R)","NA","NA","","Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r32 and descrip-tor in m128."
"INVPCID r64, m128","66 0F 38 82 /r","Valid","Invalid","Invalid","INVPCID","ModRM:reg (R)","ModRM:r/m (R)","NA","NA","","Invalidates entries in the TLBs and paging-structure caches based on invalidation type in r64 and descrip-tor in m128."
"IRET","CF","Valid","Valid","Valid","","NA","NA","NA","NA","","Interrupt return (16-bit operand size)."
"IRETD","CF","Valid","Valid","Valid","","NA","NA","NA","NA","","Interrupt return (32-bit operand size)."
"IRETQ","REX.W + CF","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Interrupt return (64-bit operand size)."
"JA rel8","77 cb","Valid","Valid","Valid","","","","","","","Jump short if above (CF=0 and ZF=0)."
"JAE rel8","73 cb","Valid","Valid","Valid","","","","","","","Jump short if above or equal (CF=0)."
"JB rel8","72 cb","Valid","Valid","Valid","","","","","","","Jump short if below (CF=1)."
"JBE rel8","76 cb","Valid","Valid","Valid","","","","","","","Jump short if below or equal (CF=1 or ZF=1)."
"JC rel8","72 cb","Valid","Valid","Valid","","","","","","","Jump short if carry (CF=1)."
"JCXZ rel8","E3 cb","Invalid","Valid","Valid","","","","","","","Jump short if CX register is 0."
"JECXZ rel8","E3 cb","Valid","Valid","Valid","","","","","","","Jump short if ECX register is 0."
"JRCXZ rel8","E3 cb","Valid","Invalid","Invalid","","","","","","","Jump short if RCX register is 0."
"JE rel8","74 cb","Valid","Valid","Valid","","","","","","","Jump short if equal (ZF=1)."
"JG rel8","7F cb","Valid","Valid","Valid","","","","","","","Jump short if greater (ZF=0 and SF=OF)."
"JGE rel8","7D cb","Valid","Valid","Valid","","","","","","","Jump short if greater or equal (SF=OF)."
"JL rel8","7C cb","Valid","Valid","Valid","","","","","","","Jump short if less (SFâ‰  OF)."
"JLE rel8","7E cb","Valid","Valid","Valid","","","","","","","Jump short if less or equal (ZF=1 or SFâ‰  OF)."
"JNA rel8","76 cb","Valid","Valid","Valid","","","","","","","Jump short if not above (CF=1 or ZF=1)."
"JNAE rel8","72 cb","Valid","Valid","Valid","","","","","","","Jump short if not above or equal (CF=1)."
"JNB rel8","73 cb","Valid","Valid","Valid","","","","","","","Jump short if not below (CF=0)."
"JNBE rel8","77 cb","Valid","Valid","Valid","","","","","","","Jump short if not below or equal (CF=0 and ZF=0)."
"JNC rel8","73 cb","Valid","Valid","Valid","","","","","","","Jump short if not carry (CF=0)."
"JNE rel8","75 cb","Valid","Valid","Valid","","","","","","","Jump short if not equal (ZF=0)."
"JNG rel8","7E cb","Valid","Valid","Valid","","","","","","","Jump short if not greater (ZF=1 or SFâ‰  OF)."
"JNGE rel8","7C cb","Valid","Valid","Valid","","","","","","","Jump short if not greater or equal (SFâ‰  OF)."
"JNL rel8","7D cb","Valid","Valid","Valid","","","","","","","Jump short if not less (SF=OF)."
"JNLE rel8","7F cb","Valid","Valid","Valid","","","","","","","Jump short if not less or equal (ZF=0 and SF=OF)."
"JNO rel8","71 cb","Valid","Valid","Valid","","","","","","","Jump short if not overflow (OF=0)."
"JNP rel8","7B cb","Valid","Valid","Valid","","","","","","","Jump short if not parity (PF=0)."
"JNS rel8","79 cb","Valid","Valid","Valid","","","","","","","Jump short if not sign (SF=0)."
"JNZ rel8","75 cb","Valid","Valid","Valid","","","","","","","Jump short if not zero (ZF=0)."
"JO rel8","70 cb","Valid","Valid","Valid","","","","","","","Jump short if overflow (OF=1)."
"JP rel8","7A cb","Valid","Valid","Valid","","","","","","","Jump short if parity (PF=1)."
"JPE rel8","7A cb","Valid","Valid","Valid","","","","","","","Jump short if parity even (PF=1)."
"JPO rel8","7B cb","Valid","Valid","Valid","","","","","","","Jump short if parity odd (PF=0)."
"JS rel8","78 cb","Valid","Valid","Valid","","","","","","","Jump short if sign (SF=1)."
"JZ rel8","74 cb","Valid","Valid","Valid","","","","","","","Jump short if zero (ZF = 1)."
"JA rel16","0F 87 cw","Invalid","Valid","Valid","","","","","","","Jump near if above (CF=0 and ZF=0). Not supported in 64-bit mode."
"JA rel32","0F 87 cd","Valid","Valid","Valid","","","","","","","Jump near if above (CF=0 and ZF=0)."
"JAE rel16","0F 83 cw","Invalid","Valid","Valid","","","","","","","Jump near if above or equal (CF=0). Not supported in 64-bit mode.Jccâ€”Jump if Condition Is Met"
"JAE rel32","0F 83 cd","Valid","Valid","Valid","","","","","","","Jump near if above or equal (CF=0)."
"JB rel16","0F 82 cw","Invalid","Valid","Valid","","","","","","","Jump near if below (CF=1). Not supported in 64-bit mode."
"JB rel32","0F 82 cd","Valid","Valid","Valid","","","","","","","Jump near if below (CF=1)."
"JBE rel16","0F 86 cw","Invalid","Valid","Valid","","","","","","","Jump near if below or equal (CF=1 or ZF=1). Not supported in 64-bit mode."
"JBE rel32","0F 86 cd","Valid","Valid","Valid","","","","","","","Jump near if below or equal (CF=1 or ZF=1)."
"JC rel16","0F 82 cw","Invalid","Valid","Valid","","","","","","","Jump near if carry (CF=1). Not supported in 64-bit mode."
"JC rel32","0F 82 cd","Valid","Valid","Valid","","","","","","","Jump near if carry (CF=1)."
"JE rel16","0F 84 cw","Invalid","Valid","Valid","","","","","","","Jump near if equal (ZF=1). Not supported in 64-bit mode."
"JE rel32","0F 84 cd","Valid","Valid","Valid","","","","","","","Jump near if equal (ZF=1)."
"JZ rel16","0F 84 cw","Invalid","Valid","Valid","","","","","","","Jump near if 0 (ZF=1). Not supported in 64-bit mode."
"JZ rel32","0F 84 cd","Valid","Valid","Valid","","","","","","","Jump near if 0 (ZF=1)."
"JG rel16","0F 8F cw","Invalid","Valid","Valid","","","","","","","Jump near if greater (ZF=0 and SF=OF). Not supported in 64-bit mode."
"JG rel32","0F 8F cd","Valid","Valid","Valid","","","","","","","Jump near if greater (ZF=0 and SF=OF)."
"JGE rel16","0F 8D cw","Invalid","Valid","Valid","","","","","","","Jump near if greater or equal (SF=OF). Not supported in 64-bit mode."
"JGE rel32","0F 8D cd","Valid","Valid","Valid","","","","","","","Jump near if greater or equal (SF=OF)."
"JL rel16","0F 8C cw","Invalid","Valid","Valid","","","","","","","Jump near if less (SFâ‰  OF). Not supported in 64-bit mode."
"JL rel32","0F 8C cd","Valid","Valid","Valid","","","","","","","Jump near if less (SFâ‰  OF)."
"JLE rel16","0F 8E cw","Invalid","Valid","Valid","","","","","","","Jump near if less or equal (ZF=1 or SFâ‰  OF). Not supported in 64-bit mode."
"JLE rel32","0F 8E cd","Valid","Valid","Valid","","","","","","","Jump near if less or equal (ZF=1 or SFâ‰  OF)."
"JNA rel16","0F 86 cw","Invalid","Valid","Valid","","","","","","","Jump near if not above (CF=1 or ZF=1). Not supported in 64-bit mode."
"JNA rel32","0F 86 cd","Valid","Valid","Valid","","","","","","","Jump near if not above (CF=1 or ZF=1)."
"JNAE rel16","0F 82 cw","Invalid","Valid","Valid","","","","","","","Jump near if not above or equal (CF=1). Not supported in 64-bit mode."
"JNAE rel32","0F 82 cd","Valid","Valid","Valid","","","","","","","Jump near if not above or equal (CF=1)."
"JNB rel16","0F 83 cw","Invalid","Valid","Valid","","","","","","","Jump near if not below (CF=0). Not supported in 64-bit mode."
"JNB rel32","0F 83 cd","Valid","Valid","Valid","","","","","","","Jump near if not below (CF=0)."
"JNBE rel16","0F 87 cw","Invalid","Valid","Valid","","","","","","","Jump near if not below or equal (CF=0 and ZF=0). Not supported in 64-bit mode."
"JNBE rel32","0F 87 cd","Valid","Valid","Valid","","","","","","","Jump near if not below or equal (CF=0 and ZF=0)."
"JNC rel16","0F 83 cw","Invalid","Valid","Valid","","","","","","","Jump near if not carry (CF=0). Not supported in 64-bit mode."
"JNC rel32","0F 83 cd","Valid","Valid","Valid","","","","","","","Jump near if not carry (CF=0)."
"JNE rel16","0F 85 cw","Invalid","Valid","Valid","","","","","","","Jump near if not equal (ZF=0). Not supported in 64-bit mode."
"JNE rel32","0F 85 cd","Valid","Valid","Valid","","","","","","","Jump near if not equal (ZF=0)."
"JNG rel16","0F 8E cw","Invalid","Valid","Valid","","","","","","","Jump near if not greater (ZF=1 or SFâ‰  OF). Not supported in 64-bit mode."
"JNG rel32","0F 8E cd","Valid","Valid","Valid","","","","","","","Jump near if not greater (ZF=1 or SFâ‰  OF)."
"JNGE rel16","0F 8C cw","Invalid","Valid","Valid","","","","","","","Jump near if not greater or equal (SFâ‰  OF). Not supported in 64-bit mode."
"JNGE rel32","0F 8C cd","Valid","Valid","Valid","","","","","","","Jump near if not greater or equal (SFâ‰  OF)."
"JNL rel16","0F 8D cw","Invalid","Valid","Valid","","","","","","","Jump near if not less (SF=OF). Not supported in 64-bit mode."
"JNL rel32","0F 8D cd","Valid","Valid","Valid","","","","","","","Jump near if not less (SF=OF)."
"JNLE rel16","0F 8F cw","Invalid","Valid","Valid","","","","","","","Jump near if not less or equal (ZF=0 and SF=OF). Not supported in 64-bit mode."
"JNLE rel32","0F 8F cd","Valid","Valid","Valid","","","","","","","Jump near if not less or equal (ZF=0 and SF=OF)."
"JNO rel16","0F 81 cw","Invalid","Valid","Valid","","","","","","","Jump near if not overflow (OF=0). Not supported in 64-bit mode."
"JNO rel32","0F 81 cd","Valid","Valid","Valid","","","","","","","Jump near if not overflow (OF=0)."
"JNP rel16","0F 8B cw","Invalid","Valid","Valid","","","","","","","Jump near if not parity (PF=0). Not supported in 64-bit mode."
"JNP rel32","0F 8B cd","Valid","Valid","Valid","","","","","","","Jump near if not parity (PF=0)."
"JNS rel16","0F 89 cw","Invalid","Valid","Valid","","","","","","","Jump near if not sign (SF=0). Not supported in 64-bit mode."
"JNS rel32","0F 89 cd","Valid","Valid","Valid","","","","","","","Jump near if not sign (SF=0)."
"JNZ rel16","0F 85 cw","Invalid","Valid","Valid","","","","","","","Jump near if not zero (ZF=0). Not supported in 64-bit mode."
"JNZ rel32","0F 85 cd","Valid","Valid","Valid","","","","","","","Jump near if not zero (ZF=0)."
"JO rel16","0F 80 cw","Invalid","Valid","Valid","","","","","","","Jump near if overflow (OF=1). Not supported in 64-bit mode."
"JO rel32","0F 80 cd","Valid","Valid","Valid","","","","","","","Jump near if overflow (OF=1)."
"JP rel16","0F 8A cw","Invalid","Valid","Valid","","","","","","","Jump near if parity (PF=1). Not supported in 64-bit mode."
"JP rel32","0F 8A cd","Valid","Valid","Valid","","","","","","","Jump near if parity (PF=1)."
"JPE rel16","0F 8A cw","Invalid","Valid","Valid","","","","","","","Jump near if parity even (PF=1). Not supported in 64-bit mode."
"JPE rel32","0F 8A cd","Valid","Valid","Valid","","","","","","","Jump near if parity even (PF=1)."
"JPO rel16","0F 8B cw","Invalid","Valid","Valid","","","","","","","Jump near if parity odd (PF=0). Not supported in 64-bit mode."
"JPO rel32","0F 8B cd","Valid","Valid","Valid","","","","","","","Jump near if parity odd (PF=0)."
"JS rel16","0F 88 cw","Invalid","Valid","Valid","","","","","","","Jump near if sign (SF=1). Not supported in 64- bit mode."
"JMP rel8","EB cb","Valid","Valid","Valid","","","","","","","Jump short, RIP = RIP + 8-bit displacement sign extended to 64-bits"
"JMP rel16","E9 cw","Invalid","Valid","Valid","","","","","","","Jump near, relative, displacement relative to next instruction. Not supported in 64-bit mode."
"JMP rel32","E9 cd","Valid","Valid","Invalid","","","","","","","Jump near, relative, RIP = RIP + 32-bit displacement sign extended to 64-bits"
"JMP r/m16","FF /4","Invalid","Valid","Valid","","","","","","","Jump near, absolute indirect, address = zero- extended r/m16. Not supported in 64-bit mode."
"JMP r/m32","FF /4","Invalid","Valid","Invalid","","","","","","","Jump near, absolute indirect, address given in r/m32. Not supported in 64-bit mode."
"JMP r/m64","FF /4","Valid","Invalid","Invalid","","","","","","","Jump near, absolute indirect, RIP = 64-Bit offset from register or memory"
"JMP ptr16:16","EA cd","Invalid","Valid","Valid","","","","","","","Jump far, absolute, address given in operand"
"JMP ptr16:32","EA cp","Invalid","Valid","Invalid","","","","","","","Jump far, absolute, address given in operand"
"JMP m16:16","FF /5","Valid","Valid","Valid","","","","","","","Jump far, absolute indirect, address given in m16:16"
"JMP m16:32","FF /5","Valid","Valid","Invalid","","","","","","","Jump far, absolute indirect, address given in m16:32."
"JMP m16:64","REX.W + FF /5","Valid","Invalid","Invalid","","","","","","","Jump far, absolute indirect, address given in m16:64."
"KADDW k1, k2, k3","VEX.L1.0F.W0 4A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 16 bits masks in k2 and k3 and place result in k1."
"KADDB k1, k2, k3","VEX.L1.66.0F.W0 4A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 8 bits masks in k2 and k3 and place result in k1."
"KADDQ k1, k2, k3","VEX.L1.0F.W1 4A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 64 bits masks in k2 and k3 and place result in k1."
"KADDD k1, k2, k3","VEX.L1.66.0F.W1 4A /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Add 32 bits masks in k2 and k3 and place result in k1."
"KANDNW k1, k2, k3","VEX.NDS.L1.0F.W0 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 16 bits masks k2 and k3 and place result in k1."
"KANDNB k1, k2, k3","VEX.L1.66.0F.W0 42 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 8 bits masks k1 and k2 and place result in k1."
"KANDNQ k1, k2, k3","VEX.L1.0F.W1 42 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 64 bits masks k2 and k3 and place result in k1."
"KANDND k1, k2, k3","VEX.L1.66.0F.W1 42 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND NOT 32 bits masks k2 and k3 and place result in k1."
"KANDW k1, k2, k3","VEX.NDS.L1.0F.W0 41 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 16 bits masks k2 and k3 and place result in k1."
"KANDB k1, k2, k3","VEX.L1.66.0F.W0 41 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 8 bits masks k2 and k3 and place result in k1."
"KANDQ k1, k2, k3","VEX.L1.0F.W1 41 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 64 bits masks k2 and k3 and place result in k1."
"KANDD k1, k2, k3","VEX.L1.66.0F.W1 41 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise AND 32 bits masks k2 and k3 and place result in k1."
"KMOVW k1, k2/m16","VEX.L0.0F.W0 90 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 16 bits mask from k2/m16 and store the result in k1."
"KMOVB k1, k2/m8","VEX.L0.66.0F.W0 90 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 8 bits mask from k2/m8 and store the result in k1."
"KMOVQ k1, k2/m64","VEX.L0.0F.W1 90 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 64 bits mask from k2/m64 and store the result in k1."
"KMOVD k1, k2/m32","VEX.L0.66.0F.W1 90 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","","","","Move 32 bits mask from k2/m32 and store the result in k1."
"KMOVW m16, k1","VEX.L0.0F.W0 91 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 16 bits mask from k1 and store the result in m16."
"KMOVB m8, k1","VEX.L0.66.0F.W0 91 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 8 bits mask from k1 and store the result in m8."
"KMOVQ m64, k1","VEX.L0.0F.W1 91 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 64 bits mask from k1 and store the result in m64."
"KMOVD m32, k1","VEX.L0.66.0F.W1 91 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w, ModRM:[7:6] must not be 11b)","ModRM:reg (r)","","","","Move 32 bits mask from k1 and store the result in m32."
"KMOVW k1, r32","VEX.L0.0F.W0 92 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 16 bits mask from r32 to k1."
"KMOVB k1, r32","VEX.L0.66.0F.W0 92 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 8 bits mask from r32 to k1."
"KMOVQ k1, r64","VEX.L0.F2.0F.W1 92 /r","Valid","Invalid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 64 bits mask from r64 to k1."
"KMOVD k1, r32","VEX.L0.F2.0F.W0 92 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 32 bits mask from r32 to k1."
"KMOVW r32, k1","VEX.L0.0F.W0 93 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 16 bits mask from k1 to r32."
"KMOVB r32, k1","VEX.L0.66.0F.W0 93 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 8 bits mask from k1 to r32."
"KMOVQ r64, k1","VEX.L0.F2.0F.W1 93 /r","Valid","Invalid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 64 bits mask from k1 to r64."
"KMOVD r32, k1","VEX.L0.F2.0F.W0 93 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Move 32 bits mask from k1 to r32."
"KNOTW k1, k2","VEX.L0.0F.W0 44 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 16 bits mask k2."
"KNOTB k1, k2","VEX.L0.66.0F.W0 44 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 8 bits mask k2."
"KNOTQ k1, k2","VEX.L0.0F.W1 44 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 64 bits mask k2."
"KNOTD k1, k2","VEX.L0.66.0F.W1 44 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise NOT of 32 bits mask k2."
"KORTESTW k1, k2","VEX.L0.0F.W0 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 16 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTB k1, k2","VEX.L0.66.0F.W0 98 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 8 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTQ k1, k2","VEX.L0.0F.W1 98 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 64 bits masks k1 and k2 and update ZF and CF accordingly."
"KORTESTD k1, k2","VEX.L0.66.0F.W1 98 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Bitwise OR 32 bits masks k1 and k2 and update ZF and CF accordingly."
"KORW k1, k2, k3","VEX.NDS.L1.0F.W0 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 16 bits masks k2 and k3 and place result in k1."
"KORB k1, k2, k3","VEX.L1.66.0F.W0 45 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 8 bits masks k2 and k3 and place result in k1."
"KORQ k1, k2, k3","VEX.L1.0F.W1 45 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 64 bits masks k2 and k3 and place result in k1."
"KORD k1, k2, k3","VEX.L1.66.0F.W1 45 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise OR 32 bits masks k2 and k3 and place result in k1."
"KSHIFTLW k1, k2, imm8","VEX.L0.66.0F3A.W1 32 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift left 16 bits in k2 by immediate and write result in k1."
"KSHIFTLB k1, k2, imm8","VEX.L0.66.0F3A.W0 32 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift left 8 bits in k2 by immediate and write result in k1."
"KSHIFTLQ k1, k2, imm8","VEX.L0.66.0F3A.W1 33 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift left 64 bits in k2 by immediate and write result in k1."
"KSHIFTLD k1, k2, imm8","VEX.L0.66.0F3A.W0 33 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift left 32 bits in k2 by immediate and write result in k1."
"KSHIFTRW k1, k2, imm8","VEX.L0.66.0F3A.W1 30 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift right 16 bits in k2 by immediate and write result in k1."
"KSHIFTRB k1, k2, imm8","VEX.L0.66.0F3A.W0 30 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift right 8 bits in k2 by immediate and write result in k1."
"KSHIFTRQ k1, k2, imm8","VEX.L0.66.0F3A.W1 31 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift right 64 bits in k2 by immediate and write result in k1."
"KSHIFTRD k1, k2, imm8","VEX.L0.66.0F3A.W0 31 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","Imm8","","","Shift right 32 bits in k2 by immediate and write result in k1."
"KTESTW k1, k2","VEX.L0.0F.W0 99 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 16 bits mask register sources."
"KTESTB k1, k2","VEX.L0.66.0F.W0 99 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 8 bits mask register sources."
"KTESTQ k1, k2","VEX.L0.0F.W1 99 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 64 bits mask register sources."
"KTESTD k1, k2","VEX.L0.66.0F.W1 99 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","","Set ZF and CF depending on sign bit AND and ANDN of 32 bits mask register sources."
"KUNPCKBW k1, k2, k3","VEX.NDS.L1.66.0F.W0 4B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 8 bits masks in k2 and k3 and write word result in k1."
"KUNPCKWD k1, k2, k3","VEX.NDS.L1.0F.W0 4B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 16 bits in k2 and k3 and write double-word result in k1."
"KUNPCKDQ k1, k2, k3","VEX.NDS.L1.0F.W1 4B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Unpack and interleave 32 bits masks in k2 and k3 and write quadword result in k1."
"KXNORW k1, k2, k3","VEX.NDS.L1.0F.W0 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 16 bits masks k2 and k3 and place result in k1."
"KXNORB k1, k2, k3","VEX.L1.66.0F.W0 46 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 8 bits masks k2 and k3 and place result in k1."
"KXNORQ k1, k2, k3","VEX.L1.0F.W1 46 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 64 bits masks k2 and k3 and place result in k1."
"KXNORD k1, k2, k3","VEX.L1.66.0F.W1 46 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XNOR 32 bits masks k2 and k3 and place result in k1."
"KXORW k1, k2, k3","VEX.NDS.L1.0F.W0 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 16 bits masks k2 and k3 and place result in k1."
"KXORB k1, k2, k3","VEX.L1.66.0F.W0 47 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 8 bits masks k2 and k3 and place result in k1."
"KXORQ k1, k2, k3","VEX.L1.0F.W1 47 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 64 bits masks k2 and k3 and place result in k1."
"KXORD k1, k2, k3","VEX.L1.66.0F.W1 47 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r, ModRM:[7:6] must be 11b)","","","Bitwise XOR 32 bits masks k2 and k3 and place result in k1."
"LAHF","9F","Invalid","Valid","Valid","","NA","NA","NA","NA","","Load: AH â† EFLAGS(SF:ZF:0:AF:0:PF:1:CF)."
"LAR r16, r16/m16","0F 02 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","r16 â† access rights referenced by r16/m16"
"LAR reg, r32/m16","0F 02 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","reg â† access rights referenced by r32/m16"
"LDDQU xmm1, mem","F2 0F F0 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load unaligned data from mem and return double quadword in xmm1."
"VLDDQU xmm1, m128","VEX.128.F2.0F.WIG F0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load unaligned packed integer values from mem to xmm1."
"VLDDQU ymm1, m256","VEX.256.F2.0F.WIG F0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load unaligned packed integer values from mem to ymm1."
"LDMXCSR m32","NP 0F AE /2","Valid","Valid","Invalid","SSE","ModRM:r/m (r)","NA","NA","NA","","Load MXCSR register from m32."
"VLDMXCSR m32","VEX.LZ.0F.WIG AE /2","Valid","Valid","Invalid","AVX","ModRM:r/m (r)","NA","NA","NA","","Load MXCSR register from m32."
"LDS r16,m16:16","C5 /r","Invalid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load DS:r16 with far pointer from memory."
"LDS r32,m16:32","C5 /r","Invalid","Valid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load DS:r32 with far pointer from memory."
"LSS r16,m16:16","0F B2 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load SS:r16 with far pointer from memory."
"LSS r32,m16:32","0F B2 /r","Valid","Valid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load SS:r32 with far pointer from memory."
"LSS r64,m16:64","REX + 0F B2 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load SS:r64 with far pointer from memory."
"LES r16,m16:16","C4 /r","Invalid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load ES:r16 with far pointer from memory."
"LES r32,m16:32","C4 /r","Invalid","Valid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load ES:r32 with far pointer from memory."
"LFS r16,m16:16","0F B4 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load FS:r16 with far pointer from memory."
"LFS r32,m16:32","0F B4 /r","Valid","Valid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load FS:r32 with far pointer from memory."
"LFS r64,m16:64","REX + 0F B4 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load FS:r64 with far pointer from memory."
"LGS r16,m16:16","0F B5 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load GS:r16 with far pointer from memory."
"LGS r32,m16:32","0F B5 /r","Valid","Valid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load GS:r32 with far pointer from memory."
"LGS r64,m16:64","REX + 0F B5 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load GS:r64 with far pointer from memory."
"LEA r16,m","8D /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Store effective address for m in register r16."
"LEA r32,m","8D /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Store effective address for m in register r32."
"LEA r64,m","REX.W + 8D /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Store effective address for m in register r64."
"LEAVE","C9","Valid","Valid","Valid","","NA","NA","NA","NA","","Set SP to BP, then pop BP."
"LEAVE","C9","Invalid","Valid","Valid","","NA","NA","NA","NA","","Set ESP to EBP, then pop EBP."
"LEAVE","C9","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Set RSP to RBP, then pop RBP."
"LFENCE","NP 0F AE E8","Valid","Valid","Valid","","NA","NA","NA","NA","","Serializes load operations."
"LGDT m16&32","0F 01 /2","Invalid","Valid","Valid","","","","","","","Load m into GDTR."
"LIDT m16&32","0F 01 /3","Invalid","Valid","Valid","","","","","","","Load m into IDTR."
"LGDT m16&64","0F 01 /2","Valid","Invalid","Invalid","","","","","","","Load m into GDTR."
"LIDT m16&64","0F 01 /3","Valid","Invalid","Invalid","","","","","","","Load m into IDTR."
"LLDT r/m16","0F 00 /2","Valid","Valid","Valid","","","","","","","Load segment selector r/m16 into LDTR."
"LMSW r/m16","0F 01 /6","Valid","Valid","Valid","","","","","","","Loads r/m16 in machine status word of CR0."
"LOCK","F0","Valid","Valid","Valid","","NA","NA","NA","NA","","Asserts LOCK# signal for duration of the accompanying instruction."
"LODS m8","AC","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL."
"LODS m16","AD","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX."
"LODS m32","AD","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX."
"LODS m64","REX.W + AD","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Load qword at address (R)SI into RAX."
"LODSB","AC","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load byte at address DS:(E)SI into AL. For 64-bit mode load byte at address (R)SI into AL."
"LODSW","AD","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load word at address DS:(E)SI into AX. For 64-bit mode load word at address (R)SI into AX."
"LODSD","AD","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Load dword at address DS:(E)SI into EAX. For 64-bit mode load dword at address (R)SI into EAX."
"LODSQ","REX.W + AD","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Load qword at address (R)SI into RAX."
"LOOP rel8","E2 cb","Valid","Valid","Valid","","","","","","","Decrement count; jump short if count â‰  0."
"LOOPE rel8","E1 cb","Valid","Valid","Valid","","","","","","","Decrement count; jump short if count â‰  0 and ZF = 1."
"LOOPNE rel8","E0 cb","Valid","Valid","Valid","","","","","","","Decrement count; jump short if count â‰  0 and ZF = 0."
"LSL r16, r16/m16","0F 03 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load: r16 â† segment limit, selector r16/m16."
"LSL r32, r32/m16","0F 03 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load: r32 â† segment limit, selector r32/m16."
"LSL r64, r32/m16","REX.W + 0F 03 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Load: r64 â† segment limit, selector r32/m16"
"LTR r/m16","0F 00 /3","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Load r/m16 into task register."
"LZCNT r16, r/m16","F3 0F BD /r","Valid","Valid","Valid","LZCNT","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of leading zero bits in r/m16, return result in r16."
"LZCNT r32, r/m32","F3 0F BD /r","Valid","Valid","Valid","LZCNT","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of leading zero bits in r/m32, return result in r32."
"LZCNT r64, r/m64","F3 REX.W 0F BD /r","Valid","Invalid","Invalid","LZCNT","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of leading zero bits in r/m64, return result in r64."
"MASKMOVDQU xmm1, xmm2","66 0F F7 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI."
"VMASKMOVDQU xmm1, xmm2","VEX.128.66.0F.WIG F7 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Selectively write bytes from xmm1 to memory location using the byte mask in xmm2. The default memory location is specified by DS:DI/EDI/RDI."
"MASKMOVQ mm1, mm2","NP 0F F7 /r","Valid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Selectively write bytes from mm1 to memory location using the byte mask in mm2. The default memory location is specified by DS:DI/EDI/RDI."
"MAXPD xmm1, xmm2/m128","66 0F 5F /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the maximum double-precision floating-point values between xmm1 and xmm2/m128."
"VMAXPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum double-precision floating-point values between xmm2 and xmm3/m128."
"VMAXPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256."
"VMAXPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMAXPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMAXPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","EVEX.NDS.512.66.0F.W1 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"MAXPS xmm1, xmm2/m128","NP 0F 5F /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the maximum single-precision floating-point values between xmm1 and xmm2/mem."
"VMAXPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum single-precision floating-point values between xmm2 and xmm3/mem."
"VMAXPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum single-precision floating-point values between ymm2 and ymm3/mem."
"VMAXPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMAXPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 5F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMAXPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","EVEX.NDS.512.0F.W0 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the maximum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"MAXSD xmm1, xmm2/m64","F2 0F 5F /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the maximum scalar double-precision floating-point value between xmm2/m64 and xmm1."
"VMAXSD xmm1, xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMAXSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","EVEX.NDS.LIG.F2.0F.W1 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Return the maximum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"MAXSS xmm1, xmm2/m32","F3 0F 5F /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the maximum scalar single-precision floating-point value between xmm2/m32 and xmm1."
"VMAXSS xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 5F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMAXSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.NDS.LIG.F3.0F.W0 5F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Return the maximum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"MFENCE","NP 0F AE F0","Valid","Valid","Valid","","NA","NA","NA","NA","","Serializes load and store operations."
"MINPD xmm1, xmm2/m128","66 0F 5D /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the minimum double-precision floating-point values between xmm1 and xmm2/mem"
"VMINPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum double-precision floating-point values between xmm2 and xmm3/mem."
"VMINPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum packed double-precision floating-point values between ymm2 and ymm3/mem."
"VMINPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed double-precision floating-point values between xmm2 and xmm3/m128/m64bcst and store result in xmm1 subject to writemask k1."
"VMINPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed double-precision floating-point values between ymm2 and ymm3/m256/m64bcst and store result in ymm1 subject to writemask k1."
"VMINPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}","EVEX.NDS.512.66.0F.W1 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed double-precision floating-point values between zmm2 and zmm3/m512/m64bcst and store result in zmm1 subject to writemask k1."
"MINPS xmm1, xmm2/m128","NP 0F 5D /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the minimum single-precision floating-point values between xmm1 and xmm2/mem."
"VMINPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum single-precision floating-point values between xmm2 and xmm3/mem."
"VMINPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum single double-precision floating-point values between ymm2 and ymm3/mem."
"VMINPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed single-precision floating-point values between xmm2 and xmm3/m128/m32bcst and store result in xmm1 subject to writemask k1."
"VMINPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 5D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed single-precision floating-point values between ymm2 and ymm3/m256/m32bcst and store result in ymm1 subject to writemask k1."
"VMINPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}","EVEX.NDS.512.0F.W0 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the minimum packed single-precision floating-point values between zmm2 and zmm3/m512/m32bcst and store result in zmm1 subject to writemask k1."
"MINSD xmm1, xmm2/m64","F2 0F 5D /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the minimum scalar double-precision floating-point value between xmm2/m64 and xmm1."
"VMINSD xmm1, xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"VMINSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","EVEX.NDS.LIG.F2.0F.W1 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Return the minimum scalar double-precision floating-point value between xmm3/m64 and xmm2."
"MINSS xmm1,xmm2/m32","F3 0F 5D /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the minimum scalar single-precision floating-point value between xmm2/m32 and xmm1."
"VMINSS xmm1,xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 5D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"VMINSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.NDS.LIG.F3.0F.W0 5D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Return the minimum scalar single-precision floating-point value between xmm3/m32 and xmm2."
"MONITOR","0F 01 C8","Valid","Valid","Valid","","NA","NA","NA","NA","","Sets up a linear address range to be monitored by hardware and activates the monitor. The address range should be a write- back memory caching type. The address is DS:RAX/EAX/AX."
"MOV r/m8,r8",88 /r,Valid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move r8 to r/m8.
"MOV r/m8,r8",REX + 88 /r,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move r8 to r/m8.
"MOV r/m16,r16",89 /r,Valid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move r16 to r/m16.
"MOV r/m32,r32",89 /r,Valid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move r32 to r/m32.
"MOV r/m64,r64",REX.W + 89 /r,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move r64 to r/m64.
"MOV r8,r/m8",8A /r,Valid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m8 to r8.
"MOV r8,r/m8",REX + 8A /r,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m8 to r8.
"MOV r16,r/m16",8B /r,Valid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m16 to r16.
"MOV r32,r/m32",8B /r,Valid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m32 to r32.
"MOV r64,r/m64",REX.W + 8B /r,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m64 to r64.
"MOV r/m16,Sreg",8C /r,Valid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move segment register to r/m16.
"MOV r/m64,Sreg",REX.W + 8C /r,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move zero extended 16-bit segment register to r/m64.
"MOV Sreg,r/m16",8E /r,Valid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r/m16 to segment register.
"MOV Sreg,r/m64",REX.W + 8E /r,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move lower 16 bits of r/m64 to segment register.
"MOV AL,moffs8",A0,Valid,Valid,Valid,,AL/AX/EAX/RAX,Moffs,NA,NA,,Move byte at (seg:offset) to AL.
"MOV AL,moffs8",REX.W + A0,Valid,Invalid,Invalid,,AL/AX/EAX/RAX,Moffs,NA,NA,,Move byte at (offset) to AL.
"MOV AX,moffs16",A1,Valid,Valid,Valid,,AL/AX/EAX/RAX,Moffs,NA,NA,,Move word at (seg:offset) to AX.
"MOV EAX,moffs32",A1,Valid,Valid,Valid,,AL/AX/EAX/RAX,Moffs,NA,NA,,Move doubleword at (seg:offset) to EAX.
"MOV RAX,moffs64",REX.W + A1,Valid,Invalid,Invalid,,AL/AX/EAX/RAX,Moffs,NA,NA,,Move quadword at (offset) to RAX.
"MOV moffs8,AL",A2,Valid,Valid,Valid,,Moffs (w),AL/AX/EAX/RAX,NA,NA,,Move AL to (seg:offset).
"MOV moffs8,AL",REX.W + A2,Valid,Invalid,Invalid,,Moffs (w),AL/AX/EAX/RAX,NA,NA,,Move AL to (offset).
"MOV moffs16,AX",A3,Valid,Valid,Valid,,Moffs (w),AL/AX/EAX/RAX,NA,NA,,Move AX to (seg:offset).
"MOV moffs32,EAX",A3,Valid,Valid,Valid,,Moffs (w),AL/AX/EAX/RAX,NA,NA,,Move EAX to (seg:offset).
"MOV moffs64,RAX",REX.W + A3,Valid,Invalid,Invalid,,Moffs (w),AL/AX/EAX/RAX,NA,NA,,Move RAX to (offset).
"MOV r8,imm8",B0 +rb ib,Valid,Valid,Valid,,opcode +rd (w),imm8/16/32/64,NA,NA,,Move imm8 to r8.
"MOV r8,imm8",REX + B0 +rb ib,Valid,Invalid,Invalid,,opcode +rd (w),imm8/16/32/65,NA,NA,,Move imm8 to r8.
"MOV r16,imm16",B8 +rw iw,Valid,Valid,Valid,,opcode +rd (w),imm8/16/32/66,NA,NA,,Move imm16 to r16.
"MOV r32,imm32",B8 +rd id,Valid,Valid,Valid,,opcode +rd (w),imm8/16/32/67,NA,NA,,Move imm32 to r32.
"MOV r64,imm64",REX.W + B8 +rd io,Valid,Invalid,Invalid,,opcode +rd (w),imm8/16/32/68,NA,NA,,Move imm64 to r64.
"MOV r/m8,imm8",C6 /0 ib,Valid,Valid,Valid,,ModRM:r/m (w),imm8/16/32/69,NA,NA,,Move imm8 to r/m8.
"MOV r/m8,imm8",REX + C6 /0 ib,Valid,Invalid,Invalid,,ModRM:r/m (w),imm8/16/32/70,NA,NA,,Move imm8 to r/m8.
"MOV r/m16,imm16",C7 /0 iw,Valid,Valid,Valid,,ModRM:r/m (w),imm8/16/32/71,NA,NA,,Move imm16 to r/m16.
"MOV r/m32,imm32",C7 /0 id,Valid,Valid,Valid,,ModRM:r/m (w),imm8/16/32/72,NA,NA,,Move imm32 to r/m32.
"MOV r/m64,imm32",REX.W + C7 /0 id,Valid,Invalid,Invalid,,ModRM:r/m (w),imm8/16/32/73,NA,NA,,Move imm32 sign extended to 64-bits to r/m64.
"MOV r32, CR0-CR7",0F 20 /r,Invalid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move control register to r32.
"MOV r64, CR0-CR7",0F 20 /r,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move extended control register to r64.
"MOV r64, CR8",REX.R + 0F 20 /0,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move extended CR8 to r64.
"MOV CR0-CR7, r32",0F 22 /r,Invalid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r32 to control register.
"MOV CR0-CR7, r64",0F 22 /r,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r64 to extended control register.
"MOV CR8, r64",REX.R + 0F 22 /0,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r64 to extended CR8.
"MOV r32, DR0-DR7",0F 21 /r,Invalid,Valid,Valid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move debug register to r32.
"MOV r64, DR0-DR7",0F 21 /r,Valid,Invalid,Invalid,,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move extended debug register to r64.
"MOV DR0-DR7, r32",0F 23 /r,Invalid,Valid,Valid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r32 to debug register.
"MOV DR0-DR7, r64",0F 23 /r,Valid,Invalid,Invalid,,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move r64 to extended debug register.
"MOVAPD xmm1, xmm2/m128","66 0F 28 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from xmm2/mem to xmm1."
"MOVAPD xmm2/m128, xmm1","66 0F 29 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from xmm2/mem to xmm1."
"VMOVAPD xmm2/m128, xmm1","VEX.128.66.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from ymm2/mem to ymm1."
"VMOVAPD ymm2/m256, ymm1","VEX.256.66.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed double-precision floating-point values from ymm1 to ymm2/mem."
"VMOVAPD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPD xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPD ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPD zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"MOVAPS xmm1, xmm2/m128","NP 0F 28 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from xmm2/mem to xmm1."
"MOVAPS xmm2/m128, xmm1","NP 0F 29 /r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPS xmm1, xmm2/m128","VEX.128.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from xmm2/mem to xmm1."
"VMOVAPS xmm2/m128, xmm1","VEX.128.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from xmm1 to xmm2/mem."
"VMOVAPS ymm1, ymm2/m256","VEX.256.0F.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from ymm2/mem to ymm1."
"VMOVAPS ymm2/m256, ymm1","VEX.256.0F.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed single-precision floating-point values from ymm1 to ymm2/mem."
"VMOVAPS xmm1 {k1}{z}, xmm2/m128","EVEX.128.0F.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVAPS ymm1 {k1}{z}, ymm2/m256","EVEX.256.0F.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVAPS zmm1 {k1}{z}, zmm2/m512","EVEX.512.0F.W0 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVAPS xmm2/m128 {k1}{z}, xmm1","EVEX.128.0F.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVAPS ymm2/m256 {k1}{z}, ymm1","EVEX.256.0F.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVAPS zmm2/m512 {k1}{z}, zmm1","EVEX.512.0F.W0 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"MOVBE r16, m16","0F 38 F0 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Reverse byte order in m16 and move to r16."
"MOVBE r32, m32","0F 38 F0 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Reverse byte order in m32 and move to r32."
"MOVBE r64, m64","REX.W + 0F 38 F0 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Reverse byte order in m64 and move to r64."
"MOVBE m16, r16","0F 38 F1 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Reverse byte order in r16 and move to m16."
"MOVBE m32, r32","0F 38 F1 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Reverse byte order in r32 and move to m32."
"MOVBE m64, r64","REX.W + 0F 38 F1 /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Reverse byte order in r64 and move to m64."
"MOVD mm,r/m32",NP 0F 6E /r,Valid,Valid,Invalid,MMX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move doubleword from r/m32 to mm.
"MOVQ mm,r/m64",NP REX.W + 0F 6E /r,Valid,Invalid,Invalid,MMX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move quadword from r/m64 to mm.
"MOVD r/m32,mm",NP 0F 7E /r,Valid,Valid,Invalid,MMX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move doubleword from mm to r/m32.
"MOVQ r/m64,mm",NP REX.W + 0F 7E /r,Valid,Invalid,Invalid,MMX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move quadword from mm to r/m64.
"MOVD xmm,r/m32",66 0F 6E /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move doubleword from r/m32 to xmm.
"MOVQ xmm,r/m64",66 REX.W 0F 6E /r,Valid,Invalid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move quadword from r/m64 to xmm.
"MOVD r/m32,xmm",66 0F 7E /r,Valid,Valid,Invalid,SSE2,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move doubleword from xmm register to r/m32.
"MOVQ r/m64,xmm",66 REX.W 0F 7E /r,Valid,Invalid,Invalid,SSE2,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move quadword from xmm register to r/m64.
"VMOVD xmm1,r32/m32",VEX.128.66.0F.W0 6E /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move doubleword from r/m32 to xmm1.
"VMOVQ xmm1,r64/m64",VEX.128.66.0F.W1 6E /r,Valid,Invalid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move quadword from r/m64 to xmm1.
"VMOVD r32/m32,xmm1",VEX.128.66.0F.W0 7E /r,Valid,Valid,Invalid,AVX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move doubleword from xmm1 register to r/m32.
"VMOVQ r64/m64,xmm1",VEX.128.66.0F.W1 7E /r,Valid,Invalid,Invalid,AVX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move quadword from xmm1 register to r/m64.
"VMOVD xmm1,r32/m32",EVEX.128.66.0F.W0 6E /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Move doubleword from r/m32 to xmm1.
"VMOVQ xmm1,r64/m64",EVEX.128.66.0F.W1 6E /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Move quadword from r/m64 to xmm1.
"VMOVD r32/m32,xmm1",EVEX.128.66.0F.W0 7E /r,Valid,Valid,Invalid,AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Tuple1 Scalar,Move doubleword from xmm1 register to r/m32.
"VMOVQ r64/m64,xmm1",EVEX.128.66.0F.W1 7E /r,Valid,Invalid,Invalid,AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Tuple1 Scalar,Move quadword from xmm1 register to r/m64.
"MOVDDUP xmm1, xmm2/m64","F2 0F 12 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move double-precision floating-point value from xmm2/m64 and duplicate into xmm1."
"VMOVDDUP xmm1, xmm2/m64","VEX.128.F2.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move double-precision floating-point value from xmm2/m64 and duplicate into xmm1."
"VMOVDDUP ymm1, ymm2/m256","VEX.256.F2.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move even index double-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVDDUP xmm1 {k1}{z}, xmm2/m64","EVEX.128.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","MOVDDUP","Move double-precision floating-point value from xmm2/m64 and duplicate each element into xmm1 subject to writemask k1."
"VMOVDDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","MOVDDUP","Move even index double-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 subject to writemask k1."
"VMOVDDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F2.0F.W1 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","MOVDDUP","Move even index double-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 subject to writemask k1."
"MOVDQ2Q mm, xmm","F2 0F D6 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move low quadword from xmm to mmx register."
"MOVDQA xmm1, xmm2/m128","66 0F 6F /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed integer values from xmm2/mem to xmm1."
"MOVDQA xmm2/m128, xmm1","66 0F 7F /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed integer values from xmm1 to xmm2/mem."
"VMOVDQA xmm1, xmm2/m128","VEX.128.66.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed integer values from xmm2/mem to xmm1."
"VMOVDQA xmm2/m128, xmm1","VEX.128.66.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed integer values from xmm1 to xmm2/mem."
"VMOVDQA ymm1, ymm2/m256","VEX.256.66.0F.WIG 6F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move aligned packed integer values from ymm2/mem to ymm1."
"VMOVDQA ymm2/m256, ymm1","VEX.256.66.0F.WIG 7F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move aligned packed integer values from ymm1 to ymm2/mem."
"VMOVDQA32 xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA32 ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA32 zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W0 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA32 xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA32 ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA32 zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W0 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1."
"VMOVDQA64 xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned quadword integer values from xmm2/m128 to xmm1 using writemask k1."
"VMOVDQA64 ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned quadword integer values from ymm2/m256 to ymm1 using writemask k1."
"VMOVDQA64 zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 6F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move aligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1."
"VMOVDQA64 xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1."
"VMOVDQA64 ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1."
"VMOVDQA64 zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move aligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1.MOVDQA,VMOVDQA32/64â€”Move Aligned Packed Integer Values"
"MOVDQU xmm1,xmm2/m128",F3 0F 6F /r,Valid,Valid,Invalid,SSE2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move unaligned packed integer values from xmm2/m128 to xmm1.
"MOVDQU xmm2/m128,xmm1",F3 0F 7F /r,Valid,Valid,Invalid,SSE2,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move unaligned packed integer values from xmm1 to xmm2/m128.
"VMOVDQU xmm1,xmm2/m128",VEX.128.F3.0F.WIG 6F /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move unaligned packed integer values from xmm2/m128 to xmm1.
"VMOVDQU xmm2/m128,xmm1",VEX.128.F3.0F.WIG 7F /r,Valid,Valid,Invalid,AVX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move unaligned packed integer values from xmm1 to xmm2/m128.
"VMOVDQU ymm1,ymm2/m256",VEX.256.F3.0F.WIG 6F /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Move unaligned packed integer values from ymm2/m256 to ymm1.
"VMOVDQU ymm2/m256,ymm1",VEX.256.F3.0F.WIG 7F /r,Valid,Valid,Invalid,AVX,ModRM:r/m (r),ModRM:reg (w),NA,NA,,Move unaligned packed integer values from ymm1 to ymm2/m256.
"VMOVDQU8 xmm1 {k1}{z},xmm2/m128",EVEX.128.F2.0F.W0 6F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from xmm2/m128 to xmm1 using writemask k1.
"VMOVDQU8 ymm1 {k1}{z},ymm2/m256",EVEX.256.F2.0F.W0 6F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from ymm2/m256 to ymm1 using writemask k1.
"VMOVDQU8 zmm1 {k1}{z},zmm2/m512",EVEX.512.F2.0F.W0 6F /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from zmm2/m512 to zmm1 using writemask k1.
"VMOVDQU8 xmm2/m128 {k1}{z},xmm1",EVEX.128.F2.0F.W0 7F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from xmm1 to xmm2/m128 using writemask k1.
"VMOVDQU8 ymm2/m256 {k1}{z},ymm1",EVEX.256.F2.0F.W0 7F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from ymm1 to ymm2/m256 using writemask k1.
"VMOVDQU8 zmm2/m512 {k1}{z},zmm1",EVEX.512.F2.0F.W0 7F /r,Valid,Valid,Invalid,AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed byte integer values from zmm1 to zmm2/m512 using writemask k1.
"VMOVDQU16 xmm1 {k1}{z},xmm2/m128",EVEX.128.F2.0F.W1 6F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed word integer values from xmm2/m128 to xmm1 using writemask k1.
"VMOVDQU16 ymm1 {k1}{z},ymm2/m256",EVEX.256.F2.0F.W1 6F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed word integer values from ymm2/m256 to ymm1 using writemask k1.
"VMOVDQU16 zmm1 {k1}{z},zmm2/m512",EVEX.512.F2.0F.W1 6F /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed word integer values from zmm2/m512 to zmm1 using writemask k1.
"VMOVDQU16 xmm2/m128 {k1}{z},xmm1",EVEX.128.F2.0F.W1 7F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed word integer values from xmm1 to xmm2/m128 using writemask k1.
"VMOVDQU16 ymm2/m256 {k1}{z},ymm1",EVEX.256.F2.0F.W1 7F /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed word integer values from ymm1 to ymm2/m256 using writemask k1.
"VMOVDQU16 zmm2/m512 {k1}{z},zmm1",EVEX.512.F2.0F.W1 7F /r,Valid,Valid,Invalid,AVX512BW,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed word integer values from zmm1 to zmm2/m512 using writemask k1.
"VMOVDQU32 xmm1 {k1}{z},xmm2/m128",EVEX.128.F3.0F.W0 6F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from xmm2/m128 to xmm1 using writemask k1.
"VMOVDQU32 ymm1 {k1}{z},ymm2/m256",EVEX.256.F3.0F.W0 6F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from ymm2/m256 to ymm1 using writemask k1.
"VMOVDQU32 zmm1 {k1}{z},zmm2/m512",EVEX.512.F3.0F.W0 6F /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from zmm2/m512 to zmm1 using writemask k1.
"VMOVDQU32 xmm2/m128 {k1}{z},xmm1",EVEX.128.F3.0F.W0 7F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from xmm1 to xmm2/m128 using writemask k1.
"VMOVDQU32 ymm2/m256 {k1}{z},ymm1",EVEX.256.F3.0F.W0 7F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from ymm1 to ymm2/m256 using writemask k1.
"VMOVDQU32 zmm2/m512 {k1}{z},zmm1",EVEX.512.F3.0F.W0 7F /r,Valid,Valid,Invalid,AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed doubleword integer values from zmm1 to zmm2/m512 using writemask k1.
"VMOVDQU64 xmm1 {k1}{z},xmm2/m128",EVEX.128.F3.0F.W1 6F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from xmm2/m128 to xmm1 using writemask k1.
"VMOVDQU64 ymm1 {k1}{z},ymm2/m256",EVEX.256.F3.0F.W1 6F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from ymm2/m256 to ymm1 using writemask k1.
"VMOVDQU64 zmm1 {k1}{z},zmm2/m512",EVEX.512.F3.0F.W1 6F /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from zmm2/m512 to zmm1 using writemask k1.
"VMOVDQU64 xmm2/m128 {k1}{z},xmm1",EVEX.128.F3.0F.W1 7F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from xmm1 to xmm2/m128 using writemask k1.
"VMOVDQU64 ymm2/m256 {k1}{z},ymm1",EVEX.256.F3.0F.W1 7F /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from ymm1 to ymm2/m256 using writemask k1.
"VMOVDQU64 zmm2/m512 {k1}{z},zmm1",EVEX.512.F3.0F.W1 7F /r,Valid,Valid,Invalid,AVX512F,ModRM:r/m (r),ModRM:reg (w),NA,NA,Full Vector Mem,Move unaligned packed quadword integer values from zmm1 to zmm2/m512 using writemask k1.
"MOVHLPS xmm1, xmm2","NP 0F 12 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move two packed single-precision floating-point values from high quadword of xmm2 to low quadword of xmm1."
"VMOVHLPS xmm1, xmm2, xmm3","VEX.NDS.128.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","","Merge two packed single-precision floating-point values from high quadword of xmm3 and low quadword of xmm2."
"VMOVHLPS xmm1, xmm2, xmm3","EVEX.NDS.128.0F.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","","Merge two packed single-precision floating-point values from high quadword of xmm3 and low quadword of xmm2."
"MOVHPD xmm1, m64","66 0F 16 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Move double-precision floating-point value from m64 to high quadword of xmm1."
"VMOVHPD xmm2, xmm1, m64","VEX.NDS.128.66.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Merge double-precision floating-point value from m64 and the low quadword of xmm1."
"VMOVHPD xmm2, xmm1, m64","EVEX.NDS.128.66.0F.W1 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Merge double-precision floating-point value from m64 and the low quadword of xmm1."
"MOVHPD m64, xmm1","66 0F 17 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move double-precision floating-point value from high quadword of xmm1 to m64."
"VMOVHPD m64, xmm1","VEX.128.66.0F.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move double-precision floating-point value from high quadword of xmm1 to m64."
"VMOVHPD m64, xmm1","EVEX.128.66.0F.W1 17 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Move double-precision floating-point value from high quadword of xmm1 to m64."
"MOVHPS xmm1, m64","NP 0F 16 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Move two packed single-precision floating-point values from m64 to high quadword of xmm1."
"VMOVHPS xmm2, xmm1, m64","VEX.NDS.128.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1."
"VMOVHPS xmm2, xmm1, m64","EVEX.NDS.128.0F.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple2","Merge two packed single-precision floating-point values from m64 and the low quadword of xmm1."
"MOVHPS m64, xmm1","NP 0F 17 /r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move two packed single-precision floating-point values from high quadword of xmm1 to m64."
"VMOVHPS m64, xmm1","VEX.128.0F.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move two packed single-precision floating-point values from high quadword of xmm1 to m64."
"VMOVHPS m64, xmm1","EVEX.128.0F.W0 17 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple2","Move two packed single-precision floating-point values from high quadword of xmm1 to m64."
"MOVLHPS xmm1, xmm2","NP 0F 16 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move two packed single-precision floating-point values from low quadword of xmm2 to high quadword of xmm1."
"VMOVLHPS xmm1, xmm2, xmm3","VEX.NDS.128.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","","Merge two packed single-precision floating-point values from low quadword of xmm3 and low quadword of xmm2."
"VMOVLHPS xmm1, xmm2, xmm3","EVEX.NDS.128.0F.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","","Merge two packed single-precision floating-point values from low quadword of xmm3 and low quadword of xmm2."
"MOVLPD xmm1,m64",66 0F 12 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Move double-precision floating-point value from m64 to low quadword of xmm1.
"VMOVLPD xmm2,xmm1,m64",VEX.NDS.128.66.0F.WIG 12 /r,Valid,Valid,Invalid,AVX,ModRM:r/m (r),VEX.vvvv,ModRM:r/m (r),NA,,Merge double-precision floating-point value from m64 and the high quadword of xmm1.
"VMOVLPD xmm2,xmm1,m64",EVEX.NDS.128.66.0F.W1 12 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Merge double-precision floating-point value from m64 and the high quadword of xmm1.
"MOVLPD m64,xmm1",66 0F 13 /r,Valid,Valid,Invalid,SSE2,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move double-precision floating-point value from low quadword of xmm1 to m64.
"VMOVLPD m64,xmm1",VEX.128.66.0F.WIG 13 /r,Valid,Valid,Invalid,AVX,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Move double-precision floating-point value from low quadword of xmm1 to m64.
"VMOVLPD m64,xmm1",EVEX.128.66.0F.W1 13 /r,Valid,Valid,Invalid,AVX512F,ModRM:r/m (w),ModRM:reg (r),NA,NA,Tuple1 Scalar,Move double -precision floating-point value from low quadword of xmm1 to m64.
"MOVLPS xmm1, m64","NP 0F 12 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Move two packed single-precision floating-point values from m64 to low quadword of xmm1."
"VMOVLPS xmm2, xmm1, m64","VEX.NDS.128.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1."
"VMOVLPS xmm2, xmm1, m64","EVEX.NDS.128.0F.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple2","Merge two packed single-precision floating-point values from m64 and the high quadword of xmm1."
"MOVLPS m64, xmm1","0F 13/r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move two packed single-precision floating-point values from low quadword of xmm1 to m64."
"VMOVLPS m64, xmm1","VEX.128.0F.WIG 13/r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move two packed single-precision floating-point values from low quadword of xmm1 to m64."
"VMOVLPS m64, xmm1","EVEX.128.0F.W0 13/r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple2","Move two packed single-precision floating-point values from low quadword of xmm1 to m64."
"MOVMSKPD reg, xmm","66 0F 50 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 2-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros."
"VMOVMSKPD reg, xmm2","VEX.128.66.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 2-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVMSKPD reg, ymm2","VEX.256.66.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 4-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"MOVMSKPS reg, xmm","NP 0F 50 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 4-bit sign mask from xmm and store in reg. The upper bits of r32 or r64 are filled with zeros."
"VMOVMSKPS reg, xmm2","VEX.128.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 4-bit sign mask from xmm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"VMOVMSKPS reg, ymm2","VEX.256.0F.WIG 50 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Extract 8-bit sign mask from ymm2 and store in reg. The upper bits of r32 or r64 are zeroed."
"MOVNTDQ m128, xmm1","66 0F E7 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed integer values in xmm1 to m128 using non-temporal hint."
"VMOVNTDQ m128, xmm1","VEX.128.66.0F.WIG E7 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed integer values in xmm1 to m128 using non-temporal hint."
"VMOVNTDQ m256, ymm1","VEX.256.66.0F.WIG E7 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed integer values in ymm1 to m256 using non-temporal hint."
"VMOVNTDQ m128, xmm1","EVEX.128.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed integer values in xmm1 to m128 using non-temporal hint."
"VMOVNTDQ m256, ymm1","EVEX.256.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed integer values in zmm1 to m256 using non-temporal hint."
"VMOVNTDQ m512, zmm1","EVEX.512.66.0F.W0 E7 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed integer values in zmm1 to m512 using non-temporal hint."
"MOVNTDQA xmm1, m128","66 0F 38 2A /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move double quadword from m128 to xmm1 using non-temporal hint if WC memory type."
"VMOVNTDQA xmm1, m128","VEX.128.66.0F38.WIG 2A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move double quadword from m128 to xmm using non-temporal hint if WC memory type."
"VMOVNTDQA ymm1, m256","VEX.256.66.0F38.WIG 2A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMOVNTDQA xmm1, m128","EVEX.128.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move 128-bit data from m128 to xmm using non-temporal hint if WC memory type."
"VMOVNTDQA ymm1, m256","EVEX.256.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move 256-bit data from m256 to ymm using non-temporal hint if WC memory type."
"VMOVNTDQA zmm1, m512","EVEX.512.66.0F38.W0 2A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move 512-bit data from m512 to zmm using non-temporal hint if WC memory type."
"MOVNTI m32, r32","NP 0F C3 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Move doubleword from r32 to m32 using non- temporal hint."
"MOVNTI m64, r64","NP REX.W + 0F C3 /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Move quadword from r64 to m64 using non- temporal hint."
"MOVNTPD m128, xmm1","66 0F 2B /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed double-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m128, xmm1","VEX.128.66.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed double-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m256, ymm1","VEX.256.66.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed double-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPD m128, xmm1","EVEX.128.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed double-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPD m256, ymm1","EVEX.256.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed double-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPD m512, zmm1","EVEX.512.66.0F.W1 2B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed double-precision values in zmm1 to m512 using non-temporal hint."
"MOVNTPS m128, xmm1","NP 0F 2B /r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed single-precision values xmm1 to mem using non-temporal hint."
"VMOVNTPS m128, xmm1","VEX.128.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed single-precision values xmm1 to mem using non-temporal hint."
"VMOVNTPS m256, ymm1","VEX.256.0F.WIG 2B /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move packed single-precision values ymm1 to mem using non-temporal hint."
"VMOVNTPS m128, xmm1","EVEX.128.0F.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed single-precision values in xmm1 to m128 using non-temporal hint."
"VMOVNTPS m256, ymm1","EVEX.256.0F.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed single-precision values in ymm1 to m256 using non-temporal hint."
"VMOVNTPS m512, zmm1","EVEX.512.0F.W0 2B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move packed single-precision values in zmm1 to m512 using non-temporal hint."
"MOVNTQ m64, mm","NP 0F E7 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Move quadword from mm to m64 using non- temporal hint."
"MOVQ mm, mm/m64","NP 0F 6F /r","Valid","Valid","Invalid","MMX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move quadword from mm/m64 to mm."
"MOVQ mm/m64, mm","NP 0F 7F /r","Valid","Valid","Invalid","MMX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move quadword from mm to mm/m64."
"MOVQ xmm1, xmm2/m64","F3 0F 7E /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move quadword from xmm2/mem64 to xmm1."
"VMOVQ xmm1, xmm2/m64","VEX.128.F3.0F.WIG 7E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move quadword from xmm2 to xmm1."
"VMOVQ xmm1, xmm2/m64","EVEX.128.F3.0F.W1 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Move quadword from xmm2/m64 to xmm1."
"MOVQ xmm2/m64, xmm1","66 0F D6 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move quadword from xmm1 to xmm2/mem64."
"VMOVQ xmm1/m64, xmm2","VEX.128.66.0F.WIG D6 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move quadword from xmm2 register to xmm1/m64."
"VMOVQ xmm1/m64, xmm2","EVEX.128.66.0F.W1 D6 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Move quadword from xmm2 register to xmm1/m64."
"MOVQ2DQ xmm, mm","F3 0F D6 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move quadword from mmx to low quadword of xmm."
"MOVS m8, m8","A4","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI."
"MOVS m16, m16","A5","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI."
"MOVS m32, m32","A5","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI."
"MOVS m64, m64","REX.W + A5","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Move qword from address (R|E)SI to (R|E)DI."
"MOVSB","A4","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, Move byte from address DS:(E)SI to ES:(E)DI. For 64-bit mode move byte from address (R|E)SI to (R|E)DI."
"MOVSW","A5","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, move word from address DS:(E)SI to ES:(E)DI. For 64-bit mode move word at address (R|E)SI to (R|E)DI."
"MOVSD","A5","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, move dword from address DS:(E)SI to ES:(E)DI. For 64-bit mode move dword from address (R|E)SI to (R|E)DI."
"MOVSQ","REX.W + A5","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Move qword from address (R|E)SI to (R|E)DI."
"MOVSD xmm1, xmm2","F2 0F 10 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Move scalar double-precision floating-point value from xmm2 to xmm1 register."
"MOVSD xmm1, m64","F2 0F 10 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Load scalar double-precision floating-point value from m64 to xmm1 register."
"MOVSD xmm1/m64, xmm2","F2 0F 11 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move scalar double-precision floating-point value from xmm2 register to xmm1/m64."
"VMOVSD xmm1, xmm2, xmm3","VEX.NDS.LIG.F2.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Merge scalar double-precision floating-point value from xmm2 and xmm3 to xmm1 register."
"VMOVSD xmm1, m64","VEX.LIG.F2.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Load scalar double-precision floating-point value from m64 to xmm1 register."
"VMOVSD xmm1, xmm2, xmm3","VEX.NDS.LIG.F2.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","vvvv (r)","ModRM:reg (r)","NA","NA","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1."
"VMOVSD m64, xmm1","VEX.LIG.F2.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Store scalar double-precision floating-point value from xmm1 register to m64."
"VMOVSD xmm1 {k1}{z}, xmm2, xmm3","EVEX.NDS.LIG.F2.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1 under writemask k1."
"VMOVSD xmm1 {k1}{z}, m64","EVEX.LIG.F2.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Load scalar double-precision floating-point value from m64 to xmm1 register under writemask k1."
"VMOVSD xmm1 {k1}{z}, xmm2, xmm3","EVEX.NDS.LIG.F2.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","vvvv (r)","ModRM:reg (r)","NA","NA","Merge scalar double-precision floating-point value from xmm2 and xmm3 registers to xmm1 under writemask k1."
"VMOVSD m64 {k1}, xmm1","EVEX.LIG.F2.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Store scalar double-precision floating-point value from xmm1 register to m64 under writemask k1."
"MOVSHDUP xmm1, xmm2/m128","F3 0F 16 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSHDUP xmm1, xmm2/m128","VEX.128.F3.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move odd index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSHDUP ymm1, ymm2/m256","VEX.256.F3.0F.WIG 16 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move odd index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVSHDUP xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move odd index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask."
"VMOVSHDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move odd index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask."
"VMOVSHDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move odd index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask."
"MOVSLDUP xmm1, xmm2/m128","F3 0F 12 /r","Valid","Valid","Invalid","SSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSLDUP xmm1, xmm2/m128","VEX.128.F3.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move even index single-precision floating-point values from xmm2/mem and duplicate each element into xmm1."
"VMOVSLDUP ymm1, ymm2/m256","VEX.256.F3.0F.WIG 12 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move even index single-precision floating-point values from ymm2/mem and duplicate each element into ymm1."
"VMOVSLDUP xmm1 {k1}{z}, xmm2/m128","EVEX.128.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move even index single-precision floating-point values from xmm2/m128 and duplicate each element into xmm1 under writemask."
"VMOVSLDUP ymm1 {k1}{z}, ymm2/m256","EVEX.256.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move even index single-precision floating-point values from ymm2/m256 and duplicate each element into ymm1 under writemask."
"VMOVSLDUP zmm1 {k1}{z}, zmm2/m512","EVEX.512.F3.0F.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move even index single-precision floating-point values from zmm2/m512 and duplicate each element into zmm1 under writemask."
"MOVSS xmm1, xmm2","F3 0F 10 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Merge scalar single-precision floating-point value from xmm2 to xmm1 register."
"MOVSS xmm1, m32","F3 0F 10 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Load scalar single-precision floating-point value from m32 to xmm1 register."
"VMOVSS xmm1, xmm2, xmm3","VEX.NDS.LIG.F3.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Merge scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register"
"VMOVSS xmm1, m32","VEX.LIG.F3.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Load scalar single-precision floating-point value from m32 to xmm1 register."
"MOVSS xmm2/m32, xmm1","F3 0F 11 /r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move scalar single-precision floating-point value from xmm1 register to xmm2/m32."
"VMOVSS xmm1, xmm2, xmm3","VEX.NDS.LIG.F3.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","vvvv (r)","ModRM:reg (r)","NA","NA","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register."
"VMOVSS m32, xmm1","VEX.LIG.F3.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move scalar single-precision floating-point value from xmm1 register to m32."
"VMOVSS xmm1 {k1}{z}, xmm2, xmm3","EVEX.NDS.LIG.F3.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1."
"VMOVSS xmm1 {k1}{z}, m32","EVEX.LIG.F3.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Move scalar single-precision floating-point values from m32 to xmm1 under writemask k1."
"VMOVSS xmm1 {k1}{z}, xmm2, xmm3","EVEX.NDS.LIG.F3.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","vvvv (r)","ModRM:reg (r)","NA","NA","Move scalar single-precision floating-point value from xmm2 and xmm3 to xmm1 register under writemask k1."
"VMOVSS m32 {k1}, xmm1","EVEX.LIG.F3.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Move scalar single-precision floating-point values from xmm1 to m32 under writemask k1."
"MOVSX r16, r/m8","0F BE /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to word with sign-extension."
"MOVSX r32, r/m8","0F BE /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to doubleword with sign- extension."
"MOVSX r64, r/m8","REX + 0F BE /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to quadword with sign-extension."
"MOVSX r32, r/m16","0F BF /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move word to doubleword, with sign- extension."
"MOVSX r64, r/m16","REX.W + 0F BF /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move word to quadword with sign-extension."
"MOVSXD r64, r/m32","REX.W + 63 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move doubleword to quadword with sign- extension."
"MOVUPD xmm1, xmm2/m128","66 0F 10 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from xmm2/mem to xmm1."
"MOVUPD xmm2/m128, xmm1","66 0F 11 /r","Valid","Valid","Invalid","SSE2","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from xmm2/mem to xmm1."
"VMOVUPD xmm2/m128, xmm1","VEX.128.66.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from ymm2/mem to ymm1."
"VMOVUPD ymm2/m256, ymm1","VEX.256.66.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed double-precision floating-point from ymm1 to ymm2/mem."
"VMOVUPD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point from xmm2/m128 to xmm1 using writemask k1."
"VMOVUPD xmm2/m128 {k1}{z}, xmm1","EVEX.128.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point from xmm1 to xmm2/m128 using writemask k1."
"VMOVUPD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point from ymm2/m256 to ymm1 using writemask k1."
"VMOVUPD ymm2/m256 {k1}{z}, ymm1","EVEX.256.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point from ymm1 to ymm2/m256 using writemask k1."
"VMOVUPD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F.W1 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVUPD zmm2/m512 {k1}{z}, zmm1","EVEX.512.66.0F.W1 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed double-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"MOVUPS xmm1, xmm2/m128","NP 0F 10 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from xmm2/mem to xmm1."
"MOVUPS xmm2/m128, xmm1","NP 0F 11 /r","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPS xmm1, xmm2/m128","VEX.128.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from xmm2/mem to xmm1."
"VMOVUPS xmm2/m128, xmm1","VEX.128.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from xmm1 to xmm2/mem."
"VMOVUPS ymm1, ymm2/m256","VEX.256.0F.WIG 10 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from ymm2/mem to ymm1."
"VMOVUPS ymm2/m256, ymm1","VEX.256.0F.WIG 11 /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","NA","Move unaligned packed single-precision floating-point from ymm1 to ymm2/mem."
"VMOVUPS xmm1 {k1}{z}, xmm2/m128","EVEX.128.0F.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VMOVUPS ymm1 {k1}{z}, ymm2/m256","EVEX.256.0F.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VMOVUPS zmm1 {k1}{z}, zmm2/m512","EVEX.512.0F.W0 10 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VMOVUPS xmm2/m128 {k1}{z}, xmm1","EVEX.128.0F.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from xmm1 to xmm2/m128 using writemask k1."
"VMOVUPS ymm2/m256 {k1}{z}, ymm1","EVEX.256.0F.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from ymm1 to ymm2/m256 using writemask k1."
"VMOVUPS zmm2/m512 {k1}{z}, zmm1","EVEX.512.0F.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Full Vector Mem","Move unaligned packed single-precision floating-point values from zmm1 to zmm2/m512 using writemask k1."
"MOVZX r16, r/m8","0F B6 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to word with zero-extension."
"MOVZX r32, r/m8","0F B6 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to doubleword, zero-extension."
"MOVZX r64, r/m8","REX.W + 0F B6 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move byte to quadword, zero-extension."
"MOVZX r32, r/m16","0F B7 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move word to doubleword, zero-extension."
"MOVZX r64, r/m16","REX.W + 0F B7 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move word to quadword, zero-extension."
"MPSADBW xmm1, xmm2/m128, imm8","66 0F 3A 42 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm1 and xmm2/m128 and writes the results in xmm1. Starting offsets within xmm1 and xmm2/m128 are determined by imm8."
"VMPSADBW xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 42 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and xmm3/m128 and writes the results in xmm1. Starting offsets within xmm2 and xmm3/m128 are determined by imm8."
"VMPSADBW ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 42 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Sums absolute 8-bit integer difference of adjacent groups of 4 byte integers in xmm2 and ymm3/m128 and writes the results in ymm1. Starting offsets within ymm2 and xmm3/m128 are determined by imm8."
"MUL r/m8","F6 /4","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Unsigned multiply (AX â† AL âˆ— r/m8)."
"MUL r/m8","REX + F6 /4","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Unsigned multiply (AX â† AL âˆ— r/m8)."
"MUL r/m16","F7 /4","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Unsigned multiply (DX:AX â† AX âˆ— r/m16)."
"MUL r/m32","F7 /4","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Unsigned multiply (EDX:EAX â† EAX âˆ— r/m32)."
"MUL r/m64","REX.W + F7 /4","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Unsigned multiply (RDX:RAX â† RAX âˆ— r/m64)."
"MULPD xmm1, xmm2/m128","66 0F 59 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply packed double-precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1."
"VMULPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1."
"VMULPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1."
"VMULPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F.W1 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values in zmm3/m512/m64bcst with zmm2 and store result in zmm1."
"MULPS xmm1, xmm2/m128","NP 0F 59 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply packed single-precision floating-point values in xmm2/m128 with xmm1 and store result in xmm1."
"VMULPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values in xmm3/m128 with xmm2 and store result in xmm1."
"VMULPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values in ymm3/m256 with ymm2 and store result in ymm1."
"VMULPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and store result in xmm1."
"VMULPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 59 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and store result in ymm1."
"VMULPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst {er}","EVEX.NDS.512.0F.W0 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values in zmm3/m512/m32bcst with zmm2 and store result in zmm1."
"MULSD xmm1,xmm2/m64","F2 0F 59 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the low double-precision floating-point value in xmm2/m64 by low double-precision floating-point value in xmm1."
"VMULSD xmm1,xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2."
"VMULSD xmm1 {k1}{z}, xmm2, xmm3/m64 {er}","EVEX.NDS.LIG.F2.0F.W1 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply the low double-precision floating-point value in xmm3/m64 by low double-precision floating-point value in xmm2."
"MULSS xmm1,xmm2/m32","F3 0F 59 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the low single-precision floating-point value in xmm2/m32 by the low single-precision floating-point value in xmm1."
"VMULSS xmm1,xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 59 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2."
"VMULSS xmm1 {k1}{z}, xmm2, xmm3/m32 {er}","EVEX.NDS.LIG.F3.0F.W0 59 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply the low single-precision floating-point value in xmm3/m32 by the low single-precision floating-point value in xmm2."
"MULX r32a, r32b, r/m32","VEX.NDD.LZ.F2.0F38.W0 F6 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (w)","ModRM:r/m (r)","RDX/EDX","","Unsigned multiply of r/m32 with EDX without affecting arithmetic flags."
"MULX r64a, r64b, r/m64","VEX.NDD.LZ.F2.0F38.W1 F6 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (w)","ModRM:r/m (r)","RDX/EDX","","Unsigned multiply of r/m64 with RDX without affecting arithmetic flags."
"MWAIT","0F 01 C9","Valid","Valid","Valid","","NA","NA","NA","NA","","A hint that allow the processor to stop instruction execution and enter an implementation-dependent optimized state until occurrence of a class of events."
"NEG r/m8","F6 /3","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Two's complement negate r/m8."
"NEG r/m8","REX + F6 /3","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Two's complement negate r/m8."
"NEG r/m16","F7 /3","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Two's complement negate r/m16."
"NEG r/m32","F7 /3","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Two's complement negate r/m32."
"NEG r/m64","REX.W + F7 /3","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Two's complement negate r/m64."
"NOP","NP 90","Valid","Valid","Valid","","NA","NA","NA","NA","","One byte no-operation instruction."
"NOP r/m16","NP 0F 1F /0","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Multi-byte no-operation instruction."
"NOP r/m32","NP 0F 1F /0","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Multi-byte no-operation instruction."
"NOT r/m8","F6 /2","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Reverse each bit of r/m8."
"NOT r/m8","REX + F6 /2","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Reverse each bit of r/m8."
"NOT r/m16","F7 /2","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Reverse each bit of r/m16."
"NOT r/m32","F7 /2","Valid","Valid","Valid","","ModRM:r/m (r, w)","NA","NA","NA","","Reverse each bit of r/m32."
"NOT r/m64","REX.W + F7 /2","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","NA","NA","NA","","Reverse each bit of r/m64."
"OR AL, imm8","0C ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AL OR imm8."
"OR AX, imm16","0D iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AX OR imm16."
"OR EAX, imm32","0D id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","EAX OR imm32."
"OR RAX, imm32","REX.W + 0D id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","RAX OR imm32 (sign-extended)."
"OR r/m8, imm8","80 /1 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m8 OR imm8."
"OR r/m8, imm8","REX + 80 /1 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m8 OR imm8."
"OR r/m16, imm16","81 /1 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m16 OR imm16."
"OR r/m32, imm32","81 /1 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m32 OR imm32."
"OR r/m64, imm32","REX.W + 81 /1 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m64 OR imm32 (sign-extended)."
"OR r/m16, imm8","83 /1 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m16 OR imm8 (sign-extended)."
"OR r/m32, imm8","83 /1 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m32 OR imm8 (sign-extended)."
"OR r/m64, imm8","REX.W + 83 /1 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m64 OR imm8 (sign-extended)."
"OR r/m8, r8","08 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m8 OR r8."
"OR r/m8, r8","REX + 08 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m8 OR r8."
"OR r/m16, r16","09 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m16 OR r16."
"OR r/m32, r32","09 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m32 OR r32."
"OR r/m64, r64","REX.W + 09 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m64 OR r64."
"OR r8, r/m8","0A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r8 OR r/m8."
"OR r8, r/m8","REX + 0A /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r8 OR r/m8."
"OR r16, r/m16","0B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r16 OR r/m16."
"OR r32, r/m32","0B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r32 OR r/m32."
"OR r64, r/m64","REX.W + 0B /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r64 OR r/m64."
"ORPD xmm1, xmm2/m128","66 0F 56/r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical OR of packed double-precision floating-point values in xmm1 and xmm2/mem."
"VORPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VORPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VORPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 56 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VORPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 56 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VORPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 56 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"ORPS xmm1, xmm2/m128","NP 0F 56 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical OR of packed single-precision floating-point values in xmm1 and xmm2/mem."
"VORPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VORPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F 56 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VORPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VORPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 56 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VORPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 56 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical OR of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"OUT imm8, AL","E6 ib","Valid","Valid","Valid","","","","","","","Output byte in AL to I/O port address imm8."
"OUT imm8, AX","E7 ib","Valid","Valid","Valid","","","","","","","Output word in AX to I/O port address imm8."
"OUT imm8, EAX","E7 ib","Valid","Valid","Valid","","","","","","","Output doubleword in EAX to I/O port address imm8."
"OUT DX, AL","EE","Valid","Valid","Valid","","","","","","","Output byte in AL to I/O port address in DX."
"OUT DX, AX","EF","Valid","Valid","Valid","","","","","","","Output word in AX to I/O port address in DX."
"OUT DX, EAX","EF","Valid","Valid","Valid","","","","","","","Output doubleword in EAX to I/O port address in DX."
"OUTS DX, m8","6E","Valid","Valid","Valid","","NA","NA","NA","NA","","Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"OUTS DX, m16","6F","Valid","Valid","Valid","","NA","NA","NA","NA","","Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"OUTS DX, m32","6F","Valid","Valid","Valid","","NA","NA","NA","NA","","Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"OUTSB","6E","Valid","Valid","Valid","","NA","NA","NA","NA","","Output byte from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"OUTSW","6F","Valid","Valid","Valid","","NA","NA","NA","NA","","Output word from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"OUTSD","6F","Valid","Valid","Valid","","NA","NA","NA","NA","","Output doubleword from memory location specified in DS:(E)SI or RSI to I/O port specified in DX."
"PABSB mm1, mm2/m64","NP 0F 38 1C /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of bytes in mm2/m64 and store UNSIGNED result in mm1."
"PABSB xmm1, xmm2/m128","66 0F 38 1C /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1."
"PABSW mm1, mm2/m64","NP 0F 38 1D /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 16-bit integers in mm2/m64 and store UNSIGNED result in mm1."
"PABSW xmm1, xmm2/m128","66 0F 38 1D /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"PABSD mm1, mm2/m64","NP 0F 38 1E /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 32-bit integers in mm2/m64 and store UNSIGNED result in mm1."
"PABSD xmm1, xmm2/m128","66 0F 38 1E /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSB xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSW xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSD xmm1, xmm2/m128","VEX.128.66.0F38.WIG 1E /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 32-bit integers in xmm2/m128 and store UNSIGNED result in xmm1."
"VPABSB ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSW ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 16-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSD ymm1, ymm2/m256","VEX.256.66.0F38.WIG 1E /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Compute the absolute value of 32-bit integers in ymm2/m256 and store UNSIGNED result in ymm1."
"VPABSB xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Compute the absolute value of bytes in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1."
"VPABSB ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Compute the absolute value of bytes in ymm2/m256 and store UNSIGNED result in ymm1 using writemask k1."
"VPABSB zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.WIG 1C /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Compute the absolute value of bytes in zmm2/m512 and store UNSIGNED result in zmm1 using writemask k1."
"VPABSW xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.WIG 1D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector Mem","Compute the absolute value of 16-bit integers in xmm2/m128 and store UNSIGNED result in xmm1 using writemask k1.PABSB/PABSW/PABSD/PABSQ â€” Packed Absolute Value"
"PACKSSWB mm1, mm2/m64","NP 0F 63 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 4 packed signed word integers from mm1 and from mm2/m64 into 8 packed signed byte integers in mm1 using signed saturation."
"PACKSSWB xmm1, xmm2/m128","66 0F 63 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 8 packed signed word integers from xmm1 and from xxm2/m128 into 16 packed signed byte integers in xxm1 using signed saturation."
"PACKSSDW mm1, mm2/m64","NP 0F 6B /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 2 packed signed doubleword integers from mm1 and from mm2/m64 into 4 packed signed word integers in mm1 using signed saturation."
"PACKSSDW xmm1, xmm2/m128","66 0F 6B /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 4 packed signed doubleword integers from xmm1 and from xxm2/m128 into 8 packed signed word integers in xxm1 using signed saturation."
"VPACKSSWB xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 8 packed signed word integers from xmm2 and from xmm3/m128 into 16 packed signed byte integers in xmm1 using signed saturation."
"VPACKSSDW xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 6B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 4 packed signed doubleword integers from xmm2 and from xmm3/m128 into 8 packed signed word integers in xmm1 using signed saturation."
"VPACKSSWB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 16 packed signed word integers from ymm2 and from ymm3/m256 into 32 packed signed byte integers in ymm1 using signed saturation."
"VPACKSSDW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 6B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 8 packed signed doubleword integers from ymm2 and from ymm3/m256 into 16 packed signed word integers in ymm1using signed saturation."
"VPACKSSWB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts packed signed word integers from xmm2 and from xmm3/m128 into packed signed byte integers in xmm1 using signed saturation under writemask k1."
"VPACKSSWB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts packed signed word integers from ymm2 and from ymm3/m256 into packed signed byte integers in ymm1 using signed saturation under writemask k1."
"VPACKSSWB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG 63 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts packed signed word integers from zmm2 and from zmm3/m512 into packed signed byte integers in zmm1 using signed saturation under writemask k1."
"VPACKSSDW xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 6B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Converts packed signed doubleword integers from xmm2 and from xmm3/m128/m32bcst into packed signed word integers in xmm1 using signed saturation under writemask k1.PACKSSWB/PACKSSDWâ€”Pack with Signed Saturation"
"PACKUSDW xmm1, xmm2/m128","66 0F 38 2B /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Convert 4 packed signed doubleword integers from xmm1 and 4 packed signed doubleword integers from xmm2/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation."
"VPACKUSDW xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F38 2B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Convert 4 packed signed doubleword integers from xmm2 and 4 packed signed doubleword integers from xmm3/m128 into 8 packed unsigned word integers in xmm1 using unsigned saturation."
"VPACKUSDW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38 2B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Convert 8 packed signed doubleword integers from ymm2 and 8 packed signed doubleword integers from ymm3/m256 into 16 packed unsigned word integers in ymm1 using unsigned saturation."
"VPACKUSDW xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 2B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Convert packed signed doubleword integers from xmm2 and packed signed doubleword integers from xmm3/m128/m32bcst into packed unsigned word integers in xmm1 using unsigned saturation under writemask k1. EVEX.NDS.256.66.0F38.W0 2B /r C V/V AVX512VL AVX512BW Convert packed signed doubleword integers from ymm2 and packed signed doubleword integers from ymm3/m256/m32bcst into packed unsigned word integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSDW zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 2B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Convert packed signed doubleword integers from zmm2 and packed signed doubleword integers from zmm3/m512/m32bcst into packed unsigned word integers in zmm1 using unsigned saturation under writemask k1."
"PACKUSWB mm, mm/m64","NP 0F 67 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 4 signed word integers from mm and 4 signed word integers from mm/m64 into 8 unsigned byte integers in mm using unsigned saturation."
"PACKUSWB xmm1, xmm2/m128","66 0F 67 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Converts 8 signed word integers from xmm1 and 8 signed word integers from xmm2/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation."
"VPACKUSWB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 8 signed word integers from xmm2 and 8 signed word integers from xmm3/m128 into 16 unsigned byte integers in xmm1 using unsigned saturation."
"VPACKUSWB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Converts 16 signed word integers from ymm2 and 16signed word integers from ymm3/m256 into 32 unsigned byte integers in ymm1 using unsigned saturation."
"VPACKUSWB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts signed word integers from xmm2 and signed word integers from xmm3/m128 into unsigned byte integers in xmm1 using unsigned saturation under writemask k1."
"VPACKUSWB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts signed word integers from ymm2 and signed word integers from ymm3/m256 into unsigned byte integers in ymm1 using unsigned saturation under writemask k1."
"VPACKUSWB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG 67 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Converts signed word integers from zmm2 and signed word integers from zmm3/m512 into unsigned byte integers in zmm1 using unsigned saturation under writemask k1."
"PADDB mm, mm/m64","NP 0F FC /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed byte integers from mm/m64 and mm."
"PADDW mm, mm/m64","NP 0F FD /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed word integers from mm/m64 and mm."
"PADDD mm, mm/m64","NP 0F FE /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed doubleword integers from mm/m64 and mm."
"PADDQ mm, mm/m64","NP 0F D4 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed quadword integers from mm/m64 and mm."
"PADDB xmm1, xmm2/m128","66 0F FC /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed byte integers from xmm2/m128 and xmm1."
"PADDW xmm1, xmm2/m128","66 0F FD /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed word integers from xmm2/m128 and xmm1."
"PADDD xmm1, xmm2/m128","66 0F FE /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed doubleword integers from xmm2/m128 and xmm1."
"PADDQ xmm1, xmm2/m128","66 0F D4 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed quadword integers from xmm2/m128 and xmm1."
"VPADDB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1."
"VPADDW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed word integers from xmm2, xmm3/m128 and store in xmm1."
"VPADDD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG FE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed doubleword integers from xmm2, xmm3/m128 and store in xmm1."
"VPADDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed quadword integers from xmm2, xmm3/m128 and store in xmm1."
"VPADDB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1."
"VPADDW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed word integers from ymm2, ymm3/m256 and store in ymm1."
"VPADDD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG FE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed doubleword integers from ymm2, ymm3/m256 and store in ymm1."
"VPADDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG D4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed quadword integers from ymm2, ymm3/m256 and store in ymm1."
"VPADDB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed byte integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1."
"VPADDW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed word integers from xmm2, and xmm3/m128 and store in xmm1 using writemask k1."
"VPADDD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 FE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Add packed doubleword integers from xmm2, and xmm3/m128/m32bcst and store in xmm1 using writemask k1."
"VPADDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 D4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Add packed quadword integers from xmm2, and xmm3/m128/m64bcst and store in xmm1 using writemask k1."
"VPADDB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG FC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed byte integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1."
"VPADDW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG FD /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed word integers from ymm2, and ymm3/m256 and store in ymm1 using writemask k1.PADDB/PADDW/PADDD/PADDQâ€”Add Packed Integers"
"PADDSB mm, mm/m64","NP 0F EC /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed signed byte integers from mm/m64 and mm and saturate the results."
"PADDSB xmm1, xmm2/m128","66 0F EC /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed signed byte integers from xmm2/m128 and xmm1 saturate the results."
"PADDSW mm, mm/m64","NP 0F ED /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed signed word integers from mm/m64 and mm and saturate the results."
"PADDSW xmm1, xmm2/m128","66 0F ED /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Add packed signed word integers from xmm2/m128 and xmm1 and saturate the results."
"VPADDSB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed signed byte integers from xmm3/m128 and xmm2 saturate the results."
"VPADDSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed signed word integers from xmm3/m128 and xmm2 and saturate the results."
"VPADDSB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1."
"VPADDSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed byte integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed byte integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG EC /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed byte integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed word integers from xmm2, and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed word integers from ymm2, and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDSW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG ED /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Add packed signed word integers from zmm2, and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"PADDUSB mm,mm/m64",NP 0F DC /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Add packed unsigned byte integers from mm/m64 and mm and saturate the results.
"PADDUSB xmm1,xmm2/m128",66 0F DC /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Add packed unsigned byte integers from xmm2/m128 and xmm1 saturate the results.
"PADDUSW mm,mm/m64",NP 0F DD /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Add packed unsigned word integers from mm/m64 and mm and saturate the results.
"PADDUSW xmm1,xmm2/m128",66 0F DD /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Add packed unsigned word integers from xmm2/m128 to xmm1 and saturate the results.
"VPADDUSB xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG DC /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Add packed unsigned byte integers from xmm3/m128 to xmm2 and saturate the results.
"VPADDUSW xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG DD /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Add packed unsigned word integers from xmm3/m128 to xmm2 and saturate the results.
"VPADDUSB ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG DC /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,"Add packed unsigned byte integers from ymm2,and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSW ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG DD /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,"Add packed unsigned word integers from ymm2,and ymm3/m256 and store the saturated results in ymm1."
"VPADDUSB xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG DC /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned byte integers from xmm2,and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSB ymm1 {k1}{z},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG DC /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned byte integers from ymm2,and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSB zmm1 {k1}{z},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG DC /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned byte integers from zmm2,and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"VPADDUSW xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG DD /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned word integers from xmm2,and xmm3/m128 and store the saturated results in xmm1 under writemask k1."
"VPADDUSW ymm1 {k1}{z},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG DD /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned word integers from ymm2,and ymm3/m256 and store the saturated results in ymm1 under writemask k1."
"VPADDUSW zmm1 {k1}{z},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG DD /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Add packed unsigned word integers from zmm2,and zmm3/m512 and store the saturated results in zmm1 under writemask k1."
"PALIGNR mm1, mm2/m64, imm8","NP 0F 3A 0F /r ib","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into mm1."
"PALIGNR xmm1, xmm2/m128, imm8","66 0F 3A 0F /r ib","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Concatenate destination and source operands, extract byte-aligned result shifted to the right by constant value in imm8 into xmm1."
"VPALIGNR xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Concatenate xmm2 and xmm3/m128, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1."
"VPALIGNR ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1."
"VPALIGNR xmm1 {k1}{z}, xmm2, xmm3/m128, imm8","EVEX.NDS.128.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","imm8","","Concatenate xmm2 and xmm3/m128 into a 32-byte intermediate result, extract byte aligned result shifted to the right by constant value in imm8 and result is stored in xmm1."
"VPALIGNR ymm1 {k1}{z}, ymm2, ymm3/m256, imm8","EVEX.NDS.256.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","imm8","","Concatenate pairs of 16 bytes in ymm2 and ymm3/m256 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and two 16-byte results are stored in ymm1."
"VPALIGNR zmm1 {k1}{z}, zmm2, zmm3/m512, imm8","EVEX.NDS.512.66.0F3A.WIG 0F /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","imm8","","Concatenate pairs of 16 bytes in zmm2 and zmm3/m512 into 32-byte intermediate result, extract byte-aligned, 16-byte result shifted to the right by constant values in imm8 from each intermediate result, and four 16-byte results are stored in zmm1."
"PAND mm, mm/m64","NP 0F DB /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise AND mm/m64 and mm."
"PAND xmm1, xmm2/m128","66 0F DB /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise AND of xmm2/m128 and xmm1."
"VPAND xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG DB /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise AND of xmm3/m128 and xmm."
"VPAND ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG DB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise AND of ymm2, and ymm3/m256 and store result in ymm1."
"VPANDD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F.W0 DB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 DB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"PANDN mm, mm/m64","NP 0F DF /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise AND NOT of mm/m64 and mm."
"PANDN xmm1, xmm2/m128","66 0F DF /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise AND NOT of xmm2/m128 and xmm1."
"VPANDN xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG DF /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise AND NOT of xmm3/m128 and xmm2."
"VPANDN ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG DF /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise AND NOT of ymm2, and ymm3/m256 and store result in ymm1."
"VPANDND xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and store result in xmm1 using writemask k1."
"VPANDND ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and store result in ymm1 using writemask k1."
"VPANDND zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F.W0 DF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and store result in zmm1 using writemask k1."
"VPANDNQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed quadword integers in xmm2 and xmm3/m128/m64bcst and store result in xmm1 using writemask k1."
"VPANDNQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed quadword integers in ymm2 and ymm3/m256/m64bcst and store result in ymm1 using writemask k1."
"VPANDNQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 DF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND NOT of packed quadword integers in zmm2 and zmm3/m512/m64bcst and store result in zmm1 using writemask k1."
"PAUSE","F3 90","Valid","Valid","Valid","","NA","NA","NA","NA","","Gives hint to processor that improves performance of spin-wait loops."
"PAVGB mm1, mm2/m64","NP 0F E0 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Average packed unsigned byte integers from mm2/m64 and mm1 with rounding."
"PAVGB xmm1, xmm2/m128","66 0F E0 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Average packed unsigned byte integers from xmm2/m128 and xmm1 with rounding."
"PAVGW mm1, mm2/m64","NP 0F E3 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Average packed unsigned word integers from mm2/m64 and mm1 with rounding."
"PAVGW xmm1, xmm2/m128","66 0F E3 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Average packed unsigned word integers from xmm2/m128 and xmm1 with rounding."
"VPAVGB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Average packed unsigned byte integers from xmm3/m128 and xmm2 with rounding."
"VPAVGW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Average packed unsigned word integers from xmm3/m128 and xmm2 with rounding."
"VPAVGB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1."
"VPAVGW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1."
"VPAVGB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned byte integers from xmm2, and xmm3/m128 with rounding and store to xmm1 under writemask k1."
"VPAVGB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned byte integers from ymm2, and ymm3/m256 with rounding and store to ymm1 under writemask k1."
"VPAVGB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG E0 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned byte integers from zmm2, and zmm3/m512 with rounding and store to zmm1 under writemask k1."
"VPAVGW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned word integers from xmm2, xmm3/m128 with rounding to xmm1 under writemask k1."
"VPAVGW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned word integers from ymm2, ymm3/m256 with rounding to ymm1 under writemask k1."
"VPAVGW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG E3 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Average packed unsigned word integers from zmm2, zmm3/m512 with rounding to zmm1 under writemask k1."
"PBLENDVB xmm1, xmm2/m128, <XMM0>","66 0F 38 10 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","<XMM0>","NA","","Select byte values from xmm1 and xmm2/m128 from mask specified in the high bit of each byte in XMM0 and store the values into xmm1."
"VPBLENDVB xmm1, xmm2, xmm3/m128, xmm4","VEX.NDS.128.66.0F3A.W0 4C /r /is4","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Select byte values from xmm2 and xmm3/m128 using mask bits in the specified mask register, xmm4, and store the values into xmm1."
"VPBLENDVB ymm1, ymm2, ymm3/m256, ymm4","VEX.NDS.256.66.0F3A.W0 4C /r /is4","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8[7:4]","","Select byte values from ymm2 and ymm3/m256 from mask specified in the high bit of each byte in ymm4 and store the values into ymm1."
"PBLENDW xmm1, xmm2/m128, imm8","66 0F 3A 0E /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Select words from xmm1 and xmm2/m128 from mask specified in imm8 and store the values into xmm1."
"VPBLENDW xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 0E /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Select words from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1."
"VPBLENDW ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.WIG 0E /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Select words from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1."
"PCLMULQDQ xmm1, xmm2/m128, imm8","66 0F 3A 44 /r ib","Valid","Valid","Invalid","PCLMULQDQ","ModRM:reg (r, w)","ModRM:r/m (r)","imm8","NA","","Carry-less multiplication of one quadword of xmm1 by one quadword of xmm2/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm1 and xmm2/m128 should be used."
"VPCLMULQDQ xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.WIG 44 /r ib","Valid","Valid","Invalid","PCLMULQDQ AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Carry-less multiplication of one quadword of xmm2 by one quadword of xmm3/m128, stores the 128-bit result in xmm1. The immediate is used to determine which quadwords of xmm2 and xmm3/m128 should be used."
"PCMPEQB mm,mm/m64",NP 0F 74 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed bytes in mm/m64 and mm for equality.
"PCMPEQB xmm1,xmm2/m128",66 0F 74 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed bytes in xmm2/m128 and xmm1 for equality.
"PCMPEQW mm,mm/m64",NP 0F 75 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed words in mm/m64 and mm for equality.
"PCMPEQW xmm1,xmm2/m128",66 0F 75 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed words in xmm2/m128 and xmm1 for equality.
"PCMPEQD mm,mm/m64",NP 0F 76 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed doublewords in mm/m64 and mm for equality.
"PCMPEQD xmm1,xmm2/m128",66 0F 76 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed doublewords in xmm2/m128 and xmm1 for equality.
"VPCMPEQB xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 74 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed bytes in xmm3/m128 and xmm2 for equality.
"VPCMPEQW xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 75 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed words in xmm3/m128 and xmm2 for equality.
"VPCMPEQD xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 76 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed doublewords in xmm3/m128 and xmm2 for equality.
"VPCMPEQB ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 74 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed bytes in ymm3/m256 and ymm2 for equality.
"VPCMPEQW ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 75 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed words in ymm3/m256 and ymm2 for equality.
"VPCMPEQD ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 76 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed doublewords in ymm3/m256 and ymm2 for equality.
"VPCMPEQD k1 {k2},xmm2,xmm3/m128/m32bcst",EVEX.NDS.128.66.0F.W0 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Equal between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2},ymm2,ymm3/m256/m32bcst",EVEX.NDS.256.66.0F.W0 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Equal between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQD k1 {k2},zmm2,zmm3/m512/m32bcst",EVEX.NDS.512.66.0F.W0 76 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Equal between int32 vectors in zmm2 and zmm3/m512/m32bcst, and set destination k1 according to the comparison results under writemask k2."
"VPCMPEQB k1 {k2},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG 74 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed bytes in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQB k1 {k2},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG 74 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed bytes in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQB k1 {k2},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG 74 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed bytes in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG 75 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed words in xmm3/m128 and xmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG 75 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed words in ymm3/m256 and ymm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQW k1 {k2},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG 75 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed words in zmm3/m512 and zmm2 for equality and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"PCMPEQQ xmm1, xmm2/m128","66 0F 38 29 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed qwords in xmm2/m128 and xmm1 for equality."
"VPCMPEQQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 29 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed quadwords in xmm3/m128 and xmm2 for equality."
"VPCMPEQQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 29 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed quadwords in ymm3/m256 and ymm2 for equality."
"VPCMPEQQ k1 {k2}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Equal between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Equal between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPEQQ k1 {k2}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Equal between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"PCMPESTRI xmm1, xmm2/m128, imm8","66 0F 3A 61 /r imm8","Valid","Valid","Invalid","SSE4_2","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX."
"VPCMPESTRI xmm1, xmm2/m128, imm8","VEX.128.66.0F3A 61 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with explicit lengths, generating an index, and storing the result in ECX."
"PCMPESTRM xmm1, xmm2/m128, imm8","66 0F 3A 60 /r imm8","Valid","Valid","Invalid","SSE4_2","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0."
"VPCMPESTRM xmm1, xmm2/m128, imm8","VEX.128.66.0F3A 60 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with explicit lengths, generating a mask, and storing the result in XMM0."
"PCMPGTB mm,mm/m64",NP 0F 64 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed byte integers in mm and mm/m64 for greater than.
"PCMPGTB xmm1,xmm2/m128",66 0F 64 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed byte integers in xmm1 and xmm2/m128 for greater than.
"PCMPGTW mm,mm/m64",NP 0F 65 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed word integers in mm and mm/m64 for greater than.
"PCMPGTW xmm1,xmm2/m128",66 0F 65 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed word integers in xmm1 and xmm2/m128 for greater than.
"PCMPGTD mm,mm/m64",NP 0F 66 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed doubleword integers in mm and mm/m64 for greater than.
"PCMPGTD xmm1,xmm2/m128",66 0F 66 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Compare packed signed doubleword integers in xmm1 and xmm2/m128 for greater than.
"VPCMPGTB xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 64 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than.
"VPCMPGTW xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 65 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed word integers in xmm2 and xmm3/m128 for greater than.
"VPCMPGTD xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG 66 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed doubleword integers in xmm2 and xmm3/m128 for greater than.
"VPCMPGTB ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 64 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than.
"VPCMPGTW ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 65 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed word integers in ymm2 and ymm3/m256 for greater than.
"VPCMPGTD ymm1,ymm2,ymm3/m256",VEX.NDS.256.66.0F.WIG 66 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Compare packed signed doubleword integers in ymm2 and ymm3/m256 for greater than.
"VPCMPGTD k1 {k2},xmm2,xmm3/m128/m32bcst",EVEX.NDS.128.66.0F.W0 66 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Greater between int32 vector xmm2 and int32 vector xmm3/m128/m32bcst,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTD k1 {k2},ymm2,ymm3/m256/m32bcst",EVEX.NDS.256.66.0F.W0 66 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Greater between int32 vector ymm2 and int32 vector ymm3/m256/m32bcst,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTD k1 {k2},zmm2,zmm3/m512/m32bcst",EVEX.NDS.512.66.0F.W0 66 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Compare Greater between int32 elements in zmm2 and zmm3/m512/m32bcst,and set destination k1 according to the comparison results under writemask. k2."
"VPCMPGTB k1 {k2},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG 64 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed byte integers in xmm2 and xmm3/m128 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTB k1 {k2},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG 64 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed byte integers in ymm2 and ymm3/m256 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTB k1 {k2},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG 64 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed byte integers in zmm2 and zmm3/m512 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG 65 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed word integers in xmm2 and xmm3/m128 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2},ymm2,ymm3/m256",EVEX.NDS.256.66.0F.WIG 65 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed word integers in ymm2 and ymm3/m256 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"VPCMPGTW k1 {k2},zmm2,zmm3/m512",EVEX.NDS.512.66.0F.WIG 65 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Compare packed signed word integers in zmm2 and zmm3/m512 for greater than,and set vector mask k1 to reflect the zero/nonzero status of each element of the result,under writemask."
"PCMPGTQ xmm1,xmm2/m128","66 0F 38 37 /r","Valid","Valid","Invalid","SSE4_2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed qwords in xmm2/m128 and xmm1 for greater than."
"VPCMPGTQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 37 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed qwords in xmm2 and xmm3/m128 for greater than."
"VPCMPGTQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 37 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed qwords in ymm2 and ymm3/m256 for greater than."
"VPCMPGTQ k1 {k2}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Greater between int64 vector xmm2 and int64 vector xmm3/m128/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Greater between int64 vector ymm2 and int64 vector ymm3/m256/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"VPCMPGTQ k1 {k2}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 37 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare Greater between int64 vector zmm2 and int64 vector zmm3/m512/m64bcst, and set vector mask k1 to reflect the zero/nonzero status of each element of the result, under writemask."
"PCMPISTRI xmm1, xmm2/m128, imm8","66 0F 3A 63 /r imm8","Valid","Valid","Invalid","SSE4_2","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX."
"VPCMPISTRI xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.WIG 63 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with implicit lengths, generating an index, and storing the result in ECX."
"PCMPISTRM xmm1, xmm2/m128, imm8","66 0F 3A 62 /r imm8","Valid","Valid","Invalid","SSE4_2","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with implicit lengths, generating a mask, and storing the result in XMM0."
"VPCMPISTRM xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.WIG 62 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","imm8","NA","","Perform a packed comparison of string data with implicit lengths, generating a Mask, and storing the result in XMM0."
"PDEP r32a, r32b, r/m32","VEX.NDS.LZ.F2.0F38.W0 F5 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Parallel deposit of bits from r32b using mask in r/m32, result is writ-ten to r32a."
"PDEP r64a, r64b, r/m64","VEX.NDS.LZ.F2.0F38.W1 F5 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Parallel deposit of bits from r64b using mask in r/m64, result is writ-ten to r64a."
"PEXT r32a, r32b, r/m32","VEX.NDS.LZ.F3.0F38.W0 F5 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Parallel extract of bits from r32b using mask in r/m32, result is writ-ten to r32a."
"PEXT r64a, r64b, r/m64","VEX.NDS.LZ.F3.0F38.W1 F5 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Parallel extract of bits from r64b using mask in r/m64, result is writ-ten to r64a."
"PEXTRB reg/m8,xmm2,imm8",66 0F 3A 14 /r ib,Valid,Valid,Invalid,SSE4_1,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r32 or r64 are zeroed.
"PEXTRD r/m32,xmm2,imm8",66 0F 3A 16 /r ib,Valid,Valid,Invalid,SSE4_2,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r/m32.
"PEXTRQ r/m64,xmm2,imm8",66 REX.W 0F 3A 16 /r ib,Valid,Invalid,Invalid,SSE4_3,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a qword integer value from xmm2 at the source qword offset specified by imm8 into r/m64.
"VPEXTRB reg/m8,xmm2,imm8",VEX.128.66.0F3A.W0 14 /r ib,Valid,Valid,Invalid,AVX,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros.
"VPEXTRD r32/m32,xmm2,imm8",VEX.128.66.0F3A.W0 16 /r ib,Valid,Valid,Invalid,AVX,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
"VPEXTRQ r64/m64,xmm2,imm8",VEX.128.66.0F3A.W1 16 /r ib,Valid,Valid,Invalid,AVX,ModRM:r/m (w),ModRM:reg (r),NA,NA,,Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
"VPEXTRB reg/m8,xmm2,imm8",EVEX.128.66.0F3A.WIG 14 /r ib,Valid,Valid,Invalid,AVX512BW,ModRM:r/m (w),ModRM:reg (r),NA,NA,Tuple1 Scalar,Extract a byte integer value from xmm2 at the source byte offset specified by imm8 into reg or m8. The upper bits of r64/r32 is filled with zeros.
"VPEXTRD r32/m32,xmm2,imm8",EVEX.128.66.0F3A.W0 16 /r ib,Valid,Valid,Invalid,AVX512DQ,ModRM:r/m (w),ModRM:reg (r),NA,NA,Tuple1 Scalar,Extract a dword integer value from xmm2 at the source dword offset specified by imm8 into r32/m32.
"VPEXTRQ r64/m64,xmm2,imm8",EVEX.128.66.0F3A.W1 16 /r ib,Valid,Invalid,Invalid,AVX512DQ,ModRM:r/m (w),ModRM:reg (r),NA,NA,Tuple1 Scalar,Extract a qword integer value from xmm2 at the source dword offset specified by imm8 into r64/m64.
"PEXTRW reg, mm, imm8","NP 0F C5 /r ib","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Extract the word specified by imm8 from mm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed."
"PEXTRW reg, xmm, imm8","66 0F C5 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Extract the word specified by imm8 from xmm and move it to reg, bits 15-0. The upper bits of r32 or r64 is zeroed. 66 0F 3A 15"
"PEXTRW reg/m16, xmm, imm8","/r ib","Valid","Valid","Invalid","SSE4_1","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","NA","Extract the word specified by imm8 from xmm and copy it to lowest 16 bits of reg or m16. Zero-extend the result in the destination, r32 or r64."
"VPEXTRW reg, xmm1, imm8","VEX.128.66.0F.W0 C5 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg/m16, xmm2, imm8","VEX.128.66.0F3A.W0 15 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","NA","Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg, xmm1, imm8","EVEX.128.66.0F.WIG C5 /r ib","Valid","Valid","Invalid","AVX512B","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Extract the word specified by imm8 from xmm1 and move it to reg, bits 15:0. Zero-extend the result. The upper bits of r64/r32 is filled with zeros."
"VPEXTRW reg/m16, xmm2, imm8","EVEX.128.66.0F3A.WIG 15 /r ib","Valid","Valid","Invalid","AVX512B","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","Tuple1 Scalar","Extract a word integer value from xmm2 at the source word offset specified by imm8 into reg or m16. The upper bits of r64/r32 is filled with zeros."
"PHADDSW mm1, mm2/m64","NP 0F 38 03 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 16-bit signed integers horizontally, pack saturated integers to mm1."
"PHADDSW xmm1, xmm2/m128","66 0F 38 03 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 16-bit signed integers horizontally, pack saturated integers to xmm1."
"VPHADDSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 03 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 16-bit signed integers horizontally, pack saturated integers to xmm1."
"VPHADDSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 03 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 16-bit signed integers horizontally, pack saturated integers to ymm1."
"PHADDW mm1, mm2/m64","NP 0F 38 01 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 16-bit integers horizontally, pack to mm1."
"PHADDW xmm1, xmm2/m128","66 0F 38 01 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 16-bit integers horizontally, pack to xmm1."
"PHADDD mm1, mm2/m64","NP 0F 38 02 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 32-bit integers horizontally, pack to mm1."
"PHADDD xmm1, xmm2/m128","66 0F 38 02 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Add 32-bit integers horizontally, pack to xmm1."
"VPHADDW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 01 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 16-bit integers horizontally, pack to xmm1."
"VPHADDD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 02 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 32-bit integers horizontally, pack to xmm1."
"VPHADDW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 01 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 16-bit signed integers horizontally, pack to ymm1."
"VPHADDD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 02 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Add 32-bit signed integers horizontally, pack to ymm1."
"PHMINPOSUW xmm1, xmm2/m128","66 0F 38 41 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1."
"VPHMINPOSUW xmm1, xmm2/m128","VEX.128.66.0F38.WIG 41 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Find the minimum unsigned word in xmm2/m128 and place its value in the low word of xmm1 and its index in the second-lowest word of xmm1."
"PHSUBSW mm1, mm2/m64","NP 0F 38 07 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 16-bit signed integer horizontally, pack saturated integers to mm1."
"PHSUBSW xmm1, xmm2/m128","66 0F 38 07 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1."
"VPHSUBSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 07 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 16-bit signed integer horizontally, pack saturated integers to xmm1."
"VPHSUBSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 07 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 16-bit signed integer horizontally, pack saturated integers to ymm1."
"PHSUBW mm1, mm2/m64","NP 0F 38 05 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 16-bit signed integers horizontally, pack to mm1."
"PHSUBW xmm1, xmm2/m128","66 0F 38 05 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 16-bit signed integers horizontally, pack to xmm1."
"PHSUBD mm1, mm2/m64","NP 0F 38 06 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 32-bit signed integers horizontally, pack to mm1."
"PHSUBD xmm1, xmm2/m128","66 0F 38 06 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract 32-bit signed integers horizontally, pack to xmm1."
"VPHSUBW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 05 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 16-bit signed integers horizontally, pack to xmm1."
"VPHSUBD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 06 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 32-bit signed integers horizontally, pack to xmm1."
"VPHSUBW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 05 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 16-bit signed integers horizontally, pack to ymm1."
"VPHSUBD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 06 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Subtract 32-bit signed integers horizontally, pack to ymm1."
"PINSRB xmm1,r32/m8,imm8",66 0F 3A 20 /r ib,Valid,Valid,Invalid,SSE4_1,ModRM:reg (w),ModRM:r/m (r),imm8,NA,,Insert a byte integer value from r32/m8 into xmm1 at the destination element in xmm1 specified by imm8.
"PINSRD xmm1,r/m32,imm8",66 0F 3A 22 /r ib,Valid,Valid,Invalid,SSE4_1,ModRM:reg (w),ModRM:r/m (r),imm8,NA,,Insert a dword integer value from r/m32 into the xmm1 at the destination element specified by imm8.
"PINSRQ xmm1,r/m64,imm8",66 REX.W 0F 3A 22 /r ib,Valid,Invalid,Invalid,SSE4_1,ModRM:reg (w),ModRM:r/m (r),imm8,NA,,Insert a qword integer value from r/m64 into the xmm1 at the destination element specified by imm8.
"VPINSRB xmm1,xmm2,r32/m8,imm8",VEX.NDS.128.66.0F3A.W0 20 /r ib,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),imm8,,Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
"VPINSRD xmm1,xmm2,r/m32,imm8",VEX.NDS.128.66.0F3A.W0 22 /r ib,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),imm8,,Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
"VPINSRQ xmm1,xmm2,r/m64,imm8",VEX.NDS.128.66.0F3A.W1 22 /r ib,Valid,Invalid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),imm8,,Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
"VPINSRB xmm1,xmm2,r32/m8,imm8",EVEX.NDS.128.66.0F3A.WIG 20 /r ib,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),imm8,Tuple1 Scalar,Merge a byte integer value from r32/m8 and rest from xmm2 into xmm1 at the byte offset in imm8.
"VPINSRD xmm1,xmm2,r32/m32,imm8",EVEX.NDS.128.66.0F3A.W0 22 /r ib,Valid,Valid,Invalid,AVX512DQ,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),imm8,Tuple1 Scalar,Insert a dword integer value from r32/m32 and rest from xmm2 into xmm1 at the dword offset in imm8.
"VPINSRQ xmm1,xmm2,r64/m64,imm8",EVEX.NDS.128.66.0F3A.W1 22 /r ib,Valid,Invalid,Invalid,AVX512DQ,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),imm8,Tuple1 Scalar,Insert a qword integer value from r64/m64 and rest from xmm2 into xmm1 at the qword offset in imm8.
"PINSRW mm, r32/m16, imm8","NP 0F C4 /r ib","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Insert the low word from r32 or from m16 into mm at the word position specified by imm8."
"PINSRW xmm, r32/m16, imm8","66 0F C4 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Move the low word of r32 or from m16 into xmm at the word position specified by imm8."
"VPINSRW xmm1, xmm2, r32/m16, imm8","VEX.NDS.128.66.0F.W0 C4 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","NA","Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8."
"VPINSRW xmm1, xmm2, r32/m16, imm8","EVEX.NDS.128.66.0F.WIG C4 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Insert a word integer value from r32/m16 and rest from xmm2 into xmm1 at the word offset in imm8."
"PMADDUBSW mm1, mm2/m64","NP 0F 38 04 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to mm1."
"PMADDUBSW xmm1, xmm2/m128","66 0F 38 04 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1."
"VPMADDUBSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1."
"VPMADDUBSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1."
"VPMADDUBSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to xmm1 under writemask k1."
"VPMADDUBSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to ymm1 under writemask k1."
"VPMADDUBSW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 04 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply signed and unsigned bytes, add horizontal pair of signed words, pack saturated signed-words to zmm1 under writemask k1."
"PMADDWD mm, mm/m64","NP 0F F5 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed words in mm by the packed words in mm/m64, add adjacent doubleword results, and store in mm."
"PMADDWD xmm1, xmm2/m128","66 0F F5 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed word integers in xmm1 by the packed word integers in xmm2/m128, add adjacent doubleword results, and store in xmm1."
"VPMADDWD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1."
"VPMADDWD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1."
"VPMADDWD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed word integers in xmm2 by the packed word integers in xmm3/m128, add adjacent doubleword results, and store in xmm1 under writemask k1."
"VPMADDWD ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed word integers in ymm2 by the packed word integers in ymm3/m256, add adjacent doubleword results, and store in ymm1 under writemask k1."
"VPMADDWD zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG F5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed word integers in zmm2 by the packed word integers in zmm3/m512, add adjacent doubleword results, and store in zmm1 under writemask k1."
"PMAXSW mm1, mm2/m64","NP 0F EE /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare signed word integers in mm2/m64 and mm1 and return maximum values."
"PMAXSB xmm1, xmm2/m128","66 0F 38 3C /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"PMAXSW xmm1, xmm2/m128","66 0F EE /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1."
"PMAXSD xmm1, xmm2/m128","66 0F 38 3D /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"VPMAXSB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed word integers in xmm3/m128 and xmm2 and store packed maximum values in xmm1."
"VPMAXSD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 3D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXSB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed word integers in ymm3/m256 and ymm2 and store packed maximum values in ymm1."
"VPMAXSD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 3D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXSB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 3C /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG EE /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXSD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 3D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 using writemask k1.PMAXSB/PMAXSW/PMAXSD/PMAXSQâ€”Maximum of Packed Signed Integers"
"PMAXUB mm1, mm2/m64","NP 0F DE /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare unsigned byte integers in mm2/m64 and mm1 and returns maximum values."
"PMAXUB xmm1, xmm2/m128","66 0F DE /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"PMAXUW xmm1, xmm2/m128","66 0F 38 3E/r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned word integers in xmm2/m128 and xmm1 and stores maximum packed values in xmm1."
"VPMAXUB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F DE /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXUW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38 3E/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned word integers in xmm3/m128 and xmm2 and store maximum packed values in xmm1."
"VPMAXUB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F DE /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXUW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38 3E/r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned word integers in ymm3/m256 and ymm2 and store maximum packed values in ymm1."
"VPMAXUB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG DE /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"VPMAXUW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1 under writemask k1."
"VPMAXUW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1 under writemask k1."
"VPMAXUW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 3E /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in zmm2 and zmm3/m512 and store packed maximum values in zmm1 under writemask k1."
"PMAXUD xmm1, xmm2/m128","66 0F 38 3F /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed maximum values in xmm1."
"VPMAXUD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 3F /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed maximum values in xmm1."
"VPMAXUD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 3F /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed maximum values in ymm1."
"VPMAXUD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed maximum values in xmm1 under writemask k1."
"VPMAXUD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed maximum values in ymm1 under writemask k1."
"VPMAXUD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 3F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed maximum values in zmm1 under writemask k1."
"VPMAXUQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed maximum values in xmm1 under writemask k1."
"VPMAXUQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed maximum values in ymm1 under writemask k1."
"VPMAXUQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 3F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed maximum values in zmm1 under writemask k1."
"PMINSW mm1, mm2/m64","NP 0F EA /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare signed word integers in mm2/m64 and mm1 and return minimum values."
"PMINSB xmm1, xmm2/m128","66 0F 38 38 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"PMINSW xmm1, xmm2/m128","66 0F EA /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1."
"VPMINSB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38 38 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F EA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMINSB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38 38 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F EA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMINSB xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSB ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSB zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 38 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINSW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG EA /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed word integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"PMINSD xmm1, xmm2/m128","66 0F 38 39 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed signed dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"VPMINSD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 39 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINSD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 39 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed signed dword integers in ymm2 and ymm3/m128 and store packed minimum values in ymm1."
"VPMINSD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINSQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed qword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINSQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed qword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINSQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed signed qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1."
"PMINUB mm1, mm2/m64","NP 0F DA /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare unsigned byte integers in mm2/m64 and mm1 and returns minimum values."
"PMINUB xmm1, xmm2/m128","66 0F DA /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned byte integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"PMINUW xmm1, xmm2/m128","66 0F 38 3A/r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned word integers in xmm2/m128 and xmm1 and store packed minimum values in xmm1."
"VPMINUB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F DA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38 3A/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1."
"VPMINUB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F DA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38 3A/r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1."
"VPMINUB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F DA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1 under writemask k1."
"VPMINUB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F DA /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1 under writemask k1."
"VPMINUB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F DA /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte integers in zmm2 and zmm3/m512 and store packed minimum values in zmm1 under writemask k1."
"VPMINUW xmm1{k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38 3A/r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in xmm3/m128 and xmm2 and return packed minimum values in xmm1 under writemask k1."
"VPMINUW ymm1{k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38 3A/r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in ymm3/m256 and ymm2 and return packed minimum values in ymm1 under writemask k1."
"VPMINUW zmm1{k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38 3A/r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned word integers in zmm3/m512 and zmm2 and return packed minimum values in zmm1 under writemask k1."
"PMINUD xmm1, xmm2/m128","66 0F 38 3B /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Compare packed unsigned dword integers in xmm1 and xmm2/m128 and store packed minimum values in xmm1."
"VPMINUD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 3B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned dword integers in xmm2 and xmm3/m128 and store packed minimum values in xmm1."
"VPMINUD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 3B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Compare packed unsigned dword integers in ymm2 and ymm3/m256 and store packed minimum values in ymm1."
"VPMINUD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in xmm2 and xmm3/m128/m32bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in ymm2 and ymm3/m256/m32bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 3B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned dword integers in zmm2 and zmm3/m512/m32bcst and store packed minimum values in zmm1 under writemask k1."
"VPMINUQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in xmm2 and xmm3/m128/m64bcst and store packed minimum values in xmm1 under writemask k1."
"VPMINUQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in ymm2 and ymm3/m256/m64bcst and store packed minimum values in ymm1 under writemask k1."
"VPMINUQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 3B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Compare packed unsigned qword integers in zmm2 and zmm3/m512/m64bcst and store packed minimum values in zmm1 under writemask k1."
"PMOVMSKB reg, mm","NP 0F D7 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move a byte mask of mm to reg. The upper bits of r32 or r64 are zeroed"
"PMOVMSKB reg, xmm","66 0F D7 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move a byte mask of xmm to reg. The upper bits of r32 or r64 are zeroed"
"VPMOVMSKB reg, xmm1","VEX.128.66.0F.WIG D7 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move a byte mask of xmm1 to reg. The upper bits of r32 or r64 are filled with zeros."
"VPMOVMSKB reg, ymm1","VEX.256.66.0F.WIG D7 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Move a 32-bit mask of ymm1 to reg. The upper bits of r64 are filled with zeros."
"PMOVSXBW xmm1, xmm2/m64","66 0f 38 20 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"PMOVSXBD xmm1, xmm2/m32","66 0f 38 21 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"PMOVSXBQ xmm1, xmm2/m16","66 0f 38 22 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"PMOVSXWD xmm1, xmm2/m64","66 0f 38 23/r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"PMOVSXWQ xmm1, xmm2/m32","66 0f 38 24 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"PMOVSXDQ xmm1, xmm2/m64","66 0f 38 25 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVSXBW xmm1, xmm2/m64","VEX.128.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVSXBD xmm1, xmm2/m32","VEX.128.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVSXBQ xmm1, xmm2/m16","VEX.128.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVSXWD xmm1, xmm2/m64","VEX.128.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVSXWQ xmm1, xmm2/m32","VEX.128.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMOVSXDQ xmm1, xmm2/m64","VEX.128.66.0F38.WIG 25 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVSXBW ymm1, xmm2/m128","VEX.256.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXBD ymm1, xmm2/m64","VEX.256.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVSXBQ ymm1, xmm2/m32","VEX.256.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVSXWD ymm1, xmm2/m128","VEX.256.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVSXWQ ymm1, xmm2/m64","VEX.256.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1."
"VPMOVSXDQ ymm1, xmm2/m128","VEX.256.66.0F38.WIG 25 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in ymm1."
"VPMOVSXBW xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 8 packed 8-bit integers in xmm2/m64 to 8 packed 16-bit integers in zmm1."
"VPMOVSXBW ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVSXBW zmm1 {k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 20 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMOVSXBD xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1.PMOVSXâ€”Packed Move with Sign Extend"
"VPMOVSXBD ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBD zmm1 {k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 21 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 16 packed 8-bit integers in the low 16 bytes of xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXBQ xmm1 {k1}{z}, xmm2/m16","EVEX.128.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Sign extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXBQ ymm1 {k1}{z}, xmm2/m32","EVEX.256.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Sign extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXBQ zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.WIG 22 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Sign extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVSXWD xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 4 packed 16-bit integers in the low 8 bytes of ymm2/mem to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWD ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 8 packed 16-bit integers in the low 16 bytes of ymm2/m128 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWD zmm1 {k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 23 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 16 packed 16-bit integers in the low 32 bytes of ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVSXWQ xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVSXWQ ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVSXWQ zmm1 {k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 24 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Sign extend 8 packed 16-bit integers in the low 16 bytes of xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVSXDQ xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 4 packed 32-bit integers in the low 16 bytes of xmm2/m128 to 4 packed 64-bit integers in zmm1 using writemask k1."
"VPMOVSXDQ zmm1 {k1}{z}, ymm2/m256","EVEX.512.66.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Sign extend 8 packed 32-bit integers in the low 32 bytes of ymm2/m256 to 8 packed 64-bit integers in zmm1 using writemask k1."
"PMOVZXBW xmm1, xmm2/m64","66 0f 38 30 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"PMOVZXBD xmm1, xmm2/m32","66 0f 38 31 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"PMOVZXBQ xmm1, xmm2/m16","66 0f 38 32 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"PMOVZXWD xmm1, xmm2/m64","66 0f 38 33 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"PMOVZXWQ xmm1, xmm2/m32","66 0f 38 34 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"PMOVZXDQ xmm1, xmm2/m64","66 0f 38 35 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVZXBW xmm1, xmm2/m64","VEX.128.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXBD xmm1, xmm2/m32","VEX.128.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1."
"VPMOVZXBQ xmm1, xmm2/m16","VEX.128.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1."
"VPMOVZXWD xmm1, xmm2/m64","VEX.128.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1."
"VPMOVZXWQ xmm1, xmm2/m32","VEX.128.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1."
"VPMOVZXDQ xmm1, xmm2/m64","VEX.128.66.0F38.WIG 35 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 2 packed 32-bit integers in the low 8 bytes of xmm2/m64 to 2 packed 64-bit integers in xmm1."
"VPMOVZXBW ymm1, xmm2/m128","VEX.256.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXBD ymm1, xmm2/m64","VEX.256.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1."
"VPMOVZXBQ ymm1, xmm2/m32","VEX.256.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1."
"VPMOVZXWD ymm1, xmm2/m128","VEX.256.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 8 packed 16-bit integers xmm2/m128 to 8 packed 32-bit integers in ymm1."
"VPMOVZXWQ ymm1, xmm2/m64","VEX.256.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in xmm1."
"VPMOVZXDQ ymm1, xmm2/m128","VEX.256.66.0F38.WIG 35 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Zero extend 4 packed 32-bit integers in xmm2/m128 to 4 packed 64-bit integers in ymm1.PMOVZXâ€”Packed Move with Zero Extend"
"VPMOVZXBW xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 16-bit integers in xmm1."
"VPMOVZXBW ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 16-bit integers in ymm1."
"VPMOVZXBW zmm1 {k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 30 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 32 packed 8-bit integers in ymm2/m256 to 32 packed 16-bit integers in zmm1."
"VPMOVZXBD xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBD ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 32-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBD zmm1 {k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 31 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 16 packed 8-bit integers in xmm2/m128 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXBQ xmm1 {k1}{z}, xmm2/m16","EVEX.128.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Zero extend 2 packed 8-bit integers in the low 2 bytes of xmm2/m16 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXBQ ymm1 {k1}{z}, xmm2/m32","EVEX.256.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Zero extend 4 packed 8-bit integers in the low 4 bytes of xmm2/m32 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXBQ zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.WIG 32 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Oct Vector Mem","Zero extend 8 packed 8-bit integers in the low 8 bytes of xmm2/m64 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWD xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 32-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWD ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWD zmm1 {k1}{z}, ymm2/m256","EVEX.512.66.0F38.WIG 33 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Zero extend 16 packed 16-bit integers in ymm2/m256 to 16 packed 32-bit integers in zmm1 subject to writemask k1."
"VPMOVZXWQ xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 2 packed 16-bit integers in the low 4 bytes of xmm2/m32 to 2 packed 64-bit integers in xmm1 subject to writemask k1."
"VPMOVZXWQ ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 4 packed 16-bit integers in the low 8 bytes of xmm2/m64 to 4 packed 64-bit integers in ymm1 subject to writemask k1."
"VPMOVZXWQ zmm1 {k1}{z}, xmm2/m128","EVEX.512.66.0F38.WIG 34 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Quarter Vector Mem","Zero extend 8 packed 16-bit integers in xmm2/m128 to 8 packed 64-bit integers in zmm1 subject to writemask k1."
"PMULDQ xmm1, xmm2/m128","66 0F 38 28 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply packed signed doubleword integers in xmm1 by packed signed doubleword integers in xmm2/m128, and store the quadword results in xmm1."
"VPMULDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 28 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPMULDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 28 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPMULDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed signed doubleword integers in xmm2 by packed signed doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 using writemask k1."
"VPMULDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed signed doubleword integers in ymm2 by packed signed doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 using writemask k1."
"VPMULDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed signed doubleword integers in zmm2 by packed signed doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 using writemask k1."
"PMULHRSW mm1, mm2/m64","NP 0F 38 0B /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to mm1."
"PMULHRSW xmm1, xmm2/m128","66 0F 38 0B /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1."
"VPMULHRSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1."
"VPMULHRSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1."
"VPMULHRSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to xmm1 under writemask k1."
"VPMULHRSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to ymm1 under writemask k1."
"VPMULHRSW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 0B /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply 16-bit signed words, scale and round signed doublewords, pack high 16 bits to zmm1 under writemask k1."
"PMULHUW mm1, mm2/m64","NP 0F E4 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed unsigned word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1."
"PMULHUW xmm1, xmm2/m128","66 0F E4 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed unsigned word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1."
"VPMULHUW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULHUW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULHUW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed unsigned word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHUW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed unsigned word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHUW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG E4 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed unsigned word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"PMULHW mm, mm/m64","NP 0F E5 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed signed word integers in mm1 register and mm2/m64, and store the high 16 bits of the results in mm1."
"PMULHW xmm1, xmm2/m128","66 0F E5 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the high 16 bits of the results in xmm1."
"VPMULHW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1."
"VPMULHW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1."
"VPMULHW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the high 16 bits of the results in xmm1 under writemask k1."
"VPMULHW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the high 16 bits of the results in ymm1 under writemask k1."
"VPMULHW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG E5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the high 16 bits of the results in zmm1 under writemask k1."
"PMULLD xmm1, xmm2/m128","66 0F 38 40 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed dword signed integers in xmm1 and xmm2/m128 and store the low 32 bits of each product in xmm1."
"VPMULLD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 40 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1."
"VPMULLD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 40 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed dword signed integers in ymm2 and ymm3/m256 and store the low 32 bits of each product in ymm1."
"VPMULLD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed dword signed integers in xmm2 and xmm3/m128/m32bcst and store the low 32 bits of each product in xmm1 under writemask k1."
"VPMULLD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed dword signed integers in ymm2 and ymm3/m256/m32bcst and store the low 32 bits of each product in ymm1 under writemask k1."
"VPMULLD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 40 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed dword signed integers in zmm2 and zmm3/m512/m32bcst and store the low 32 bits of each product in zmm1 under writemask k1."
"VPMULLQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 40 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed qword signed integers in xmm2 and xmm3/m128/m64bcst and store the low 64 bits of each product in xmm1 under writemask k1."
"VPMULLQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 40 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed qword signed integers in ymm2 and ymm3/m256/m64bcst and store the low 64 bits of each product in ymm1 under writemask k1."
"VPMULLQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 40 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply the packed qword signed integers in zmm2 and zmm3/m512/m64bcst and store the low 64 bits of each product in zmm1 under writemask k1."
"PMULLW mm, mm/m64","NP 0F D5 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed signed word integers in mm1 register and mm2/m64, and store the low 16 bits of the results in mm1."
"PMULLW xmm1, xmm2/m128","66 0F D5 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply the packed signed word integers in xmm1 and xmm2/m128, and store the low 16 bits of the results in xmm1."
"VPMULLW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed dword signed integers in xmm2 and xmm3/m128 and store the low 32 bits of each product in xmm1."
"VPMULLW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1."
"VPMULLW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in xmm2 and xmm3/m128, and store the low 16 bits of the results in xmm1 under writemask k1."
"VPMULLW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in ymm2 and ymm3/m256, and store the low 16 bits of the results in ymm1 under writemask k1."
"VPMULLW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG D5 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Multiply the packed signed word integers in zmm2 and zmm3/m512, and store the low 16 bits of the results in zmm1 under writemask k1."
"PMULUDQ mm1, mm2/m64","NP 0F F4 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply unsigned doubleword integer in mm1 by unsigned doubleword integer in mm2/m64, and store the quadword result in mm1."
"PMULUDQ xmm1, xmm2/m128","66 0F F4 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Multiply packed unsigned doubleword integers in xmm1 by packed unsigned doubleword integers in xmm2/m128, and store the quadword results in xmm1."
"VPMULUDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F4 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128, and store the quadword results in xmm1."
"VPMULUDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG F4 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256, and store the quadword results in ymm1."
"VPMULUDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed unsigned doubleword integers in xmm2 by packed unsigned doubleword integers in xmm3/m128/m64bcst, and store the quadword results in xmm1 under writemask k1."
"VPMULUDQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed unsigned doubleword integers in ymm2 by packed unsigned doubleword integers in ymm3/m256/m64bcst, and store the quadword results in ymm1 under writemask k1."
"VPMULUDQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 F4 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed unsigned doubleword integers in zmm2 by packed unsigned doubleword integers in zmm3/m512/m64bcst, and store the quadword results in zmm1 under writemask k1."
"POP r/m16","8F /0","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Pop top of stack into m16; increment stack pointer."
"POP r/m32","8F /0","Invalid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Pop top of stack into m32; increment stack pointer."
"POP r/m64","8F /0","Valid","Invalid","Invalid","","ModRM:r/m (w)","NA","NA","NA","","Pop top of stack into m64; increment stack pointer. Cannot encode 32-bit operand size."
"POP r16","58 +rw","Valid","Valid","Valid","","opcode +rd (w)","NA","NA","NA","","Pop top of stack into r16; increment stack pointer."
"POP r32","58 +rd","Invalid","Valid","Valid","","opcode +rd (w)","NA","NA","NA","","Pop top of stack into r32; increment stack pointer."
"POP r64","58 +rd","Valid","Invalid","Invalid","","opcode +rd (w)","NA","NA","NA","","Pop top of stack into r64; increment stack pointer. Cannot encode 32-bit operand size."
"POP DS","1F","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into DS; increment stack pointer."
"POP ES","07","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into ES; increment stack pointer."
"POP SS","17","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into SS; increment stack pointer."
"POP FS","0F A1","Valid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into FS; increment stack pointer by 16 bits."
"POP FS","0F A1","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into FS; increment stack pointer by 32 bits."
"POP FS","0F A1","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Pop top of stack into FS; increment stack pointer by 64 bits."
"POP GS","0F A9","Valid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into GS; increment stack pointer by 16 bits."
"POP GS","0F A9","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into GS; increment stack pointer by 32 bits."
"POP GS","0F A9","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Pop top of stack into GS; increment stack pointer by 64 bits."
"POPA","61","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop DI, SI, BP, BX, DX, CX, and AX."
"POPAD","61","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop EDI, ESI, EBP, EBX, EDX, ECX, and EAX."
"POPCNT r16, r/m16","F3 0F B8 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","POPCNT on r/m16"
"POPCNT r32, r/m32","F3 0F B8 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","POPCNT on r/m32"
"POPCNT r64, r/m64","F3 REX.W 0F B8 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","POPCNT on r/m64"
"POPF","9D","Valid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into lower 16 bits of EFLAGS."
"POPFD","9D","Invalid","Valid","Valid","","NA","NA","NA","NA","","Pop top of stack into EFLAGS."
"POPFQ","9D","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Pop top of stack and zero-extend into RFLAGS."
"POR mm, mm/m64","NP 0F EB /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise OR of mm/m64 and mm."
"POR xmm1, xmm2/m128","66 0F EB /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise OR of xmm2/m128 and xmm1."
"VPOR xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG EB /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise OR of xmm2/m128 and xmm3."
"VPOR ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG EB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise OR of ymm2/m256 and ymm3."
"VPORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed doubleword integers in xmm2 and xmm3/m128/m32bcst using writemask k1."
"VPORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed doubleword integers in ymm2 and ymm3/m256/m32bcst using writemask k1."
"VPORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F.W0 EB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed quadword integers in xmm2 and xmm3/m128/m64bcst using writemask k1."
"VPORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed quadword integers in ymm2 and ymm3/m256/m64bcst using writemask k1."
"VPORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 EB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise OR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"PREFETCHT0 m8","0F 18 /1","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Move data from m8 closer to the processor using T0 hint."
"PREFETCHT1 m8","0F 18 /2","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Move data from m8 closer to the processor using T1 hint."
"PREFETCHT2 m8","0F 18 /3","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Move data from m8 closer to the processor using T2 hint."
"PREFETCHNTA m8","0F 18 /0","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Move data from m8 closer to the processor using NTA hint."
"PREFETCHW m8","0F 0D /1","Valid","Valid","Invalid","PRFCHW","","","","","","Move data from m8 closer to the processor in anticipation of a write."
"PREFETCHWT1 m8","0F 0D /2","Valid","Valid","Invalid","PREFETCHWT1","","","","","","Move data from m8 closer to the processor using T1 hint with intent to write."
"PSADBW mm1, mm2/m64","NP 0F F6 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Computes the absolute differences of the packed unsigned byte integers from mm2 /m64 and mm1; differences are then summed to produce an unsigned word integer result."
"PSADBW xmm1, xmm2/m128","66 0F F6 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Computes the absolute differences of the packed unsigned byte integers from xmm2/m128 and xmm1; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results."
"VPSADBW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Computes the absolute differences of the packed unsigned byte integers from xmm3/m128 and xmm2; the 8 low differences and 8 high differences are then summed separately to produce two unsigned word integer results."
"VPSADBW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Computes the absolute differences of the packed unsigned byte integers from ymm3/m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSADBW xmm1, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from xmm3/m128 and xmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSADBW ymm1, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from ymm3/m256 and ymm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"VPSADBW zmm1, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG F6 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Computes the absolute differences of the packed unsigned byte integers from zmm3/m512 and zmm2; then each consecutive 8 differences are summed separately to produce four unsigned word integer results."
"PSHUFB mm1, mm2/m64","NP 0F 38 00 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shuffle bytes in mm1 according to contents of mm2/m64."
"PSHUFB xmm1, xmm2/m128","66 0F 38 00 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shuffle bytes in xmm1 according to contents of xmm2/m128."
"VPSHUFB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shuffle bytes in xmm2 according to contents of xmm3/m128."
"VPSHUFB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shuffle bytes in ymm2 according to contents of ymm3/m256."
"VPSHUFB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shuffle bytes in xmm2 according to contents of xmm3/m128 under write mask k1."
"VPSHUFB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shuffle bytes in ymm2 according to contents of ymm3/m256 under write mask k1."
"VPSHUFB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.WIG 00 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shuffle bytes in zmm2 according to contents of zmm3/m512 under write mask k1."
"PSHUFD xmm1, xmm2/m128, imm8","66 0F 70 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFD xmm1, xmm2/m128, imm8","VEX.128.66.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the doublewords in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFD ymm1, ymm2/m256, imm8","VEX.256.66.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the doublewords in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Shuffle the doublewords in xmm2/m128/m32bcst based on the encoding in imm8 and store the result in xmm1 using writemask k1."
"VPSHUFD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Shuffle the doublewords in ymm2/m256/m32bcst based on the encoding in imm8 and store the result in ymm1 using writemask k1."
"VPSHUFD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.512.66.0F.W0 70 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Shuffle the doublewords in zmm2/m512/m32bcst based on the encoding in imm8 and store the result in zmm1 using writemask k1."
"PSHUFHW xmm1, xmm2/m128, imm8","F3 0F 70 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFHW xmm1, xmm2/m128, imm8","VEX.128.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFHW ymm1, ymm2/m256, imm8","VEX.256.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFHW xmm1 {k1}{z}, xmm2/m128, imm8","EVEX.128.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the high words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1."
"VPSHUFHW ymm1 {k1}{z}, ymm2/m256, imm8","EVEX.256.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the high words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1."
"VPSHUFHW zmm1 {k1}{z}, zmm2/m512, imm8","EVEX.512.F3.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the high words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1."
"PSHUFLW xmm1, xmm2/m128, imm8","F2 0F 70 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFLW xmm1, xmm2/m128, imm8","VEX.128.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1."
"VPSHUFLW ymm1, ymm2/m256, imm8","VEX.256.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","NA","Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1."
"VPSHUFLW xmm1 {k1}{z}, xmm2/m128, imm8","EVEX.128.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the low words in xmm2/m128 based on the encoding in imm8 and store the result in xmm1 under write mask k1."
"VPSHUFLW ymm1 {k1}{z}, ymm2/m256, imm8","EVEX.256.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the low words in ymm2/m256 based on the encoding in imm8 and store the result in ymm1 under write mask k1."
"VPSHUFLW zmm1 {k1}{z}, zmm2/m512, imm8","EVEX.512.F2.0F.WIG 70 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector Mem","Shuffle the low words in zmm2/m512 based on the encoding in imm8 and store the result in zmm1 under write mask k1."
"PSHUFW mm1, mm2/m64, imm8","NP 0F 70 /r ib","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Shuffle the words in mm2/m64 based on the encoding in imm8 and store the result in mm1."
"PSIGNB mm1, mm2/m64","NP 0F 38 08 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed byte integers in mm1 depending on the corresponding sign in mm2/m64."
"PSIGNB xmm1, xmm2/m128","66 0F 38 08 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed byte integers in xmm1 depending on the corresponding sign in xmm2/m128."
"PSIGNW mm1, mm2/m64","NP 0F 38 09 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed word integers in mm1 depending on the corresponding sign in mm2/m128."
"PSIGNW xmm1, xmm2/m128","66 0F 38 09 /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed word integers in xmm1 depending on the corresponding sign in xmm2/m128."
"PSIGND mm1, mm2/m64","NP 0F 38 0A /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed doubleword integers in mm1 depending on the corresponding sign in mm2/m128."
"PSIGND xmm1, xmm2/m128","66 0F 38 0A /r","Valid","Valid","Invalid","SSSE3","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Negate/zero/preserve packed doubleword integers in xmm1 depending on the corresponding sign in xmm2/m128."
"VPSIGNB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 08 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate/zero/preserve packed byte integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGNW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 09 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate/zero/preserve packed word integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGND xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.WIG 0A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate/zero/preserve packed doubleword integers in xmm2 depending on the corresponding sign in xmm3/m128."
"VPSIGNB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 08 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate packed byte integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGNW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 09 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate packed 16-bit integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"VPSIGND ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.WIG 0A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Negate packed doubleword integers in ymm2 if the corresponding sign in ymm3/m256 is less than zero."
"PSLLDQ xmm1, imm8","66 0F 73 /7 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift xmm1 left by imm8 bytes while shifting in 0s."
"VPSLLDQ xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift xmm2 left by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSLLDQ ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift ymm2 left by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSLLDQ xmm1,xmm2/m128, imm8","EVEX.NDD.128.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift xmm2/m128 left by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSLLDQ ymm1, ymm2/m256, imm8","EVEX.NDD.256.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift ymm2/m256 left by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSLLDQ zmm1, zmm2/m512, imm8","EVEX.NDD.512.66.0F.WIG 73 /7 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift zmm2/m512 left by imm8 bytes while shifting in 0s and store result in zmm1."
"PSLLW mm, mm/m64","NP 0F F1 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift words in mm left mm/m64 while shifting in 0s."
"PSLLW xmm1, xmm2/m128","66 0F F1 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift words in xmm1 left by xmm2/m128 while shifting in 0s."
"PSLLW mm1, imm8","NP 0F 71 /6 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift words in mm left by imm8 while shifting in 0s."
"PSLLW xmm1, imm8","66 0F 71 /6 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift words in xmm1 left by imm8 while shifting in 0s."
"PSLLD mm, mm/m64","NP 0F F2 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift doublewords in mm left by mm/m64 while shifting in 0s."
"PSLLD xmm1, xmm2/m128","66 0F F2 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift doublewords in xmm1 left by xmm2/m128 while shifting in 0s."
"PSLLD mm, imm8","NP 0F 72 /6 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift doublewords in mm left by imm8 while shifting in 0s."
"PSLLD xmm1, imm8","66 0F 72 /6 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift doublewords in xmm1 left by imm8 while shifting in 0s."
"PSLLQ mm, mm/m64","NP 0F F3 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift quadword in mm left by mm/m64 while shifting in 0s."
"PSLLQ xmm1, xmm2/m128","66 0F F3 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift quadwords in xmm1 left by xmm2/m128 while shifting in 0s."
"PSLLQ mm, imm8","NP 0F 73 /6 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift quadword in mm left by imm8 while shifting in 0s."
"PSLLQ xmm1, imm8","66 0F 73 /6 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift quadwords in xmm1 left by imm8 while shifting in 0s."
"VPSLLW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLW xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift words in xmm2 left by imm8 while shifting in 0s."
"VPSLLD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F2 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLD xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 72 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift doublewords in xmm2 left by imm8 while shifting in 0s."
"VPSLLQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLQ xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 73 /6 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift quadwords in xmm2 left by imm8 while shifting in 0s."
"VPSLLW ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLW ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift words in ymm2 left by imm8 while shifting in 0s.PSLLW/PSLLD/PSLLQâ€”Shift Packed Data Left Logical"
"VPSLLD ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG F2 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLD ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 72 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift doublewords in ymm2 left by imm8 while shifting in 0s."
"VPSLLQ ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG F3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s."
"VPSLLQ ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 73 /6 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift quadwords in ymm2 left by imm8 while shifting in 0s."
"VPSLLW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLW ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLW zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.WIG F1 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLW xmm1 {k1}{z}, xmm2/m128, imm8","EVEX.NDD.128.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in xmm2/m128 left by imm8 while shifting in 0s using writemask k1."
"VPSLLW ymm1 {k1}{z}, ymm2/m256, imm8","EVEX.NDD.256.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in ymm2/m256 left by imm8 while shifting in 0s using writemask k1."
"VPSLLW zmm1 {k1}{z}, zmm2/m512, imm8","EVEX.NDD.512.66.0F.WIG 71 /6 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in zmm2/m512 left by imm8 while shifting in 0 using writemask k1."
"VPSLLD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLD ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLD zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.W0 F2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s under writemask k1."
"VPSLLD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.NDD.128.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in xmm2/m128/m32bcst left by imm8 while shifting in 0s using writemask k1."
"VPSLLD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.NDD.256.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in ymm2/m256/m32bcst left by imm8 while shifting in 0s using writemask k1."
"VPSLLD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.NDD.512.66.0F.W0 72 /6 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in zmm2/m512/m32bcst left by imm8 while shifting in 0s using writemask k1."
"VPSLLQ xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in xmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLQ ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in ymm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLQ zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.W1 F3 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in zmm2 left by amount specified in xmm3/m128 while shifting in 0s using writemask k1.PSLLW/PSLLD/PSLLQâ€”Shift Packed Data Left Logical"
"PSRAW mm,mm/m64",NP 0F E1 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Shift words in mm right by mm/m64 while shifting in sign bits.
"PSRAW xmm1,xmm2/m128",66 0F E1 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),NA,NA,,Shift words in xmm1 right by xmm2/m128 while shifting in sign bits.
"PSRAW mm,imm8",NP 0F 71 /4 ib,Valid,Valid,Invalid,MMX,"ModRM:r/m (r, w)",imm8,NA,NA,,Shift words in mm right by imm8 while shifting in sign bits
"PSRAW xmm1,imm8",66 0F 71 /4 ib,Valid,Valid,Invalid,SSE2,"ModRM:r/m (r, w)",imm8,NA,NA,,Shift words in xmm1 right by imm8 while shifting in sign bits
"PSRAD mm,mm/m64",NP 0F E2 /r,Valid,Valid,Invalid,MMX,"ModRM:reg (r, w)",ModRM:r/m (r),,NA,,Shift doublewords in mm right by mm/m64 while shifting in sign bits.
"PSRAD xmm1,xmm2/m128",66 0F E2 /r,Valid,Valid,Invalid,SSE2,"ModRM:reg (r, w)",ModRM:r/m (r),,NA,,Shift doubleword in xmm1 right by xmm2 /m128 while shifting in sign bits.
"PSRAD mm,imm8",NP 0F 72 /4 ib,Valid,Valid,Invalid,MMX,"ModRM:r/m (r, w)",imm8,NA,NA,,Shift doublewords in mm right by imm8 while shifting in sign bits.
"PSRAD xmm1,imm8",66 0F 72 /4 ib,Valid,Valid,Invalid,SSE2,"ModRM:r/m (r, w)",imm8,NA,NA,,Shift doublewords in xmm1 right by imm8 while shifting in sign bits.
"VPSRAW xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG E1 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
"VPSRAW xmm1,xmm2,imm8",VEX.NDD.128.66.0F.WIG 71 /4 ib,Valid,Valid,Invalid,AVX,VEX.vvvv (w),ModRM:r/m (r),imm8,NA,,Shift words in xmm2 right by imm8 while shifting in sign bits.
"VPSRAD xmm1,xmm2,xmm3/m128",VEX.NDS.128.66.0F.WIG E2 /r,Valid,Valid,Invalid,AVX,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits.
"VPSRAD xmm1,xmm2,imm8",VEX.NDD.128.66.0F.WIG 72 /4 ib,Valid,Valid,Invalid,AVX,VEX.vvvv (w),ModRM:r/m (r),imm8,NA,,Shift doublewords in xmm2 right by imm8 while shifting in sign bits.
"VPSRAW ymm1,ymm2,xmm3/m128",VEX.NDS.256.66.0F.WIG E1 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
"VPSRAW ymm1,ymm2,imm8",VEX.NDD.256.66.0F.WIG 71 /4 ib,Valid,Valid,Invalid,AVX2,VEX.vvvv (w),ModRM:r/m (r),imm8,NA,,Shift words in ymm2 right by imm8 while shifting in sign bits.
"VPSRAD ymm1,ymm2,xmm3/m128",VEX.NDS.256.66.0F.WIG E2 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),VEX.vvvv (r),ModRM:r/m (r),NA,,Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits.
"VPSRAD ymm1,ymm2,imm8",VEX.NDD.256.66.0F.WIG 72 /4 ib,Valid,Valid,Invalid,AVX2,VEX.vvvv (w),ModRM:r/m (r),imm8,NA,,Shift doublewords in ymm2 right by imm8 while shifting in sign bits.
"VPSRAW xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.WIG E1 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAW ymm1 {k1}{z},ymm2,xmm3/m128",EVEX.NDS.256.66.0F.WIG E1 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAW zmm1 {k1}{z},zmm2,xmm3/m128",EVEX.NDS.512.66.0F.WIG E1 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAW xmm1 {k1}{z},xmm2/m128,imm8",EVEX.NDD.128.66.0F.WIG 71 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512BW,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector Mem,Shift words in xmm2/m128 right by imm8 while shifting in sign bits using writemask k1.
"VPSRAW ymm1 {k1}{z},ymm2/m256,imm8",EVEX.NDD.256.66.0F.WIG 71 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512BW,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector Mem,Shift words in ymm2/m256 right by imm8 while shifting in sign bits using writemask k1.
"VPSRAW zmm1 {k1}{z},zmm2/m512,imm8",EVEX.NDD.512.66.0F.WIG 71 /4 ib,Valid,Valid,Invalid,AVX512BW,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector Mem,Shift words in zmm2/m512 right by imm8 while shifting in sign bits using writemask k1.
"VPSRAD xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.W0 E2 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAD ymm1 {k1}{z},ymm2,xmm3/m128",EVEX.NDS.256.66.0F.W0 E2 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAD zmm1 {k1}{z},zmm2,xmm3/m128",EVEX.NDS.512.66.0F.W0 E2 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAD xmm1 {k1}{z},xmm2/m128/m32bcst,imm8",EVEX.NDD.128.66.0F.W0 72 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift doublewords in xmm2/m128/m32bcst right by imm8 while shifting in sign bits using writemask k1.
"VPSRAD ymm1 {k1}{z},ymm2/m256/m32bcst,imm8",EVEX.NDD.256.66.0F.W0 72 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift doublewords in ymm2/m256/m32bcst right by imm8 while shifting in sign bits using writemask k1.
"VPSRAD zmm1 {k1}{z},zmm2/m512/m32bcst,imm8",EVEX.NDD.512.66.0F.W0 72 /4 ib,Valid,Valid,Invalid,AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift doublewords in zmm2/m512/m32bcst right by imm8 while shifting in sign bits using writemask k1.
"VPSRAQ xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.NDS.128.66.0F.W1 E2 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAQ ymm1 {k1}{z},ymm2,xmm3/m128",EVEX.NDS.256.66.0F.W1 E2 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAQ zmm1 {k1}{z},zmm2,xmm3/m128",EVEX.NDS.512.66.0F.W1 E2 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Mem128,Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in sign bits using writemask k1.
"VPSRAQ xmm1 {k1}{z},xmm2/m128/m64bcst,imm8",EVEX.NDD.128.66.0F.W1 72 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift quadwords in xmm2/m128/m64bcst right by imm8 while shifting in sign bits using writemask k1.
"VPSRAQ ymm1 {k1}{z},ymm2/m256/m64bcst,imm8",EVEX.NDD.256.66.0F.W1 72 /4 ib,Valid,Valid,Invalid,AVX512VL AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift quadwords in ymm2/m256/m64bcst right by imm8 while shifting in sign bits using writemask k1.
"VPSRAQ zmm1 {k1}{z},zmm2/m512/m64bcst,imm8",EVEX.NDD.512.66.0F.W1 72 /4 ib,Valid,Valid,Invalid,AVX512F,EVEX.vvvv (w),ModRM:r/m (r),imm8,NA,Full Vector,Shift quadwords in zmm2/m512/m64bcst right by imm8 while shifting in sign bits using writemask k1.
"PSRLDQ xmm1, imm8","66 0F 73 /3 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift xmm1 right by imm8 while shifting in 0s."
"VPSRLDQ xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift xmm2 right by imm8 bytes while shifting in 0s."
"VPSRLDQ ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift ymm1 right by imm8 bytes while shifting in 0s."
"VPSRLDQ xmm1, xmm2/m128, imm8","EVEX.NDD.128.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift xmm2/m128 right by imm8 bytes while shifting in 0s and store result in xmm1."
"VPSRLDQ ymm1, ymm2/m256, imm8","EVEX.NDD.256.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift ymm2/m256 right by imm8 bytes while shifting in 0s and store result in ymm1."
"VPSRLDQ zmm1, zmm2/m512, imm8","EVEX.NDD.512.66.0F.WIG 73 /3 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift zmm2/m512 right by imm8 bytes while shifting in 0s and store result in zmm1."
"PSRLW mm, mm/m64","NP 0F D1 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift words in mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLW xmm1, xmm2/m128","66 0F D1 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift words in xmm1 right by amount specified in xmm2/m128 while shifting in 0s."
"PSRLW mm, imm8","NP 0F 71 /2 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift words in mm right by imm8 while shifting in 0s."
"PSRLW xmm1, imm8","66 0F 71 /2 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift words in xmm1 right by imm8 while shifting in 0s."
"PSRLD mm, mm/m64","NP 0F D2 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift doublewords in mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLD xmm1, xmm2/m128","66 0F D2 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift doublewords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s."
"PSRLD mm, imm8","NP 0F 72 /2 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift doublewords in mm right by imm8 while shifting in 0s."
"PSRLD xmm1, imm8","66 0F 72 /2 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift doublewords in xmm1 right by imm8 while shifting in 0s."
"PSRLQ mm, mm/m64","NP 0F D3 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift mm right by amount specified in mm/m64 while shifting in 0s."
"PSRLQ xmm1, xmm2/m128","66 0F D3 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Shift quadwords in xmm1 right by amount specified in xmm2/m128 while shifting in 0s."
"PSRLQ mm, imm8","NP 0F 73 /2 ib","Valid","Valid","Invalid","MMX","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift mm right by imm8 while shifting in 0s."
"PSRLQ xmm1, imm8","66 0F 73 /2 ib","Valid","Valid","Invalid","SSE2","ModRM:r/m (r, w)","imm8","NA","NA","NA","Shift quadwords in xmm1 right by imm8 while shifting in 0s."
"VPSRLW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLW xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift words in xmm2 right by imm8 while shifting in 0s."
"VPSRLD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D2 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLD xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 72 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift doublewords in xmm2 right by imm8 while shifting in 0s."
"VPSRLQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D3 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLQ xmm1, xmm2, imm8","VEX.NDD.128.66.0F.WIG 73 /2 ib","Valid","Valid","Invalid","AVX","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift quadwords in xmm2 right by imm8 while shifting in 0s."
"VPSRLW ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLW ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift words in ymm2 right by imm8 while shifting in 0s.PSRLW/PSRLD/PSRLQâ€”Shift Packed Data Right Logical"
"VPSRLD ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG D2 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLD ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 72 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift doublewords in ymm2 right by imm8 while shifting in 0s."
"VPSRLQ ymm1, ymm2, xmm3/m128","VEX.NDS.256.66.0F.WIG D3 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s."
"VPSRLQ ymm1, ymm2, imm8","VEX.NDD.256.66.0F.WIG 73 /2 ib","Valid","Valid","Invalid","AVX2","VEX.vvvv (w)","ModRM:r/m (r)","imm8","NA","NA","Shift quadwords in ymm2 right by imm8 while shifting in 0s."
"VPSRLW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLW ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLW zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.WIG D1 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift words in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLW xmm1 {k1}{z}, xmm2/m128, imm8","EVEX.NDD.128.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in xmm2/m128 right by imm8 while shifting in 0s using writemask k1."
"VPSRLW ymm1 {k1}{z}, ymm2/m256, imm8","EVEX.NDD.256.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in ymm2/m256 right by imm8 while shifting in 0s using writemask k1."
"VPSRLW zmm1 {k1}{z}, zmm2/m512, imm8","EVEX.NDD.512.66.0F.WIG 71 /2 ib","Valid","Valid","Invalid","AVX512BW","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector Mem","Shift words in zmm2/m512 right by imm8 while shifting in 0s using writemask k1."
"VPSRLD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLD ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLD zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.W0 D2 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift doublewords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.NDD.128.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in xmm2/m128/m32bcst right by imm8 while shifting in 0s using writemask k1."
"VPSRLD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.NDD.256.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512VL AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in ymm2/m256/m32bcst right by imm8 while shifting in 0s using writemask k1."
"VPSRLD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.NDD.512.66.0F.W0 72 /2 ib","Valid","Valid","Invalid","AVX512F","EVEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Shift doublewords in zmm2/m512/m32bcst right by imm8 while shifting in 0s using writemask k1."
"VPSRLQ xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in xmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLQ ymm1 {k1}{z}, ymm2, xmm3/m128","EVEX.NDS.256.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in ymm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLQ zmm1 {k1}{z}, zmm2, xmm3/m128","EVEX.NDS.512.66.0F.W1 D3 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","Mem128","Shift quadwords in zmm2 right by amount specified in xmm3/m128 while shifting in 0s using writemask k1.PSRLW/PSRLD/PSRLQâ€”Shift Packed Data Right Logical"
"PSUBB mm, mm/m64","NP 0F F8 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed byte integers in mm/m64 from packed byte integers in mm."
"PSUBB xmm1, xmm2/m128","66 0F F8 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed byte integers in xmm2/m128 from packed byte integers in xmm1."
"PSUBW mm, mm/m64","NP 0F F9 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed word integers in mm/m64 from packed word integers in mm."
"PSUBW xmm1, xmm2/m128","66 0F F9 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed word integers in xmm2/m128 from packed word integers in xmm1."
"PSUBD mm, mm/m64","NP 0F FA /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed doubleword integers in mm/m64 from packed doubleword integers in mm."
"PSUBD xmm1, xmm2/m128","66 0F FA /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed doubleword integers in xmm2/mem128 from packed doubleword integers in xmm1."
"VPSUBB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed byte integers in xmm3/m128 from xmm2."
"VPSUBW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed word integers in xmm3/m128 from xmm2."
"VPSUBD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG FA /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed doubleword integers in xmm3/m128 from xmm2."
"VPSUBB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed byte integers in ymm3/m256 from ymm2."
"VPSUBW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed word integers in ymm3/m256 from ymm2."
"VPSUBD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG FA /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed doubleword integers in ymm3/m256 from ymm2."
"VPSUBB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed byte integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed byte integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG F8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed byte integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1."
"VPSUBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed word integers in xmm3/m128 from xmm2 and store in xmm1 using writemask k1."
"VPSUBW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed word integers in ymm3/m256 from ymm2 and store in ymm1 using writemask k1."
"VPSUBW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG F9 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed word integers in zmm3/m512 from zmm2 and store in zmm1 using writemask k1.PSUBB/PSUBW/PSUBDâ€”Subtract Packed Integers"
"PSUBQ mm1, mm2/m64","NP 0F FB /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract quadword integer in mm1 from mm2 /m64."
"PSUBQ xmm1, xmm2/m128","66 0F FB /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed quadword integers in xmm1 from xmm2/m128."
"VPSUBQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG FB/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed quadword integers in xmm3/m128 from xmm2."
"VPSUBQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG FB /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed quadword integers in ymm3/m256 from ymm2."
"VPSUBQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 FB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed quadword integers in xmm3/m128/m64bcst from xmm2 and store in xmm1 using writemask k1."
"VPSUBQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 FB /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed quadword integers in ymm3/m256/m64bcst from ymm2 and store in ymm1 using writemask k1."
"VPSUBQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 FB/r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed quadword integers in zmm3/m512/m64bcst from zmm2 and store in zmm1 using writemask k1."
"PSUBSB mm, mm/m64","NP 0F E8 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract signed packed bytes in mm/m64 from signed packed bytes in mm and saturate results."
"PSUBSB xmm1, xmm2/m128","66 0F E8 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed signed byte integers in xmm2/m128 from packed signed byte integers in xmm1 and saturate results."
"PSUBSW mm, mm/m64","NP 0F E9 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract signed packed words in mm/m64 from signed packed words in mm and saturate results."
"PSUBSW xmm1, xmm2/m128","66 0F E9 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed signed word integers in xmm2/m128 from packed signed word integers in xmm1 and saturate results."
"VPSUBSB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results."
"VPSUBSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results."
"VPSUBSB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results."
"VPSUBSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results."
"VPSUBSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed signed byte integers in xmm3/m128 from packed signed byte integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed signed byte integers in ymm3/m256 from packed signed byte integers in ymm2 and saturate results and store in ymm1 using writemask k1."
"VPSUBSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG E8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed signed byte integers in zmm3/m512 from packed signed byte integers in zmm2 and saturate results and store in zmm1 using writemask k1."
"VPSUBSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed signed word integers in xmm3/m128 from packed signed word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG E9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed signed word integers in ymm3/m256 from packed signed word integers in ymm2 and saturate results and store in ymm1 using writemask k1.PSUBSB/PSUBSWâ€”Subtract Packed Signed Integers with Signed Saturation"
"PSUBUSB mm, mm/m64","NP 0F D8 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract unsigned packed bytes in mm/m64 from unsigned packed bytes in mm and saturate result."
"PSUBUSB xmm1, xmm2/m128","66 0F D8 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed unsigned byte integers in xmm2/m128 from packed unsigned byte integers in xmm1 and saturate result."
"PSUBUSW mm, mm/m64","NP 0F D9 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract unsigned packed words in mm/m64 from unsigned packed words in mm and saturate result."
"PSUBUSW xmm1, xmm2/m128","66 0F D9 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed unsigned word integers in xmm2/m128 from packed unsigned word integers in xmm1 and saturate result."
"VPSUBUSB xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2 and saturate result."
"VPSUBUSW xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate result."
"VPSUBUSB ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2 and saturate result."
"VPSUBUSW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2 and saturate result."
"VPSUBUSB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed unsigned byte integers in xmm3/m128 from packed unsigned byte integers in xmm2, saturate results and store in xmm1 using writemask k1."
"VPSUBUSB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed unsigned byte integers in ymm3/m256 from packed unsigned byte integers in ymm2, saturate results and store in ymm1 using writemask k1."
"VPSUBUSB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F.WIG D8 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed unsigned byte integers in zmm3/m512 from packed unsigned byte integers in zmm2, saturate results and store in zmm1 using writemask k1."
"VPSUBUSW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed unsigned word integers in xmm3/m128 from packed unsigned word integers in xmm2 and saturate results and store in xmm1 using writemask k1."
"VPSUBUSW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F.WIG D9 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Subtract packed unsigned word integers in ymm3/m256 from packed unsigned word integers in ymm2, saturate results and store in ymm1 using writemask k1.PSUBUSB/PSUBUSWâ€”Subtract Packed Unsigned Integers with Unsigned Saturation"
"PTEST xmm1, xmm2/m128","66 0F 38 17 /r","Valid","Valid","Invalid","SSE4_1","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF if xmm2/m128 AND xmm1 result is all 0s. Set CF if xmm2/m128 AND NOT xmm1 result is all 0s."
"VPTEST xmm1, xmm2/m128","VEX.128.66.0F38.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"VPTEST ymm1, ymm2/m256","VEX.256.66.0F38.WIG 17 /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on bitwise AND and ANDN of sources."
"PTWRITE r64/m64","F3 REX.W 0F AE /4","Valid","Invalid","Invalid","","ModRM:rm (r)","NA","NA","NA","","Reads the data from r64/m64 to encod into a PTW packet if dependencies are met (see details below)."
"PTWRITE r32/m32","F3 0F AE /4","Valid","Valid","Invalid","","ModRM:rm (r)","NA","NA","NA","","Reads the data from r32/m32 to encode into a PTW packet if dependencies are met (see details below)."
"PUNPCKHBW mm, mm/m64","NP 0F 68 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order bytes from mm and mm/m64 into mm."
"PUNPCKHBW xmm1, xmm2/m128","66 0F 68 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order bytes from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHWD mm, mm/m64","NP 0F 69 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order words from mm and mm/m64 into mm."
"PUNPCKHWD xmm1, xmm2/m128","66 0F 69 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order words from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHDQ mm, mm/m64","NP 0F 6A /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order doublewords from mm and mm/m64 into mm."
"PUNPCKHDQ xmm1, xmm2/m128","66 0F 6A /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order doublewords from xmm1 and xmm2/m128 into xmm1."
"PUNPCKHQDQ xmm1, xmm2/m128","66 0F 6D /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpack and interleave high-order quadwords from xmm1 and xmm2/m128 into xmm1."
"VPUNPCKHBW xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 68/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHWD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 69/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order words from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 6A/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKHQDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 6D/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKHBW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 68 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHWD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 69 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 6A /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHQDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 6D /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave high-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKHBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 68 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Interleave high-order bytes from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKHWD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 69 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Interleave high-order words from xmm2 and xmm3/m128 into xmm1 register using k1 write mask."
"VPUNPCKHDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 6A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Interleave high-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register using k1 write mask."
"VPUNPCKHQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 6D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Interleave high-order quadword from xmm2 and xmm3/m128/m64bcst into xmm1 register using k1 write mask.PUNPCKHBW/PUNPCKHWD/PUNPCKHDQ/PUNPCKHQDQâ€” Unpack High Data"
"PUNPCKLBW mm, mm/m32","NP 0F 60 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order bytes from mm and mm/m32 into mm."
"PUNPCKLBW xmm1, xmm2/m128","66 0F 60 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order bytes from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLWD mm, mm/m32","NP 0F 61 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order words from mm and mm/m32 into mm."
"PUNPCKLWD xmm1, xmm2/m128","66 0F 61 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order words from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLDQ mm, mm/m32","NP 0F 62 /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order doublewords from mm and mm/m32 into mm."
"PUNPCKLDQ xmm1, xmm2/m128","66 0F 62 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order doublewords from xmm1 and xmm2/m128 into xmm1."
"PUNPCKLQDQ xmm1, xmm2/m128","66 0F 6C /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Interleave low-order quadword from xmm1 and xmm2/m128 into xmm1 register."
"VPUNPCKLBW xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 60/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLWD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 61/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order words from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 62/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order doublewords from xmm2 and xmm3/m128 into xmm1."
"VPUNPCKLQDQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 6C/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order quadword from xmm2 and xmm3/m128 into xmm1 register."
"VPUNPCKLBW ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 60 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order bytes from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLWD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 61 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order words from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 62 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order doublewords from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLQDQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 6C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Interleave low-order quadword from ymm2 and ymm3/m256 into ymm1 register."
"VPUNPCKLBW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 60 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Interleave low-order bytes from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
"VPUNPCKLWD xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F.WIG 61 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Interleave low-order words from xmm2 and xmm3/m128 into xmm1 register subject to write mask k1."
"VPUNPCKLDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 62 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Interleave low-order doublewords from xmm2 and xmm3/m128/m32bcst into xmm1 register subject to write mask k1."
"VPUNPCKLQDQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 6C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Interleave low-order quadword from zmm2 and zmm3/m512/m64bcst into zmm1 register subject to write mask k1.PUNPCKLBW/PUNPCKLWD/PUNPCKLDQ/PUNPCKLQDQâ€”Unpack Low Data"
"PUSH r/m16","FF /6","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Push r/m16."
"PUSH r/m32","FF /6","Invalid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Push r/m32."
"PUSH r/m64","FF /6","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Push r/m64."
"PUSH r16","50+rw","Valid","Valid","Valid","","opcode +rd (r)","NA","NA","NA","","Push r16."
"PUSH r32","50+rd","Invalid","Valid","Valid","","opcode +rd (r)","NA","NA","NA","","Push r32."
"PUSH r64","50+rd","Valid","Invalid","Invalid","","opcode +rd (r)","NA","NA","NA","","Push r64."
"PUSH imm8","6A ib","Valid","Valid","Valid","","","","","","","Push imm8."
"PUSH imm16","68 iw","Valid","Valid","Valid","","","","","","","Push imm16."
"PUSH imm32","68 id","Valid","Valid","Valid","","","","","","","Push imm32."
"PUSH CS","0E","Invalid","Valid","Valid","","","","","","","Push CS."
"PUSH SS","16","Invalid","Valid","Valid","","","","","","","Push SS."
"PUSH DS","1E","Invalid","Valid","Valid","","","","","","","Push DS."
"PUSH ES","06","Invalid","Valid","Valid","","","","","","","Push ES."
"PUSH FS","0F A0","Valid","Valid","Valid","","","","","","","Push FS."
"PUSH GS","0F A8","Valid","Valid","Valid","","","","","","","Push GS."
"PUSHA","60","Invalid","Valid","Valid","","NA","NA","NA","NA","","Push AX, CX, DX, BX, original SP, BP, SI, and DI."
"PUSHAD","60","Invalid","Valid","Valid","","NA","NA","NA","NA","","Push EAX, ECX, EDX, EBX, original ESP, EBP, ESI, and EDI."
"PUSHF","9C","Valid","Valid","Valid","","NA","NA","NA","NA","","Push lower 16 bits of EFLAGS."
"PUSHFD","9C","Invalid","Valid","Valid","","NA","NA","NA","NA","","Push EFLAGS."
"PUSHFQ","9C","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Push RFLAGS."
"PXOR mm, mm/m64","NP 0F EF /r","Valid","Valid","Invalid","MMX","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise XOR of mm/m64 and mm."
"PXOR xmm1, xmm2/m128","66 0F EF /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Bitwise XOR of xmm2/m128 and xmm1."
"VPXOR xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG EF /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise XOR of xmm3/m128 and xmm2."
"VPXOR ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG EF /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Bitwise XOR of ymm3/m256 and ymm2."
"VPXORD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed doubleword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed doubleword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F.W0 EF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed doubleword integers in zmm2 and zmm3/m512/m32bcst using writemask k1."
"VPXORQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed quadword integers in xmm2 and xmm3/m128 using writemask k1."
"VPXORQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed quadword integers in ymm2 and ymm3/m256 using writemask k1."
"VPXORQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 EF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise XOR of packed quadword integers in zmm2 and zmm3/m512/m64bcst using writemask k1."
"RCL r/m8, 1","D0 /2","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 9 bits (CF, r/m8) left once."
"RCL r/m8, 1","REX + D0 /2","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 9 bits (CF, r/m8) left once."
"RCL r/m8, CL","D2 /2","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 9 bits (CF, r/m8) left CL times."
"RCL r/m8, CL","REX + D2 /2","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 9 bits (CF, r/m8) left CL times."
"RCL r/m8, imm8","C0 /2 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 9 bits (CF, r/m8) left imm8 times."
"RCL r/m8, imm8","REX + C0 /2 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 9 bits (CF, r/m8) left imm8 times."
"RCL r/m16, 1","D1 /2","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 17 bits (CF, r/m16) left once."
"RCL r/m16, CL","D3 /2","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 17 bits (CF, r/m16) left CL times."
"RCL r/m16, imm8","C1 /2 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 17 bits (CF, r/m16) left imm8 times."
"RCL r/m32, 1","D1 /2","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 33 bits (CF, r/m32) left once."
"RCL r/m64, 1","REX.W + D1 /2","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 65 bits (CF, r/m64) left once. Uses a 6 bit count."
"RCL r/m32, CL","D3 /2","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 33 bits (CF, r/m32) left CL times."
"RCL r/m64, CL","REX.W + D3 /2","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 65 bits (CF, r/m64) left CL times. Uses a 6 bit count."
"RCL r/m32, imm8","C1 /2 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 33 bits (CF, r/m32) left imm8 times."
"RCL r/m64, imm8","REX.W + C1 /2 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 65 bits (CF, r/m64) left imm8 times. Uses a 6 bit count."
"RCR r/m8, 1","D0 /3","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 9 bits (CF, r/m8) right once."
"RCR r/m8, 1","REX + D0 /3","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 9 bits (CF, r/m8) right once."
"RCR r/m8, CL","D2 /3","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 9 bits (CF, r/m8) right CL times."
"RCR r/m8, CL","REX + D2 /3","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 9 bits (CF, r/m8) right CL times."
"RCR r/m8, imm8","C0 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 9 bits (CF, r/m8) right imm8 times."
"RCR r/m8, imm8","REX + C0 /3 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 9 bits (CF, r/m8) right imm8 times."
"RCR r/m16, 1","D1 /3","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 17 bits (CF, r/m16) right once."
"RCR r/m16, CL","D3 /3","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 17 bits (CF, r/m16) right CL times."
"RCR r/m16, imm8","C1 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 17 bits (CF, r/m16) right imm8 times."
"RCR r/m32, 1","D1 /3","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 33 bits (CF, r/m32) right once. Uses a 6 bit count."
"RCR r/m64, 1","REX.W + D1 /3","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 65 bits (CF, r/m64) right once. Uses a 6 bit count."
"RCR r/m32, CL","D3 /3","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 33 bits (CF, r/m32) right CL times."
"RCR r/m64, CL","REX.W + D3 /3","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 65 bits (CF, r/m64) right CL times. Uses a 6 bit count."
"RCR r/m32, imm8","C1 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 33 bits (CF, r/m32) right imm8 times."
"RCR r/m64, imm8","REX.W + C1 /3 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 65 bits (CF, r/m64) right imm8 times. Uses a 6 bit count."
"ROL r/m8, 1","D0 /0","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 8 bits r/m8 left once."
"ROL r/m8, 1","REX + D0 /0","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 8 bits r/m8 left once"
"ROL r/m8, CL","D2 /0","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 8 bits r/m8 left CL times."
"ROL r/m8, CL","REX + D2 /0","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 8 bits r/m8 left CL times."
"ROL r/m8, imm8","C0 /0 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 8 bits r/m8 left imm8 times.RCL/RCR/ROL/RORâ€”Rotate"
"ROL r/m8, imm8","REX + C0 /0 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 8 bits r/m8 left imm8 times."
"ROL r/m16, 1","D1 /0","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 16 bits r/m16 left once."
"ROL r/m16, CL","D3 /0","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 16 bits r/m16 left CL times."
"ROL r/m16, imm8","C1 /0 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 16 bits r/m16 left imm8 times."
"ROL r/m32, 1","D1 /0","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 32 bits r/m32 left once."
"ROL r/m64, 1","REX.W + D1 /0","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 64 bits r/m64 left once. Uses a 6 bit count."
"ROL r/m32, CL","D3 /0","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 32 bits r/m32 left CL times."
"ROL r/m64, CL","REX.W + D3 /0","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 64 bits r/m64 left CL times. Uses a 6 bit count."
"ROL r/m32, imm8","C1 /0 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 32 bits r/m32 left imm8 times."
"ROL r/m64, imm8","REX.W + C1 /0 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 64 bits r/m64 left imm8 times. Uses a 6 bit count."
"ROR r/m8, 1","D0 /1","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 8 bits r/m8 right once."
"ROR r/m8, 1","REX + D0 /1","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 8 bits r/m8 right once."
"ROR r/m8, CL","D2 /1","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 8 bits r/m8 right CL times."
"ROR r/m8, CL","REX + D2 /1","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 8 bits r/m8 right CL times."
"ROR r/m8, imm8","C0 /1 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 8 bits r/m16 right imm8 times."
"ROR r/m8, imm8","REX + C0 /1 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 8 bits r/m16 right imm8 times."
"ROR r/m16, 1","D1 /1","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 16 bits r/m16 right once."
"ROR r/m16, CL","D3 /1","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 16 bits r/m16 right CL times."
"ROR r/m16, imm8","C1 /1 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 16 bits r/m16 right imm8 times."
"ROR r/m32, 1","D1 /1","Valid","Valid","Valid","","ModRM:r/m (w)","1","NA","NA","","Rotate 32 bits r/m32 right once."
"ROR r/m64, 1","REX.W + D1 /1","Valid","Invalid","Invalid","","ModRM:r/m (w)","1","NA","NA","","Rotate 64 bits r/m64 right once. Uses a 6 bit count."
"ROR r/m32, CL","D3 /1","Valid","Valid","Valid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 32 bits r/m32 right CL times."
"ROR r/m64, CL","REX.W + D3 /1","Valid","Invalid","Invalid","","ModRM:r/m (w)","CL","NA","NA","","Rotate 64 bits r/m64 right CL times. Uses a 6 bit count."
"ROR r/m32, imm8","C1 /1 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 32 bits r/m32 right imm8 times."
"ROR r/m64, imm8","REX.W + C1 /1 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8","NA","NA","","Rotate 64 bits r/m64 right imm8 times. Uses a 6 bit count."
"RCPPS xmm1, xmm2/m128","NP 0F 53 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1."
"VRCPPS xmm1, xmm2/m128","VEX.128.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of packed single-precision values in xmm2/mem and stores the results in xmm1."
"VRCPPS ymm1, ymm2/m256","VEX.256.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of packed single-precision values in ymm2/mem and stores the results in ymm1."
"RCPSS xmm1, xmm2/m32","F3 0F 53 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm2/m32 and stores the result in xmm1."
"VRCPSS xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 53 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the result in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"RDFSBASE r32","F3 0F AE /0","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (w)","NA","NA","NA","","Load the 32-bit destination register with the FS base address."
"RDFSBASE r64","F3 REX.W 0F AE /0","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (w)","NA","NA","NA","","Load the 64-bit destination register with the FS base address."
"RDGSBASE r32","F3 0F AE /1","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (w)","NA","NA","NA","","Load the 32-bit destination register with the GS base address."
"RDGSBASE r64","F3 REX.W 0F AE /1","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (w)","NA","NA","NA","","Load the 64-bit destination register with the GS base address."
"RDMSR","0F 32","Valid","Valid","Valid","","NA","NA","NA","NA","","Read MSR specified by ECX into EDX:EAX."
"RDPID r32","F3 0F C7 /7","Invalid","Valid","Invalid","RDPID","ModRM:r/m (w)","NA","NA","NA","","Read IA32_TSC_AUX into r32."
"RDPID r64","F3 0F C7 /7","Valid","Invalid","Invalid","RDPID","ModRM:r/m (w)","NA","NA","NA","","Read IA32_TSC_AUX into r64."
"RDPKRU","NP 0F 01 EE","Valid","Valid","Valid","OSPKE","NA","NA","NA","NA","","Reads PKRU into EAX."
"RDPMC","0F 33","Valid","Valid","Valid","","NA","NA","NA","NA","","Read performance-monitoring counter specified by ECX into EDX:EAX."
"RDRAND r16","0F C7 /6","Valid","Valid","Invalid","RDRAND","ModRM:r/m (w)","NA","NA","NA","","Read a 16-bit random number and store in the destination register."
"RDRAND r32","0F C7 /6","Valid","Valid","Invalid","RDRAND","ModRM:r/m (w)","NA","NA","NA","","Read a 32-bit random number and store in the destination register."
"RDRAND r64","REX.W + 0F C7 /6","Valid","Invalid","Invalid","RDRAND","ModRM:r/m (w)","NA","NA","NA","","Read a 64-bit random number and store in the destination register."
"RDSEED r16","0F C7 /7","Valid","Valid","Invalid","RDSEED","","","","","","Read a 16-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDSEED r32","0F C7 /7","Valid","Valid","Invalid","RDSEED","","","","","","Read a 32-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDSEED r64","REX.W + 0F C7 /7","Valid","Invalid","Invalid","RDSEED","","","","","","Read a 64-bit NIST SP800-90B & C compliant random value and store in the destination register."
"RDTSC","0F 31","Valid","Valid","Valid","","NA","NA","NA","NA","","Read time-stamp counter into EDX:EAX."
"RDTSCP","0F 01 F9","Valid","Valid","Valid","","NA","NA","NA","NA","","Read 64-bit time-stamp counter and IA32_TSC_AUX value into EDX:EAX and ECX."
"RET","C3","Valid","Valid","Valid","","NA","NA","NA","NA","","Near return to calling procedure."
"RET","CB","Valid","Valid","Valid","","NA","NA","NA","NA","","Far return to calling procedure."
"RET imm16","C2 iw","Valid","Valid","Valid","","imm16","NA","NA","NA","","Near return to calling procedure and pop imm16 bytes from stack."
"RET imm16","CA iw","Valid","Valid","Valid","","imm16","NA","NA","NA","","Far return to calling procedure and pop imm16 bytes from stack."
"RORX r32, r/m32, imm8","VEX.LZ.F2.0F3A.W0 F0 /r ib","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","","Rotate 32-bit r/m32 right imm8 times without affecting arithmetic flags."
"RORX r64, r/m64, imm8","VEX.LZ.F2.0F3A.W1 F0 /r ib","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","","Rotate 64-bit r/m64 right imm8 times without affecting arithmetic flags."
"ROUNDPD xmm1, xmm2/m128, imm8","66 0F 3A 09 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed double precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPD xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.WIG 09 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed double-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPD ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.WIG 09 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed double-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8."
"ROUNDPS xmm1, xmm2/m128, imm8","66 0F 3A 08 /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed single precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPS xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.WIG 08 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed single-precision floating-point values in xmm2/m128 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDPS ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.WIG 08 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round packed single-precision floating-point values in ymm2/m256 and place the result in ymm1. The rounding mode is determined by imm8."
"ROUNDSD xmm1, xmm2/m64, imm8","66 0F 3A 0B /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round the low packed double precision floating-point value in xmm2/m64 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDSD xmm1, xmm2, xmm3/m64, imm8","VEX.NDS.LIG.66.0F3A.WIG 0B /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Round the low packed double precision floating-point value in xmm3/m64 and place the result in xmm1. The rounding mode is determined by imm8. Upper packed double precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"ROUNDSS xmm1, xmm2/m32, imm8","66 0F 3A 0A /r ib","Valid","Valid","Invalid","SSE4_1","ModRM:reg (w)","ModRM:r/m (r)","imm8","NA","","Round the low packed single precision floating-point value in xmm2/m32 and place the result in xmm1. The rounding mode is determined by imm8."
"VROUNDSS xmm1, xmm2, xmm3/m32, imm8","VEX.NDS.LIG.66.0F3A.WIG 0A /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Round the low packed single precision floating-point value in xmm3/m32 and place the result in xmm1. The rounding mode is determined by imm8. Also, upper packed single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"RSM","0F AA","Valid","Valid","Valid","","NA","NA","NA","NA","","Resume operation of interrupted program."
"RSQRTPS xmm1, xmm2/m128","NP 0F 52 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of the square roots of the packed single-precision floating-point values in xmm2/m128 and stores the results in xmm1."
"VRSQRTPS xmm1, xmm2/m128","VEX.128.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of the square roots of packed single-precision values in xmm2/mem and stores the results in xmm1."
"VRSQRTPS ymm1, ymm2/m256","VEX.256.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocals of the square roots of packed single-precision values in ymm2/mem and stores the results in ymm1."
"RSQRTSS xmm1, xmm2/m32","F3 0F 52 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Computes the approximate reciprocal of the square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1."
"VRSQRTSS xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 52 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Computes the approximate reciprocal of the square root of the low single precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"SAHF","9E","Invalid","Valid","Valid","","NA","NA","NA","NA","","Loads SF, ZF, AF, PF, and CF from AH into EFLAGS register."
"SAL r/m8, 1","D0 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m8 by 2, once."
"SAL r/m8, 1","REX + D0 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m8 by 2, once."
"SAL r/m8, CL","D2 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m8 by 2, CL times."
"SAL r/m8, CL","REX + D2 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m8 by 2, CL times."
"SAL r/m8, imm8","C0 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m8 by 2, imm8 times."
"SAL r/m8, imm8","REX + C0 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m8 by 2, imm8 times."
"SAL r/m16, 1","D1 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m16 by 2, once."
"SAL r/m16, CL","D3 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m16 by 2, CL times."
"SAL r/m16, imm8","C1 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m16 by 2, imm8 times."
"SAL r/m32, 1","D1 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m32 by 2, once."
"SAL r/m64, 1","REX.W + D1 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m64 by 2, once."
"SAL r/m32, CL","D3 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m32 by 2, CL times."
"SAL r/m64, CL","REX.W + D3 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m64 by 2, CL times."
"SAL r/m32, imm8","C1 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m32 by 2, imm8 times."
"SAL r/m64, imm8","REX.W + C1 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m64 by 2, imm8 times."
"SAR r/m8, 1","D0 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Signed divide r/m8 by 2, once."
"SAR r/m8, 1","REX + D0 /7","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Signed divide r/m8 by 2, once."
"SAR r/m8, CL","D2 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Signed divide r/m8 by 2, CL times."
"SAR r/m8, CL","REX + D2 /7","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Signed divide r/m8 by 2, CL times."
"SAR r/m8, imm8","C0 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Signed divide r/m8 by 2, imm8 time."
"SAR r/m8, imm8","REX + C0 /7 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Signed divide r/m8 by 2, imm8 times."
"SAR r/m16,1","D1 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Signed divide r/m16 by 2, once."
"SAR r/m16, CL","D3 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Signed divide r/m16 by 2, CL times."
"SAR r/m16, imm8","C1 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Signed divide r/m16 by 2, imm8 times."
"SAR r/m32, 1","D1 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Signed divide r/m32 by 2, once."
"SAR r/m64, 1","REX.W + D1 /7","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Signed divide r/m64 by 2, once."
"SAR r/m32, CL","D3 /7","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Signed divide r/m32 by 2, CL times."
"SAR r/m64, CL","REX.W + D3 /7","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Signed divide r/m64 by 2, CL times."
"SAR r/m32, imm8","C1 /7 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Signed divide r/m32 by 2, imm8 times."
"SAR r/m64, imm8","REX.W + C1 /7 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Signed divide r/m64 by 2, imm8 times"
"SHL r/m8, 1","D0 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m8 by 2, once."
"SHL r/m8, 1","REX + D0 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m8 by 2, once."
"SHL r/m8, CL","D2 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m8 by 2, CL times."
"SHL r/m8, CL","REX + D2 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m8 by 2, CL times."
"SHL r/m8, imm8","C0 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m8 by 2, imm8 times."
"SHL r/m8, imm8","REX + C0 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m8 by 2, imm8 times."
"SHL r/m16,1","D1 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m16 by 2, once."
"SHL r/m16, CL","D3 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m16 by 2, CL times."
"SHL r/m16, imm8","C1 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m16 by 2, imm8 times."
"SHL r/m32,1","D1 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m32 by 2, once.SAL/SAR/SHL/SHRâ€”Shift"
"SHL r/m64,1","REX.W + D1 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Multiply r/m64 by 2, once."
"SHL r/m32, CL","D3 /4","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m32 by 2, CL times."
"SHL r/m64, CL","REX.W + D3 /4","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Multiply r/m64 by 2, CL times."
"SHL r/m32, imm8","C1 /4 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m32 by 2, imm8 times."
"SHL r/m64, imm8","REX.W + C1 /4 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Multiply r/m64 by 2, imm8 times."
"SHR r/m8,1","D0 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Unsigned divide r/m8 by 2, once."
"SHR r/m8, 1","REX + D0 /5","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Unsigned divide r/m8 by 2, once."
"SHR r/m8, CL","D2 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Unsigned divide r/m8 by 2, CL times."
"SHR r/m8, CL","REX + D2 /5","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Unsigned divide r/m8 by 2, CL times."
"SHR r/m8, imm8","C0 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Unsigned divide r/m8 by 2, imm8 times."
"SHR r/m8, imm8","REX + C0 /5 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Unsigned divide r/m8 by 2, imm8 times."
"SHR r/m16, 1","D1 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Unsigned divide r/m16 by 2, once."
"SHR r/m16, CL","D3 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Unsigned divide r/m16 by 2, CL times"
"SHR r/m16, imm8","C1 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Unsigned divide r/m16 by 2, imm8 times."
"SHR r/m32, 1","D1 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","1","NA","NA","","Unsigned divide r/m32 by 2, once."
"SHR r/m64, 1","REX.W + D1 /5","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","1","NA","NA","","Unsigned divide r/m64 by 2, once."
"SHR r/m32, CL","D3 /5","Valid","Valid","Valid","","ModRM:r/m (r, w)","CL","NA","NA","","Unsigned divide r/m32 by 2, CL times."
"SHR r/m64, CL","REX.W + D3 /5","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","CL","NA","NA","","Unsigned divide r/m64 by 2, CL times."
"SHR r/m32, imm8","C1 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8","NA","NA","","Unsigned divide r/m32 by 2, imm8 times."
"SHR r/m64, imm8","REX.W + C1 /5 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8","NA","NA","","Unsigned divide r/m64 by 2, imm8 times."
"SARX r32a, r/m32, r32b","VEX.NDS.LZ.F3.0F38.W0 F7 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m32 arithmetically right with count specified in r32b."
"SHLX r32a, r/m32, r32b","VEX.NDS.LZ.66.0F38.W0 F7 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m32 logically left with count specified in r32b."
"SHRX r32a, r/m32, r32b","VEX.NDS.LZ.F2.0F38.W0 F7 /r","Valid","Valid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m32 logically right with count specified in r32b."
"SARX r64a, r/m64, r64b","VEX.NDS.LZ.F3.0F38.W1 F7 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m64 arithmetically right with count specified in r64b."
"SHLX r64a, r/m64, r64b","VEX.NDS.LZ.66.0F38.W1 F7 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m64 logically left with count specified in r64b."
"SHRX r64a, r/m64, r64b","VEX.NDS.LZ.F2.0F38.W1 F7 /r","Valid","Invalid","Invalid","BMI2","ModRM:reg (w)","ModRM:r/m (r)","VEX.vvvv (r)","NA","","Shift r/m64 logically right with count specified in r64b."
"SBB AL, imm8","1C ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","Subtract with borrow imm8 from AL."
"SBB AX, imm16","1D iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","Subtract with borrow imm16 from AX."
"SBB EAX, imm32","1D id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","Subtract with borrow imm32 from EAX."
"SBB RAX, imm32","REX.W + 1D id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","Subtract with borrow sign-extended imm.32 to 64-bits from RAX."
"SBB r/m8, imm8","80 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow imm8 from r/m8."
"SBB r/m8, imm8","REX + 80 /3 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow imm8 from r/m8."
"SBB r/m16, imm16","81 /3 iw","Valid","Valid","Valid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow imm16 from r/m16."
"SBB r/m32, imm32","81 /3 id","Valid","Valid","Valid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow imm32 from r/m32."
"SBB r/m64, imm32","REX.W + 81 /3 id","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow sign-extended imm32 to 64-bits from r/m64."
"SBB r/m16, imm8","83 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow sign-extended imm8 from r/m16."
"SBB r/m32, imm8","83 /3 ib","Valid","Valid","Valid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow sign-extended imm8 from r/m32."
"SBB r/m64, imm8","REX.W + 83 /3 ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","imm8/16/32","NA","NA","","Subtract with borrow sign-extended imm8 from r/m64."
"SBB r/m8, r8","18 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Subtract with borrow r8 from r/m8."
"SBB r/m8, r8","REX + 18 /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Subtract with borrow r8 from r/m8."
"SBB r/m16, r16","19 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Subtract with borrow r16 from r/m16."
"SBB r/m32, r32","19 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Subtract with borrow r32 from r/m32."
"SBB r/m64, r64","REX.W + 19 /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","","Subtract with borrow r64 from r/m64."
"SBB r8, r/m8","1A /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Subtract with borrow r/m8 from r8."
"SBB r8, r/m8","REX + 1A /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Subtract with borrow r/m8 from r8."
"SBB r16, r/m16","1B /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Subtract with borrow r/m16 from r16."
"SBB r32, r/m32","1B /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Subtract with borrow r/m32 from r32."
"SBB r64, r/m64","REX.W + 1B /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Subtract with borrow r/m64 from r64."
"SCAS m8","AE","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare AL with byte at ES:(E)DI or RDI, then set status flags."
"SCAS m16","AF","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare AX with word at ES:(E)DI or RDI, then set status flags."
"SCAS m32","AF","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare EAX with doubleword at ES(E)DI or RDI then set status flags."
"SCAS m64","REX.W + AF","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Compare RAX with quadword at RDI or EDI then set status flags."
"SCASB","AE","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare AL with byte at ES:(E)DI or RDI then set status flags."
"SCASW","AF","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare AX with word at ES:(E)DI or RDI then set status flags."
"SCASD","AF","Valid","Valid","Valid","","NA","NA","NA","NA","","Compare EAX with doubleword at ES:(E)DI or RDI then set status flags."
"SCASQ","REX.W + AF","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Compare RAX with quadword at RDI or EDI then set status flags."
"SETA r/m8","0F 97","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if above (CF=0 and ZF=0)."
"SETA r/m8","REX + 0F 97","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if above (CF=0 and ZF=0)."
"SETAE r/m8","0F 93","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if above or equal (CF=0)."
"SETAE r/m8","REX + 0F 93","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if above or equal (CF=0)."
"SETB r/m8","0F 92","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if below (CF=1)."
"SETB r/m8","REX + 0F 92","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if below (CF=1)."
"SETBE r/m8","0F 96","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if below or equal (CF=1 or ZF=1)."
"SETBE r/m8","REX + 0F 96","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if below or equal (CF=1 or ZF=1)."
"SETC r/m8","0F 92","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if carry (CF=1)."
"SETC r/m8","REX + 0F 92","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if carry (CF=1)."
"SETE r/m8","0F 94","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if equal (ZF=1)."
"SETE r/m8","REX + 0F 94","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if equal (ZF=1)."
"SETG r/m8","0F 9F","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if greater (ZF=0 and SF=OF)."
"SETG r/m8","REX + 0F 9F","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if greater (ZF=0 and SF=OF)."
"SETGE r/m8","0F 9D","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if greater or equal (SF=OF)."
"SETGE r/m8","REX + 0F 9D","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if greater or equal (SF=OF)."
"SETL r/m8","0F 9C","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if less (SFâ‰  OF)."
"SETL r/m8","REX + 0F 9C","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if less (SFâ‰  OF)."
"SETLE r/m8","0F 9E","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if less or equal (ZF=1 or SFâ‰  OF)."
"SETLE r/m8","REX + 0F 9E","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if less or equal (ZF=1 or SFâ‰  OF)."
"SETNA r/m8","0F 96","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not above (CF=1 or ZF=1)."
"SETNA r/m8","REX + 0F 96","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not above (CF=1 or ZF=1)."
"SETNAE r/m8","0F 92","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not above or equal (CF=1)."
"SETNAE r/m8","REX + 0F 92","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not above or equal (CF=1)."
"SETNB r/m8","0F 93","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not below (CF=0)."
"SETNB r/m8","REX + 0F 93","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not below (CF=0)."
"SETNBE r/m8","0F 97","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not below or equal (CF=0 and ZF=0)."
"SETNBE r/m8","REX + 0F 97","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not below or equal (CF=0 and ZF=0)."
"SETNC r/m8","0F 93","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not carry (CF=0)."
"SETNC r/m8","REX + 0F 93","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not carry (CF=0)."
"SETNE r/m8","0F 95","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not equal (ZF=0)."
"SETNE r/m8","REX + 0F 95","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not equal (ZF=0)."
"SETNG r/m8","0F 9E","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not greater (ZF=1 or SFâ‰  OF)"
"SETNG r/m8","REX + 0F 9E","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not greater (ZF=1 or SFâ‰  OF)."
"SETNGE r/m8","0F 9C","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not greater or equal (SFâ‰  OF)."
"SETNGE r/m8","REX + 0F 9C","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not greater or equal (SFâ‰  OF)."
"SETNL r/m8","0F 9D","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not less (SF=OF)."
"SETNL r/m8","REX + 0F 9D","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not less (SF=OF)."
"SETNLE r/m8","0F 9F","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Set byte if not less or equal (ZF=0 and SF=OF).SETccâ€”Set Byte on Condition"
"SFENCE","NP 0F AE F8","Valid","Valid","Valid","","NA","NA","NA","NA","","Serializes store operations."
"SGDT m","0F 01 /0","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Store GDTR to m."
"SHA1MSG1 xmm1, xmm2/m128","NP 0F 38 C9 /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","NA","","","Performs an intermediate calculation for the next four SHA1 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA1MSG2 xmm1, xmm2/m128","NP 0F 38 CA /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","NA","","","Performs the final calculation for the next four SHA1 message dwords using intermediate results from xmm1 and the previous message dwords from xmm2/m128, storing the result in xmm1."
"SHA1NEXTE xmm1, xmm2/m128","NP 0F 38 C8 /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","NA","","","Calculates SHA1 state variable E after four rounds of operation from the current SHA1 state variable A in xmm1. The calculated value of the SHA1 state variable E is added to the scheduled dwords in xmm2/m128, and stored with some of the scheduled dwords in xmm1."
"SHA1RNDS4 xmm1, xmm2/m128, imm8","NP 0F 3A CC /r ib","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","","","Performs four rounds of SHA1 operation operating on SHA1 state (A,B,C,D) from xmm1, with a pre-computed sum of the next 4 round message dwords and state variable E from xmm2/m128. The immediate byte controls logic functions and round constants."
"SHA256MSG1 xmm1, xmm2/m128","NP 0F 38 CC /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","NA","","","Performs an intermediate calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA256MSG2 xmm1, xmm2/m128","NP 0F 38 CD /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","NA","","","Performs the final calculation for the next four SHA256 message dwords using previous message dwords from xmm1 and xmm2/m128, storing the result in xmm1."
"SHA256RNDS2 xmm1, xmm2/m128, <XMM0>","NP 0F 38 CB /r","Valid","Valid","Invalid","SHA","ModRM:reg (r, w)","ModRM:r/m (r)","Implicit XMM0 (r)","","","Perform 2 rounds of SHA256 operation using an initial SHA256 state (C,D,G,H) from xmm1, an initial SHA256 state (A,B,E,F) from xmm2/m128, and a pre-computed sum of the next 2 round mes-sage dwords and the corresponding round constants from the implicit operand XMM0, storing the updated SHA256 state (A,B,E,F) result in xmm1."
"SHLD r/m16, r16, imm8","0F A4 /r ib","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m16 to left imm8 places while shifting bits from r16 in from the right."
"SHLD r/m16, r16, CL","0F A5 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m16 to left CL places while shifting bits from r16 in from the right."
"SHLD r/m32, r32, imm8","0F A4 /r ib","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m32 to left imm8 places while shifting bits from r32 in from the right."
"SHLD r/m64, r64, imm8","REX.W + 0F A4 /r ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m64 to left imm8 places while shifting bits from r64 in from the right."
"SHLD r/m32, r32, CL","0F A5 /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m32 to left CL places while shifting bits from r32 in from the right."
"SHLD r/m64, r64, CL","REX.W + 0F A5 /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m64 to left CL places while shifting bits from r64 in from the right."
"SHRD r/m16, r16, imm8","0F AC /r ib","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m16 to right imm8 places while shifting bits from r16 in from the left."
"SHRD r/m16, r16, CL","0F AD /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m16 to right CL places while shifting bits from r16 in from the left."
"SHRD r/m32, r32, imm8","0F AC /r ib","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m32 to right imm8 places while shifting bits from r32 in from the left."
"SHRD r/m64, r64, imm8","REX.W + 0F AC /r ib","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","imm8","NA","","Shift r/m64 to right imm8 places while shifting bits from r64 in from the left."
"SHRD r/m32, r32, CL","0F AD /r","Valid","Valid","Valid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m32 to right CL places while shifting bits from r32 in from the left."
"SHRD r/m64, r64, CL","REX.W + 0F AD /r","Valid","Invalid","Invalid","","ModRM:r/m (w)","ModRM:reg (r)","CL","NA","","Shift r/m64 to right CL places while shifting bits from r64 in from the left."
"SHUFPD xmm1, xmm2/m128, imm8","66 0F C6 /r ib","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Shuffle two pairs of double-precision floating-point values from xmm1 and xmm2/m128 using imm8 to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","Imm8","NA","Shuffle two pairs of double-precision floating-point values from xmm2 and xmm3/m128 using imm8 to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","Imm8","NA","Shuffle four pairs of double-precision floating-point values from ymm2 and ymm3/m256 using imm8 to select from each pair, interleaved result is stored in xmm1."
"VSHUFPD xmm1{k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Shuffle two paris of double-precision floating-point values from xmm2 and xmm3/m128/m64bcst using imm8 to select from each pair. store interleaved results in xmm1 subject to writemask k1."
"VSHUFPD ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Shuffle four paris of double-precision floating-point values from ymm2 and ymm3/m256/m64bcst using imm8 to select from each pair. store interleaved results in ymm1 subject to writemask k1."
"VSHUFPD zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F.W1 C6 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Shuffle eight paris of double-precision floating-point values from zmm2 and zmm3/m512/m64bcst using imm8 to select from each pair. store interleaved results in zmm1 subject to writemask k1."
"SHUFPS xmm1, xmm3/m128, imm8","NP 0F C6 /r ib","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","Imm8","NA","NA","Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1."
"VSHUFPS xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","Imm8","NA","Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1."
"VSHUFPS ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.0F.WIG C6 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","Imm8","NA","Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1."
"VSHUFPS xmm1{k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Select from quadruplet of single-precision floating-point values in xmm1 and xmm2/m128 using imm8, interleaved result pairs are stored in xmm1, subject to writemask k1."
"VSHUFPS ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Select from quadruplet of single-precision floating-point values in ymm2 and ymm3/m256 using imm8, interleaved result pairs are stored in ymm1, subject to writemask k1."
"VSHUFPS zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.0F.W0 C6 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Select from quadruplet of single-precision floating-point values in zmm2 and zmm3/m512 using imm8, interleaved result pairs are stored in zmm1, subject to writemask k1."
"SIDT m","0F 01 /1","Valid","Valid","Valid","","","","","","","Store IDTR to m."
"SLDT r/m16","0F 00 /0","Valid","Valid","Valid","","","","","","","Stores segment selector from LDTR in r/m16."
"SLDT r64/m16","REX.W + 0F 00 /0","Valid","Invalid","Invalid","","","","","","","Stores segment selector from LDTR in r64/m16."
"SMSW r/m16","0F 01 /4","Valid","Valid","Valid","","","","","","","Store machine status word to r/m16."
"SMSW r32/m16","0F 01 /4","Valid","Valid","Valid","","","","","","","Store machine status word in low-order 16 bits of r32/m16; high-order 16 bits of r32 are undefined."
"SMSW r64/m16","REX.W + 0F 01 /4","Valid","Invalid","Invalid","","","","","","","Store machine status word in low-order 16 bits of r64/m16; high-order 16 bits of r32 are undefined."
"SQRTPD xmm1, xmm2/m128","66 0F 51 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed double-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPD xmm1, xmm2/m128","VEX.128.66.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed double-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPD ymm1, ymm2/m256","VEX.256.66.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed double-precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTPD xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPD ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPD zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the result in zmm1 subject to writemask k1."
"SQRTPS xmm1, xmm2/m128","NP 0F 51 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed single-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPS xmm1, xmm2/m128","VEX.128.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed single-precision floating-point values in xmm2/m128 and stores the result in xmm1."
"VSQRTPS ymm1, ymm2/m256","VEX.256.0F.WIG 51/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes Square Roots of the packed single-precision floating-point values in ymm2/m256 and stores the result in ymm1."
"VSQRTPS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the result in xmm1 subject to writemask k1."
"VSQRTPS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 51 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the result in ymm1 subject to writemask k1."
"VSQRTPS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 51/r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes Square Roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the result in zmm1 subject to writemask k1."
"SQRTSD xmm1,xmm2/m64","F2 0F 51/r","Valid","Valid","Invalid","SSE2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes square root of the low double-precision floating-point value in xmm2/m64 and stores the results in xmm1."
"VSQRTSD xmm1,xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 51/r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VSQRTSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.F2.0F.W1 51/r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes square root of the low double-precision floating-point value in xmm3/m64 and stores the results in xmm1 under writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"SQRTSS xmm1, xmm2/m32","F3 0F 51 /r","Valid","Valid","Invalid","SSE","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Computes square root of the low single-precision floating-point value in xmm2/m32 and stores the results in xmm1."
"VSQRTSS xmm1, xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 51 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"VSQRTSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.NDS.LIG.F3.0F.W0 51 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes square root of the low single-precision floating-point value in xmm3/m32 and stores the results in xmm1 under writemask k1. Also, upper single-precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]."
"STAC","NP 0F 01 CB","Valid","Valid","Invalid","SMAP","NA","NA","NA","NA","","Set the AC flag in the EFLAGS register."
"STC","F9","Valid","Valid","Valid","","NA","NA","NA","NA","","Set CF flag."
"STD","FD","Valid","Valid","Valid","","NA","NA","NA","NA","","Set DF flag."
"STI","FB","Valid","Valid","Valid","","NA","NA","NA","NA","","Set interrupt flag; external, maskable interrupts enabled at the end of the next instruction."
"STMXCSR m32","NP 0F AE /3","Valid","Valid","Invalid","SSE","ModRM:r/m (w)","NA","NA","NA","","Store contents of MXCSR register to m32."
"VSTMXCSR m32","VEX.LZ.0F.WIG AE /3","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","NA","NA","NA","","Store contents of MXCSR register to m32."
"STOS m8","AA","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI."
"STOS m16","AB","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI."
"STOS m32","AB","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI."
"STOS m64","REX.W + AB","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Store RAX at address RDI or EDI."
"STOSB","AA","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store AL at address ES:(E)DI; For 64-bit mode store AL at address RDI or EDI."
"STOSW","AB","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store AX at address ES:(E)DI; For 64-bit mode store AX at address RDI or EDI."
"STOSD","AB","Valid","Valid","Valid","","NA","NA","NA","NA","","For legacy mode, store EAX at address ES:(E)DI; For 64-bit mode store EAX at address RDI or EDI."
"STOSQ","REX.W + AB","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Store RAX at address RDI or EDI."
"STR r/m16","0F 00 /1","Valid","Valid","Valid","","","","","","","Stores segment selector from TR in r/m16."
"SUB AL, imm8","2C ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/26/32","NA","NA","","Subtract imm8 from AL."
"SUB AX, imm16","2D iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/26/32","NA","NA","","Subtract imm16 from AX."
"SUB EAX, imm32","2D id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/26/32","NA","NA","","Subtract imm32 from EAX."
"SUB RAX, imm32","REX.W + 2D id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8/26/32","NA","NA","","Subtract imm32 sign-extended to 64-bits from RAX."
"SUB r/m8, imm8","80 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract imm8 from r/m8."
"SUB r/m8, imm8","REX + 80 /5 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract imm8 from r/m8."
"SUB r/m16, imm16","81 /5 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract imm16 from r/m16."
"SUB r/m32, imm32","81 /5 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract imm32 from r/m32."
"SUB r/m64, imm32","REX.W + 81 /5 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract imm32 sign-extended to 64-bits from r/m64."
"SUB r/m16, imm8","83 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract sign-extended imm8 from r/m16."
"SUB r/m32, imm8","83 /5 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract sign-extended imm8 from r/m32."
"SUB r/m64, imm8","REX.W + 83 /5 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/26/32","NA","NA","","Subtract sign-extended imm8 from r/m64."
"SUB r/m8, r8","28 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Subtract r8 from r/m8."
"SUB r/m8, r8","REX + 28 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Subtract r8 from r/m8."
"SUB r/m16, r16","29 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Subtract r16 from r/m16."
"SUB r/m32, r32","29 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Subtract r32 from r/m32."
"SUB r/m64, r64","REX.W + 29 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Subtract r64 from r/m64."
"SUB r8, r/m8","2A /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract r/m8 from r8."
"SUB r8, r/m8","REX + 2A /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract r/m8 from r8."
"SUB r16, r/m16","2B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract r/m16 from r16."
"SUB r32, r/m32","2B /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract r/m32 from r32."
"SUB r64, r/m64","REX.W + 2B /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","Subtract r/m64 from r64."
"SUBPD xmm1, xmm2/m128","66 0F 5C /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed double-precision floating-point values in xmm2/mem from xmm1 and store result in xmm1."
"VSUBPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed double-precision floating-point values in xmm3/mem from xmm2 and store result in xmm1."
"VSUBPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed double-precision floating-point values in ymm3/mem from ymm2 and store result in ymm1."
"VSUBPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed double-precision floating-point values from xmm3/m128/m64bcst to xmm2 and store result in xmm1 with writemask k1."
"VSUBPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed double-precision floating-point values from ymm3/m256/m64bcst to ymm2 and store result in ymm1 with writemask k1."
"VSUBPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F.W1 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed double-precision floating-point values from zmm3/m512/m64bcst to zmm2 and store result in zmm1 with writemask k1."
"SUBPS xmm1, xmm2/m128","NP 0F 5C /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract packed single-precision floating-point values in xmm2/mem from xmm1 and store result in xmm1."
"VSUBPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed single-precision floating-point values in xmm3/mem from xmm2 and stores result in xmm1."
"VSUBPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract packed single-precision floating-point values in ymm3/mem from ymm2 and stores result in ymm1."
"VSUBPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed single-precision floating-point values from xmm3/m128/m32bcst to xmm2 and stores result in xmm1 with writemask k1."
"VSUBPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 5C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed single-precision floating-point values from ymm3/m256/m32bcst to ymm2 and stores result in ymm1 with writemask k1."
"VSUBPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.0F.W0 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Subtract packed single-precision floating-point values in zmm3/m512/m32bcst from zmm2 and stores result in zmm1 with writemask k1."
"SUBSD xmm1, xmm2/m64","F2 0F 5C /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract the low double-precision floating-point value in xmm2/m64 from xmm1 and store the result in xmm1."
"VSUBSD xmm1,xmm2, xmm3/m64","VEX.NDS.LIG.F2.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1."
"VSUBSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.F2.0F.W1 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Subtract the low double-precision floating-point value in xmm3/m64 from xmm2 and store the result in xmm1 under writemask k1."
"SUBSS xmm1, xmm2/m32","F3 0F 5C /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Subtract the low single-precision floating-point value in xmm2/m32 from xmm1 and store the result in xmm1."
"VSUBSS xmm1,xmm2, xmm3/m32","VEX.NDS.LIG.F3.0F.WIG 5C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1."
"VSUBSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.NDS.LIG.F3.0F.W0 5C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Subtract the low single-precision floating-point value in xmm3/m32 from xmm2 and store the result in xmm1 under writemask k1."
"SWAPGS","0F 01 F8","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Exchanges the current GS base register value with the value contained in MSR address C0000102H."
"SYSCALL","0F 05","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Fast call to privilege level 0 system procedures."
"SYSENTER","0F 34","Valid","Valid","Valid","","NA","NA","NA","NA","","Fast call to privilege level 0 system procedures."
"SYSEXIT","0F 35","Valid","Valid","Valid","","NA","NA","NA","NA","","Fast return to privilege level 3 user code."
"SYSEXIT","REX.W + 0F 35","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Fast return to 64-bit mode privilege level 3 user code."
"SYSRET","0F 07","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Return to compatibility mode from fast system call"
"SYSRET","REX.W + 0F 07","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Return to 64-bit mode from fast system call"
"TEST AL, imm8","A8 ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AND imm8 with AL; set SF, ZF, PF according to result."
"TEST AX, imm16","A9 iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AND imm16 with AX; set SF, ZF, PF according to result."
"TEST EAX, imm32","A9 id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AND imm32 with EAX; set SF, ZF, PF according to result."
"TEST RAX, imm32","REX.W + A9 id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AND imm32 sign-extended to 64-bits with RAX; set SF, ZF, PF according to result."
"TEST r/m8, imm8","F6 /0 ib","Valid","Valid","Valid","","ModRM:r/m (r)","imm8/16/32","NA","NA","","AND imm8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m8, imm8","REX + F6 /0 ib","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8/16/32","NA","NA","","AND imm8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m16, imm16","F7 /0 iw","Valid","Valid","Valid","","ModRM:r/m (r)","imm8/16/32","NA","NA","","AND imm16 with r/m16; set SF, ZF, PF according to result."
"TEST r/m32, imm32","F7 /0 id","Valid","Valid","Valid","","ModRM:r/m (r)","imm8/16/32","NA","NA","","AND imm32 with r/m32; set SF, ZF, PF according to result."
"TEST r/m64, imm32","REX.W + F7 /0 id","Valid","Invalid","Invalid","","ModRM:r/m (r)","imm8/16/32","NA","NA","","AND imm32 sign-extended to 64-bits with r/m64; set SF, ZF, PF according to result."
"TEST r/m8, r8","84 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","AND r8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m8, r8","REX + 84 /r","Valid","Invalid","Invalid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","AND r8 with r/m8; set SF, ZF, PF according to result."
"TEST r/m16, r16","85 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","AND r16 with r/m16; set SF, ZF, PF according to result."
"TEST r/m32, r32","85 /r","Valid","Valid","Valid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","AND r32 with r/m32; set SF, ZF, PF according to result."
"TEST r/m64, r64","REX.W + 85 /r","Valid","Invalid","Invalid","","ModRM:r/m (r)","ModRM:reg (r)","NA","NA","","AND r64 with r/m64; set SF, ZF, PF according to result."
"TZCNT r16, r/m16","F3 0F BC /r","Valid","Valid","Invalid","BMI1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of trailing zero bits in r/m16, return result in r16."
"TZCNT r32, r/m32","F3 0F BC /r","Valid","Valid","Invalid","BMI1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of trailing zero bits in r/m32, return result in r32."
"TZCNT r64, r/m64","F3 REX.W 0F BC /r","Valid","Invalid","Invalid","BMI1","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Count the number of trailing zero bits in r/m64, return result in r64."
"UCOMISD xmm1, xmm2/m64","66 0F 2E /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","NA","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VUCOMISD xmm1, xmm2/m64","VEX.LIG.66.0F.WIG 2E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","NA","Compare low double-precision floating-point values in xmm1 and xmm2/mem64 and set the EFLAGS flags accordingly."
"VUCOMISD xmm1, xmm2/m64{sae}","EVEX.LIG.66.0F.W1 2E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Compare low double-precision floating-point values in xmm1 and xmm2/m64 and set the EFLAGS flags accordingly."
"UCOMISS xmm1, xmm2/m32","NP 0F 2E /r","Valid","Valid","Invalid","SSE","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","NA","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32","VEX.LIG.0F.WIG 2E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","NA","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"VUCOMISS xmm1, xmm2/m32{sae}","EVEX.LIG.0F.W0 2E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Compare low single-precision floating-point values in xmm1 and xmm2/mem32 and set the EFLAGS flags accordingly."
"UD0","0F FF","Valid","Valid","Valid","","NA","NA","NA","NA","","Raise invalid opcode exception."
"UD1 r32, r/m32","0F B9 /r","Valid","Valid","Valid","","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Raise invalid opcode exception."
"UD2","0F 0B","Valid","Valid","Valid","","NA","NA","NA","NA","","Raise invalid opcode exception."
"UNPCKHPD xmm1, xmm2/m128","66 0F 15 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm1 and xmm2/m128."
"VUNPCKHPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves double-precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPD ymm1,ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves double-precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKHPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double precision floating-point values from high quadwords of xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VUNPCKHPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double precision floating-point values from high quadwords of ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VUNPCKHPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"UNPCKHPS xmm1, xmm2/m128","NP 0F 15 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm1 and xmm2/m128."
"VUNPCKHPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128."
"VUNPCKHPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 15 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256."
"VUNPCKHPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of xmm2 and xmm3/m128/m32bcst and write result to xmm1 subject to writemask k1."
"VUNPCKHPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of ymm2 and ymm3/m256/m32bcst and write result to ymm1 subject to writemask k1."
"VUNPCKHPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from high quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to writemask k1."
"UNPCKLPD xmm1, xmm2/m128","66 0F 14 /r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm1 and xmm2/m128."
"VUNPCKLPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves double-precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPD ymm1,ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves double-precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VUNPCKLPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double precision floating-point values from low quadwords of xmm2 and xmm3/m128/m64bcst subject to write mask k1."
"VUNPCKLPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double precision floating-point values from low quadwords of ymm2 and ymm3/m256/m64bcst subject to write mask k1."
"VUNPCKLPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves double-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m64bcst subject to write mask k1."
"UNPCKLPS xmm1, xmm2/m128","NP 0F 14 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm1 and xmm2/m128."
"VUNPCKLPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/m128."
"VUNPCKLPS ymm1,ymm2,ymm3/m256","VEX.NDS.256.0F.WIG 14 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/m256."
"VUNPCKLPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of xmm2 and xmm3/mem and write result to xmm1 subject to write mask k1."
"VUNPCKLPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of ymm2 and ymm3/mem and write result to ymm1 subject to write mask k1."
"VUNPCKLPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Unpacks and Interleaves single-precision floating-point values from low quadwords of zmm2 and zmm3/m512/m32bcst and write result to zmm1 subject to write mask k1."
"VALIGND xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors xmm2 and xmm3/m128/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask."
"VALIGNQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors xmm2 and xmm3/m128/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in xmm1, under writemask."
"VALIGND ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors ymm2 and ymm3/m256/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask."
"VALIGNQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors ymm2 and ymm3/m256/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in ymm1, under writemask."
"VALIGND zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.66.0F3A.W0 03 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors zmm2 and zmm3/m512/m32bcst with double-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask."
"VALIGNQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F3A.W1 03 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Shift right and merge vectors zmm2 and zmm3/m512/m64bcst with quad-word granularity using imm8 as number of elements to shift, and store the final result in zmm1, under writemask."
"VBLENDMPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend double-precision vector xmm2 and double-precision vector xmm3/m128/m64bcst and store the result in xmm1, under control mask."
"VBLENDMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend double-precision vector ymm2 and double-precision vector ymm3/m256/m64bcst and store the result in ymm1, under control mask."
"VBLENDMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 65 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend double-precision vector zmm2 and double-precision vector zmm3/m512/m64bcst and store the result in zmm1, under control mask."
"VBLENDMPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend single-precision vector xmm2 and single-precision vector xmm3/m128/m32bcst and store the result in xmm1, under control mask."
"VBLENDMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend single-precision vector ymm2 and single-precision vector ymm3/m256/m32bcst and store the result in ymm1, under control mask."
"VBLENDMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 65 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend single-precision vector zmm2 and single-precision vector zmm3/m512/m32bcst using k1 as select control and store the result in zmm1."
"VBROADCASTSS xmm1, m32","VEX.128.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast single-precision floating-point element in mem to four locations in xmm1."
"VBROADCASTSS ymm1, m32","VEX.256.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast single-precision floating-point element in mem to eight locations in ymm1."
"VBROADCASTSD ymm1, m64","VEX.256.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast double-precision floating-point element in mem to four locations in ymm1."
"VBROADCASTF128 ymm1, m128","VEX.256.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast 128 bits of floating-point data in mem to low and high 128-bits in ymm1."
"VBROADCASTSS xmm1, xmm2","VEX.128.66.0F38.W0 18/r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast the low single-precision floating-point element in the source operand to four locations in xmm1."
"VBROADCASTSS ymm1, xmm2","VEX.256.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast low single-precision floating-point element in the source operand to eight locations in ymm1."
"VBROADCASTSD ymm1, xmm2","VEX.256.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Broadcast low double-precision floating-point element in the source operand to four locations in ymm1."
"VBROADCASTSD ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.W1 19 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Broadcast low double-precision floating-point element in xmm2/m64 to four locations in ymm1 using writemask k1."
"VBROADCASTSD zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.W1 19 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Broadcast low double-precision floating-point element in xmm2/m64 to eight locations in zmm1 using writemask k1."
"VBROADCASTF32X2 ymm1 {k1}{z}, xmm2/m64","EVEX.256.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple2","Broadcast two single-precision floating-point elements in xmm2/m64 to locations in ymm1 using writemask k1."
"VBROADCASTF32X2 zmm1 {k1}{z}, xmm2/m64","EVEX.512.66.0F38.W0 19 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple2","Broadcast two single-precision floating-point elements in xmm2/m64 to locations in zmm1 using writemask k1."
"VBROADCASTSS xmm1 {k1}{z}, xmm2/m32","EVEX.128.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in xmm1 using writemask k1."
"VBROADCASTSS ymm1 {k1}{z}, xmm2/m32","EVEX.256.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in ymm1 using writemask k1."
"VBROADCASTSS zmm1 {k1}{z}, xmm2/m32","EVEX.512.66.0F38.W0 18 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Broadcast low single-precision floating-point element in xmm2/m32 to all locations in zmm1 using writemask k1."
"VBROADCASTF32X4 ymm1 {k1}{z}, m128","EVEX.256.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple4","Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in ymm1 using writemask k1."
"VBROADCASTF32X4 zmm1 {k1}{z}, m128","EVEX.512.66.0F38.W0 1A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple4","Broadcast 128 bits of 4 single-precision floating-point data in mem to locations in zmm1 using writemask k1."
"VBROADCASTF64X2 ymm1 {k1}{z}, m128","EVEX.256.66.0F38.W1 1A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple2","Broadcast 128 bits of 2 double-precision floating-point data in mem to locations in ymm1 using writemask k1.VBROADCASTâ€”Load with Broadcast Floating-Point Data"
"VCOMPRESSPD xmm1/m128 {k1}{z}, xmm2","EVEX.128.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed double-precision floating-point values from xmm2 to xmm1/m128 using writemask k1."
"VCOMPRESSPD ymm1/m256 {k1}{z}, ymm2","EVEX.256.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed double-precision floating-point values from ymm2 to ymm1/m256 using writemask k1."
"VCOMPRESSPD zmm1/m512 {k1}{z}, zmm2","EVEX.512.66.0F38.W1 8A /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed double-precision floating-point values from zmm2 using control mask k1 to zmm1/m512."
"VCOMPRESSPS xmm1/m128 {k1}{z}, xmm2","EVEX.128.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed single-precision floating-point values from xmm2 to xmm1/m128 using writemask k1."
"VCOMPRESSPS ymm1/m256 {k1}{z}, ymm2","EVEX.256.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed single-precision floating-point values from ymm2 to ymm1/m256 using writemask k1."
"VCOMPRESSPS zmm1/m512 {k1}{z}, zmm2","EVEX.512.66.0F38.W0 8A /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed single-precision floating-point values from zmm2 using control mask k1 to zmm1/m512."
"VCVTPD2QQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 7B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values from xmm2/m128/m64bcst to two packed quadword integers in xmm1 with writemask k1."
"VCVTPD2QQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 7B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 with writemask k1."
"VCVTPD2QQ zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 7B /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values from zmm2/m512/m64bcst to eight packed quadword integers in zmm1 with writemask k1."
"VCVTPD2UDQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2UDQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 subject to writemask k1."
"VCVTPD2UDQ ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.0F.W1 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 subject to writemask k1."
"VCVTPD2UQQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values from xmm2/mem to two packed unsigned quadword integers in xmm1 with writemask k1."
"VCVTPD2UQQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 79 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert fourth packed double-precision floating-point values from ymm2/mem to four packed unsigned quadword integers in ymm1 with writemask k1."
"VCVTPD2UQQ zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.66.0F.W1 79 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values from zmm2/mem to eight packed unsigned quadword integers in zmm1 with writemask k1."
"VCVTPH2PS xmm1, xmm2/m64","VEX.128.66.0F38.W0 13 /r","Valid","Valid","Invalid","F16C","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point value in xmm1."
"VCVTPH2PS ymm1, xmm2/m128","VEX.256.66.0F38.W0 13 /r","Valid","Valid","Invalid","F16C","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point value in ymm1."
"VCVTPH2PS xmm1 {k1}{z}, xmm2/m64","EVEX.128.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Convert four packed half precision (16-bit) floating-point values in xmm2/m64 to packed single-precision floating-point values in xmm1."
"VCVTPH2PS ymm1 {k1}{z}, xmm2/m128","EVEX.256.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Convert eight packed half precision (16-bit) floating-point values in xmm2/m128 to packed single-precision floating-point values in ymm1."
"VCVTPH2PS zmm1 {k1}{z}, ymm2/m256 {sae}","EVEX.512.66.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector Mem","Convert sixteen packed half precision (16-bit) floating-point values in ymm2/m256 to packed single-precision floating-point values in zmm1."
"VCVTPS2PH xmm1/m64, xmm2, imm8","VEX.128.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","F16C","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Convert four packed single-precision floating-point values in xmm2 to packed half-precision (16-bit) floating-point values in xmm1/m64. Imm8 provides rounding controls."
"VCVTPS2PH xmm1/m128, ymm2, imm8","VEX.256.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","F16C","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Convert eight packed single-precision floating-point values in ymm2 to packed half-precision (16-bit) floating-point values in xmm1/m128. Imm8 provides rounding controls."
"VCVTPS2PH xmm1/m64 {k1}{z}, xmm2, imm8","EVEX.128.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Half Vector Mem","Convert four packed single-precision floating-point values in xmm2 to packed half-precision (16-bit) floating-point values in xmm1/m64. Imm8 provides rounding controls."
"VCVTPS2PH xmm1/m128 {k1}{z}, ymm2, imm8","EVEX.256.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Half Vector Mem","Convert eight packed single-precision floating-point values in ymm2 to packed half-precision (16-bit) floating-point values in xmm1/m128. Imm8 provides rounding controls."
"VCVTPS2PH ymm1/m256 {k1}{z}, zmm2{sae}, imm8","EVEX.512.66.0F3A.W0 1D /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Half Vector Mem","Convert sixteen packed single-precision floating-point values in zmm2 to packed half-precision (16-bit) floating-point values in ymm1/m256. Imm8 provides rounding controls."
"VCVTPS2QQ xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.66.0F.W0 7B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 subject to writemask k1."
"VCVTPS2QQ ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.66.0F.W0 7B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 subject to writemask k1."
"VCVTPS2QQ zmm1 {k1}{z}, ymm2/m256/m32bcst{er}","EVEX.512.66.0F.W0 7B /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 subject to writemask k1."
"VCVTPS2UDQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 subject to writemask k1."
"VCVTPS2UDQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 subject to writemask k1."
"VCVTPS2UDQ zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.0F.W0 79 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 subject to writemask k1."
"VCVTPS2UQQ xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.66.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed single precision floating-point values from zmm2/m64/m32bcst to two packed unsigned quadword values in zmm1 subject to writemask k1."
"VCVTPS2UQQ ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.66.0F.W0 79 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 subject to writemask k1."
"VCVTPS2UQQ zmm1 {k1}{z}, ymm2/m256/m32bcst{er}","EVEX.512.66.0F.W0 79 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 subject to writemask k1."
"VCVTQQ2PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.F3.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed quadword integers from xmm2/m128/m64bcst to packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTQQ2PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.F3.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed quadword integers from ymm2/m256/m64bcst to packed double-precision floating-point values in ymm1 with writemask k1."
"VCVTQQ2PD zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.F3.0F.W1 E6 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed quadword integers from zmm2/m512/m64bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTQQ2PS xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.0F.W1 5B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed quadword integers from xmm2/mem to packed single-precision floating-point values in xmm1 with writemask k1."
"VCVTQQ2PS xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.0F.W1 5B /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed quadword integers from ymm2/mem to packed single-precision floating-point values in xmm1 with writemask k1."
"VCVTQQ2PS ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.0F.W1 5B /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed quadword integers from zmm2/mem to eight packed single-precision floating-point values in ymm1 with writemask k1."
"VCVTSD2USI r32,xmm1/m64{er}",EVEX.LIG.F2.0F.W0 79 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r32.
"VCVTSD2USI r64,xmm1/m64{er}",EVEX.LIG.F2.0F.W1 79 /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zeroextended into r64.
"VCVTSS2USI r32,xmm1/m32{er}",EVEX.LIG.F3.0F.W0 79 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r32.
"VCVTSS2USI r64,xmm1/m32{er}",EVEX.LIG.F3.0F.W1 79 /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r64.
"VCVTTPD2QQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values from zmm2/m128/m64bcst to two packed quadword integers in zmm1 using truncation with writemask k1."
"VCVTTPD2QQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed quadword integers in ymm1 using truncation with writemask k1."
"VCVTTPD2QQ zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F.W1 7A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values from zmm2/m512 to eight packed quadword integers in zmm1 using truncation with writemask k1."
"VCVTTPD2UDQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.0F.W1 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values in xmm2/m128/m64bcst to two unsigned doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2UDQ xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.0F.W1 78 02 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values in ymm2/m256/m64bcst to four unsigned doubleword integers in xmm1 using truncation subject to writemask k1."
"VCVTTPD2UDQ ymm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.0F.W1 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values in zmm2/m512/m64bcst to eight unsigned doubleword integers in ymm1 using truncation subject to writemask k1."
"VCVTTPD2UQQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F.W1 78 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed double-precision floating-point values from xmm2/m128/m64bcst to two packed unsigned quadword integers in xmm1 using truncation with writemask k1."
"VCVTTPD2UQQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F.W1 78 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed double-precision floating-point values from ymm2/m256/m64bcst to four packed unsigned quadword integers in ymm1 using truncation with writemask k1."
"VCVTTPD2UQQ zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F.W1 78 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed double-precision floating-point values from zmm2/mem to eight packed unsigned quadword integers in zmm1 using truncation with writemask k1."
"VCVTTPS2QQ xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.66.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed signed quadword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2QQ ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.66.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed signed quadword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2QQ zmm1 {k1}{z}, ymm2/m256/m32bcst{sae}","EVEX.512.66.0F.W0 7A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed signed quadword values in zmm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned doubleword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned doubleword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2UDQ zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}","EVEX.512.0F.W0 78 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed single-precision floating-point values from zmm2/m512/m32bcst to sixteen packed unsigned doubleword values in zmm1 using truncation subject to writemask k1."
"VCVTTPS2UQQ xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.66.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed single precision floating-point values from xmm2/m64/m32bcst to two packed unsigned quadword values in xmm1 using truncation subject to writemask k1."
"VCVTTPS2UQQ ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.66.0F.W0 78 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed single precision floating-point values from xmm2/m128/m32bcst to four packed unsigned quadword values in ymm1 using truncation subject to writemask k1."
"VCVTTPS2UQQ zmm1 {k1}{z}, ymm2/m256/m32bcst{sae}","EVEX.512.66.0F.W0 78 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed single precision floating-point values from ymm2/m256/m32bcst to eight packed unsigned quadword values in zmm1 using truncation subject to writemask k1."
"VCVTTSD2USI r32,xmm1/m64{sae}",EVEX.LIG.F2.0F.W0 78 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one unsigned doubleword integer r32 using truncation.
"VCVTTSD2USI r64,xmm1/m64{sae}",EVEX.LIG.F2.0F.W1 78 /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one double-precision floating-point value from xmm1/m64 to one unsigned quadword integer zeroextended into r64 using truncation.
"VCVTTSS2USI r32,xmm1/m32{sae}",EVEX.LIG.F3.0F.W0 78 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one unsigned doubleword integer in r32 using truncation.
"VCVTTSS2USI r64,xmm1/m32{sae}",EVEX.LIG.F3.0F.W1 78 /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Fixed,Convert one single-precision floating-point value from xmm1/m32 to one unsigned quadword integer in r64 using truncation.
"VCVTUDQ2PD xmm1 {k1}{z}, xmm2/m64/m32bcst","EVEX.128.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert two packed unsigned doubleword integers from ymm2/m64/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PD ymm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.256.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PD zmm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.512.F3.0F.W0 7A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Half Vector","Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed unsigned doubleword integers from xmm2/m128/m32bcst to packed single-precision floating-point values in xmm1 with writemask k1."
"VCVTUDQ2PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed unsigned doubleword integers from ymm2/m256/m32bcst to packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUDQ2PS zmm1 {k1}{z}, zmm2/m512/m32bcst{er}","EVEX.512.F2.0F.W0 7A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert sixteen packed unsigned doubleword integers from zmm2/m512/m32bcst to sixteen packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUQQ2PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.F3.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to two packed double-precision floating-point values in xmm1 with writemask k1."
"VCVTUQQ2PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.F3.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed double-precision floating-point values in ymm1 with writemask k1."
"VCVTUQQ2PD zmm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.F3.0F.W1 7A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed double-precision floating-point values in zmm1 with writemask k1."
"VCVTUQQ2PS xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.F2.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert two packed unsigned quadword integers from xmm2/m128/m64bcst to packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUQQ2PS xmm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.F2.0F.W1 7A /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert four packed unsigned quadword integers from ymm2/m256/m64bcst to packed single-precision floating-point values in xmm1 with writemask k1."
"VCVTUQQ2PS ymm1 {k1}{z}, zmm2/m512/m64bcst{er}","EVEX.512.F2.0F.W1 7A /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert eight packed unsigned quadword integers from zmm2/m512/m64bcst to eight packed single-precision floating-point values in zmm1 with writemask k1."
"VCVTUSI2SD xmm1,xmm2,r/m32",EVEX.NDS.LIG.F2.0F.W0 7B /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Convert one unsigned doubleword integer from r/m32 to one double-precision floating-point value in xmm1.
"VCVTUSI2SD xmm1,xmm2,r/m64{er}",EVEX.NDS.LIG.F2.0F.W1 7B /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),EVEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Convert one unsigned quadword integer from r/m64 to one double-precision floating-point value in xmm1.
"VCVTUSI2SS xmm1,xmm2,r/m32{er}",EVEX.NDS.LIG.F3.0F.W0 7B /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Convert one signed doubleword integer from r/m32 to one single-precision floating-point value in xmm1.
"VCVTUSI2SS xmm1,xmm2,r/m64{er}",EVEX.NDS.LIG.F3.0F.W1 7B /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),VEX.vvvv,ModRM:r/m (r),NA,Tuple1 Scalar,Convert one signed quadword integer from r/m64 to one single-precision floating-point value in xmm1.
"VDBPSADBW xmm1 {k1}{z}, xmm2, xmm3/m128, imm8","EVEX.NDS.128.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","","Compute packed SAD word results of unsigned bytes in dword block from xmm2 with unsigned bytes of dword blocks transformed from xmm3/m128 using the shuffle controls in imm8. Results are written to xmm1 under the writemask k1."
"VDBPSADBW ymm1 {k1}{z}, ymm2, ymm3/m256, imm8","EVEX.NDS.256.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","","Compute packed SAD word results of unsigned bytes in dword block from ymm2 with unsigned bytes of dword blocks transformed from ymm3/m256 using the shuffle controls in imm8. Results are written to ymm1 under the writemask k1."
"VDBPSADBW zmm1 {k1}{z}, zmm2, zmm3/m512, imm8","EVEX.NDS.512.66.0F3A.W0 42 /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","","Compute packed SAD word results of unsigned bytes in dword block from zmm2 with unsigned bytes of dword blocks transformed from zmm3/m512 using the shuffle controls in imm8. Results are written to zmm1 under the writemask k1."
"VERR r/m16","0F 00 /4","Valid","Valid","Valid","","","","","","","Set ZF=1 if segment specified with r/m16 can be read."
"VERW r/m16","0F 00 /5","Valid","Valid","Valid","","","","","","","Set ZF=1 if segment specified with r/m16 can be written."
"VEXP2PD zmm1 {k1}{z}, zmm2/m512/m64bcst {sae}","EVEX.512.66.0F38.W1 C8 /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed double-precision floating-point values from zmm2/m512/m64bcst and stores the floating-point result in zmm1with writemask k1."
"VEXP2PS zmm1 {k1}{z}, zmm2/m512/m32bcst {sae}","EVEX.512.66.0F38.W0 C8 /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes approximations to the exponential 2^x (with less than 2^-23 of maximum relative error) of the packed single-precision floating-point values from zmm2/m512/m32bcst and stores the floating-point result in zmm1with writemask k1."
"VEXPANDPD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VEXPANDPD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VEXPANDPD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.W1 88 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VEXPANDPS xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed single-precision floating-point values from xmm2/m128 to xmm1 using writemask k1."
"VEXPANDPS ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed single-precision floating-point values from ymm2/m256 to ymm1 using writemask k1."
"VEXPANDPS zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.W0 88 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed single-precision floating-point values from zmm2/m512 to zmm1 using writemask k1."
"VEXTRACTF128 xmm1/m128, ymm2, imm8","VEX.256.66.0F3A.W0 19 /r ib","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Extract 128 bits of packed floating-point values from ymm2 and store results in xmm1/m128. EVEX.256.66.0F3A.W0 19 /r ib VEXTRACTF32X4 xmm1/m128 {k1}{z}, ymm2, imm8 C V/V AVX512VL AVX512F Extract 128 bits of packed single-precision floating-point values from ymm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTF32x4 xmm1/m128 {k1}{z}, zmm2, imm8","EVEX.512.66.0F3A.W0 19 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Tuple4","Extract 128 bits of packed single-precision floating-point values from zmm2 and store results in xmm1/m128 subject to writemask k1. EVEX.256.66.0F3A.W1 19 /r ib VEXTRACTF64X2 xmm1/m128 {k1}{z}, ymm2, imm8 B V/V AVX512VL AVX512DQ Extract 128 bits of packed double-precision floating-point values from ymm2 and store results in xmm1/m128 subject to writemask k1. EVEX.512.66.0F3A.W1 19 /r ib VEXTRACTF64X2 xmm1/m128 {k1}{z}, zmm2, imm8 B V/V AVX512DQ Extract 128 bits of packed double-precision floating-point values from zmm2 and store results in xmm1/m128 subject to writemask k1. EVEX.512.66.0F3A.W0 1B /r ib VEXTRACTF32X8 ymm1/m256 {k1}{z}, zmm2, imm8 D V/V AVX512DQ Extract 256 bits of packed single-precision floating-point values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VEXTRACTF64x4 ymm1/m256 {k1}{z}, zmm2, imm8","EVEX.512.66.0F3A.W1 1B /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Tuple4","Extract 256 bits of packed double-precision floating-point values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VEXTRACTI128 xmm1/m128, ymm2, imm8","VEX.256.66.0F3A.W0 39 /r ib","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","NA","Extract 128 bits of integer data from ymm2 and store results in xmm1/m128. EVEX.256.66.0F3A.W0 39 /r ib VEXTRACTI32X4 xmm1/m128 {k1}{z}, ymm2, imm8 C V/V AVX512VL AVX512F Extract 128 bits of double-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1."
"VEXTRACTI32x4 xmm1/m128 {k1}{z}, zmm2, imm8","EVEX.512.66.0F3A.W0 39 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Tuple4","Extract 128 bits of double-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1. EVEX.256.66.0F3A.W1 39 /r ib VEXTRACTI64X2 xmm1/m128 {k1}{z}, ymm2, imm8 B V/V AVX512VL AVX512DQ Extract 128 bits of quad-word integer values from ymm2 and store results in xmm1/m128 subject to writemask k1. EVEX.512.66.0F3A.W1 39 /r ib VEXTRACTI64X2 xmm1/m128 {k1}{z}, zmm2, imm8 B V/V AVX512DQ Extract 128 bits of quad-word integer values from zmm2 and store results in xmm1/m128 subject to writemask k1. EVEX.512.66.0F3A.W0 3B /r ib VEXTRACTI32X8 ymm1/m256 {k1}{z}, zmm2, imm8 D V/V AVX512DQ Extract 256 bits of double-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VEXTRACTI64x4 ymm1/m256 {k1}{z}, zmm2, imm8","EVEX.512.66.0F3A.W1 3B /r ib","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","Imm8","NA","Tuple4","Extract 256 bits of quad-word integer values from zmm2 and store results in ymm1/m256 subject to writemask k1."
"VFIXUPIMMPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up special numbers in float64 vector xmm1, float64 vector xmm2 and int64 vector xmm3/m128/m64bcst and store the result in xmm1, under writemask."
"VFIXUPIMMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up special numbers in float64 vector ymm1, float64 vector ymm2 and int64 vector ymm3/m256/m64bcst and store the result in ymm1, under writemask."
"VFIXUPIMMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}, imm8","EVEX.NDS.512.66.0F3A.W1 54 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up elements of float64 vector in zmm2 using int64 vector table in zmm3/m512/m64bcst, combine with preserved elements from zmm1, and store the result in zmm1."
"VFIXUPIMMPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.66.0F3A.W0 54 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up special numbers in float32 vector xmm1, float32 vector xmm2 and int32 vector xmm3/m128/m32bcst and store the result in xmm1, under writemask."
"VFIXUPIMMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.66.0F3A.W0 54 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up special numbers in float32 vector ymm1, float32 vector ymm2 and int32 vector ymm3/m256/m32bcst and store the result in ymm1, under writemask."
"VFIXUPIMMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}, imm8","EVEX.NDS.512.66.0F3A.W0 54 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Full Vector","Fix up elements of float32 vector in zmm2 using int32 vector table in zmm3/m512/m32bcst, combine with preserved elements from zmm1, and store the result in zmm1."
"VFIXUPIMMSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W1 55 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Fix up a float64 number in the low quadword element of xmm2 using scalar int32 table in xmm3/m64 and store the result in xmm1."
"VFIXUPIMMSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W0 55 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Fix up a float32 number in the low doubleword element in xmm2 using scalar int32 table in xmm3/m32 and store the result in xmm1."
"VFMADD132PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 98 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1."
"VFMADD213PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 A8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1."
"VFMADD231PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 B8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1."
"VFMADD132PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 98 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1."
"VFMADD213PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 A8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1."
"VFMADD231PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 B8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1."
"VFMADD132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, add to xmm2 and put result in xmm1."
"VFMADD213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m64bcst and put result in xmm1."
"VFMADD231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, add to xmm1 and put result in xmm1."
"VFMADD132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, add to ymm2 and put result in ymm1."
"VFMADD213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m64bcst and put result in ymm1."
"VFMADD231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, add to ymm1 and put result in ymm1."
"VFMADD132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, add to zmm2 and put result in zmm1."
"VFMADD213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 A8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m64bcst and put result in zmm1."
"VFMADD231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 B8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, add to zmm1 and put result in zmm1.VFMADD132PD/VFMADD213PD/VFMADD231PDâ€”Fused Multiply-Add of Packed Double-Precision Floating-Point Values"
"VFMADD132PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 98 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, add to xmm2 and put result in xmm1."
"VFMADD213PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 A8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, add to xmm3/mem and put result in xmm1."
"VFMADD231PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 B8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, add to xmm1 and put result in xmm1."
"VFMADD132PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 98 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, add to ymm2 and put result in ymm1."
"VFMADD213PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 A8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, add to ymm3/mem and put result in ymm1."
"VFMADD231PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 B8 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, add to ymm1 and put result in ymm1."
"VFMADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add to xmm2 and put result in xmm1."
"VFMADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, add to xmm3/m128/m32bcst and put result in xmm1."
"VFMADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add to xmm1 and put result in xmm1."
"VFMADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add to ymm2 and put result in ymm1."
"VFMADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, add to ymm3/m256/m32bcst and put result in ymm1."
"VFMADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add to ymm1 and put result in ymm1."
"VFMADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 98 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add to zmm2 and put result in zmm1."
"VFMADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 A8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, add to zmm3/m512/m32bcst and put result in zmm1."
"VFMADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 B8 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add to zmm1 and put result in zmm1.VFMADD132PS/VFMADD213PS/VFMADD231PSâ€”Fused Multiply-Add of Packed Single-Precision Floating-Point Values"
"VFMADD132SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 99 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1."
"VFMADD213SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 A9 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1."
"VFMADD231SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 B9 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1."
"VFMADD132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 99 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, add to xmm2 and put result in xmm1."
"VFMADD213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 A9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, add to xmm3/m64 and put result in xmm1."
"VFMADD231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 B9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, add to xmm1 and put result in xmm1."
"VFMADD132SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 99 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1."
"VFMADD213SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 A9 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1."
"VFMADD231SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 B9 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1."
"VFMADD132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 99 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, add to xmm2 and put result in xmm1."
"VFMADD213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 A9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, add to xmm3/m32 and put result in xmm1."
"VFMADD231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 B9 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, add to xmm1 and put result in xmm1."
"VFMADDSUB132PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 96 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm1 and xmm3/mem,add/subtract elements in xmm2 and put result in xmm1."
"VFMADDSUB213PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 A6 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm1 and xmm2,add/subtract elements in xmm3/mem and put result in xmm1."
"VFMADDSUB231PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 B6 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm2 and xmm3/mem,add/subtract elements in xmm1 and put result in xmm1."
"VFMADDSUB132PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 96 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm1 and ymm3/mem,add/subtract elements in ymm2 and put result in ymm1."
"VFMADDSUB213PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 A6 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm1 and ymm2,add/subtract elements in ymm3/mem and put result in ymm1."
"VFMADDSUB231PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 B6 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm2 and ymm3/mem,add/subtract elements in ymm1 and put result in ymm1."
"VFMADDSUB213PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 A6 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm1 and xmm2,add/subtract elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMADDSUB231PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 B6 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst,add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMADDSUB132PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 96 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst,add/subtract elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMADDSUB213PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 A6 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm1 and ymm2,add/subtract elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMADDSUB231PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 B6 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst,add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMADDSUB132PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 96 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst,add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMADDSUB213PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 A6 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm1and zmm2,add/subtract elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMADDSUB231PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 B6 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst,add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMADDSUB132PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 96 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst,add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMADDSUB132PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 96 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, add/subtract elements in xmm2 and put result in xmm1."
"VFMADDSUB213PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 A6 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/mem and put result in xmm1."
"VFMADDSUB231PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 B6 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, add/subtract elements in xmm1 and put result in xmm1."
"VFMADDSUB132PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 96 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, add/subtract elements in ymm2 and put result in ymm1."
"VFMADDSUB213PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 A6 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/mem and put result in ymm1."
"VFMADDSUB231PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 B6 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, add/subtract elements in ymm1 and put result in ymm1."
"VFMADDSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, add/subtract elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1."
"VFMADDSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, add/subtract elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMADDSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, add/subtract elements in zmm2 and put result in xmm1 subject to writemask k1."
"VFMADDSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, add/subtract elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1."
"VFMADDSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, add/subtract elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMADDSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, add/subtract elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMADDSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 A6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, add/subtract elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1."
"VFMADDSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 B6 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, add/subtract elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMADDSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 96 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, add/subtract elements in zmm2 and put result in zmm1 subject to writemask k1.VFMADDSUB132PS/VFMADDSUB213PS/VFMADDSUB231PSâ€”Fused Multiply-Alternating Add/Subtract of Packed Single-Precision"
"VFMSUB132PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 9A /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1."
"VFMSUB213PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 AA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1."
"VFMSUB231PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 BA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1."
"VFMSUB132PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 9A /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1."
"VFMSUB213PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 AA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1."
"VFMSUB231PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 BA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1.S"
"VFMSUB132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, subtract xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUB213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, subtract xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMSUB231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, subtract xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUB132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, subtract ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUB213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMSUB231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, subtract ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUB132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 9A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, subtract zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUB213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 AA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMSUB231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 BA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, subtract zmm1 and put result in zmm1 subject to writemask k1.VFMSUB132PD/VFMSUB213PD/VFMSUB231PDâ€”Fused Multiply-Subtract of Packed Double-Precision Floating-Point Values"
"VFMSUB132PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 9A /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, subtract xmm2 and put result in xmm1."
"VFMSUB213PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 AA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract xmm3/mem and put result in xmm1."
"VFMSUB231PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 BA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, subtract xmm1 and put result in xmm1."
"VFMSUB132PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 9A /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, subtract ymm2 and put result in ymm1."
"VFMSUB213PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 AA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract ymm3/mem and put result in ymm1."
"VFMSUB231PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 BA /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, subtract ymm1 and put result in ymm1."
"VFMSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract xmm2 and put result in xmm1."
"VFMSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract xmm3/m128/m32bcst and put result in xmm1."
"VFMSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract xmm1 and put result in xmm1."
"VFMSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract ymm2 and put result in ymm1."
"VFMSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract ymm3/m256/m32bcst and put result in ymm1."
"VFMSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract ymm1 and put result in ymm1."
"VFMSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 9A /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract zmm2 and put result in zmm1."
"VFMSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 AA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract zmm3/m512/m32bcst and put result in zmm1."
"VFMSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 BA /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract zmm1 and put result in zmm1.VFMSUB132PS/VFMSUB213PS/VFMSUB231PSâ€”Fused Multiply-Subtract of Packed Single-Precision Floating-Point Values"
"VFMSUB132SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 9B /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1."
"VFMSUB213SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 AB /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1."
"VFMSUB231SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 BB /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1."
"VFMSUB132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 9B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, subtract xmm2 and put result in xmm1."
"VFMSUB213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 AB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, subtract xmm3/m64 and put result in xmm1."
"VFMSUB231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 BB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, subtract xmm1 and put result in xmm1."
"VFMSUB132SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 9B /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1."
"VFMSUB213SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 AB /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1."
"VFMSUB231SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 BB /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1."
"VFMSUB132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 9B /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, subtract xmm2 and put result in xmm1."
"VFMSUB213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 AB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, subtract xmm3/m32 and put result in xmm1."
"VFMSUB231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 BB /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, subtract xmm1 and put result in xmm1."
"VFMSUBADD132PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 97 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm1 and xmm3/mem,subtract/add elements in xmm2 and put result in xmm1."
"VFMSUBADD213PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 A7 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm1 and xmm2,subtract/add elements in xmm3/mem and put result in xmm1."
"VFMSUBADD231PD xmm1,xmm2,xmm3/m128",VEX.DDS.128.66.0F38.W1 B7 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from xmm2 and xmm3/mem,subtract/add elements in xmm1 and put result in xmm1."
"VFMSUBADD132PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 97 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm1 and ymm3/mem,subtract/add elements in ymm2 and put result in ymm1."
"VFMSUBADD213PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 A7 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm1 and ymm2,subtract/add elements in ymm3/mem and put result in ymm1."
"VFMSUBADD231PD ymm1,ymm2,ymm3/m256",VEX.DDS.256.66.0F38.W1 B7 /r,Valid,Valid,Invalid,FMA,"ModRM:reg (r, w)",VEX.vvvv (r),ModRM:r/m (r),NA,,"Multiply packed double-precision floating-point values from ymm2 and ymm3/mem,subtract/add elements in ymm1 and put result in ymm1."
"VFMSUBADD132PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 97 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst,subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUBADD213PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 A7 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm1 and xmm2,subtract/add elements in xmm3/m128/m64bcst and put result in xmm1 subject to writemask k1."
"VFMSUBADD231PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 B7 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst,subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUBADD132PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 97 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst,subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUBADD213PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 A7 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm1 and ymm2,subtract/add elements in ymm3/m256/m64bcst and put result in ymm1 subject to writemask k1."
"VFMSUBADD231PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 B7 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst,subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUBADD132PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 97 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst,subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUBADD213PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 A7 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm1 and zmm2,subtract/add elements in zmm3/m512/m64bcst and put result in zmm1 subject to writemask k1."
"VFMSUBADD231PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst{er}",EVEX.DDS.512.66.0F38.W1 B7 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst,subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1."
"VFMSUBADD132PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 97 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, subtract/add elements in xmm2 and put result in xmm1."
"VFMSUBADD213PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 A7 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/mem and put result in xmm1."
"VFMSUBADD231PS xmm1, xmm2, xmm3/m128","VEX.DDS.128.66.0F38.W0 B7 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, subtract/add elements in xmm1 and put result in xmm1."
"VFMSUBADD132PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 97 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, subtract/add elements in ymm2 and put result in ymm1."
"VFMSUBADD213PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 A7 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/mem and put result in ymm1."
"VFMSUBADD231PS ymm1, ymm2, ymm3/m256","VEX.DDS.256.66.0F38.W0 B7 /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, subtract/add elements in ymm1 and put result in ymm1."
"VFMSUBADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, subtract/add elements in xmm2 and put result in xmm1 subject to writemask k1."
"VFMSUBADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, subtract/add elements in xmm3/m128/m32bcst and put result in xmm1 subject to writemask k1."
"VFMSUBADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, subtract/add elements in xmm1 and put result in xmm1 subject to writemask k1."
"VFMSUBADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, subtract/add elements in ymm2 and put result in ymm1 subject to writemask k1."
"VFMSUBADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, subtract/add elements in ymm3/m256/m32bcst and put result in ymm1 subject to writemask k1."
"VFMSUBADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, subtract/add elements in ymm1 and put result in ymm1 subject to writemask k1."
"VFMSUBADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 97 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, subtract/add elements in zmm2 and put result in zmm1 subject to writemask k1."
"VFMSUBADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 A7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, subtract/add elements in zmm3/m512/m32bcst and put result in zmm1 subject to writemask k1."
"VFMSUBADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.DDS.512.66.0F38.W0 B7 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, subtract/add elements in zmm1 and put result in zmm1 subject to writemask k1.VFMSUBADD132PS/VFMSUBADD213PS/VFMSUBADD231PSâ€”Fused Multiply-Alternating Subtract/Add of Packed Single-Precision"
"VFNMADD132PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 9C /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 AC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1."
"VFNMADD231PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 BC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 9C /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD213PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 AC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1."
"VFNMADD231PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 BC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD132PD xmm0 {k1}{z}, xmm1, xmm2/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m64bcst and put result in xmm1."
"VFNMADD231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m64bcst and put result in ymm1."
"VFNMADD231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 9C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm2 and put result in zmm1."
"VFNMADD213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 AC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m64bcst and put result in zmm1."
"VFNMADD231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 BC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and add to zmm1 and put result in zmm1.VFNMADD132PD/VFNMADD213PD/VFNMADD231PDâ€”Fused Negative Multiply-Add of Packed Double-Precision Floating-Point Values"
"VFNMADD132PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 9C /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 AC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1."
"VFNMADD231PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 BC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 9C /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD213PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 AC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/mem and put result in ymm1."
"VFNMADD231PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 BC /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and add to xmm3/m128/m32bcst and put result in xmm1."
"VFNMADD231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm2 and put result in ymm1."
"VFNMADD213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and add to ymm3/m256/m32bcst and put result in ymm1."
"VFNMADD231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result and add to ymm1 and put result in ymm1."
"VFNMADD132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 9C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm2 and put result in zmm1."
"VFNMADD213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 AC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and add to zmm3/m512/m32bcst and put result in zmm1."
"VFNMADD231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 BC /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result and add to zmm1 and put result in zmm1.VFNMADD132PS/VFNMADD213PS/VFNMADD231PSâ€”Fused Negative Multiply-Add of Packed Single-Precision Floating-Point Values"
"VFNMADD132SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 9D /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm3/mem, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 AD /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/mem and put result in xmm1."
"VFNMADD231SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 BD /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm2 and xmm3/mem, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 9D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 AD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m64 and put result in xmm1."
"VFNMADD231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 BD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 9D /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 AD /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1."
"VFNMADD231SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 BD /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMADD132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 9D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and add to xmm2 and put result in xmm1."
"VFNMADD213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 AD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and add to xmm3/m32 and put result in xmm1."
"VFNMADD231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 BD /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and add to xmm1 and put result in xmm1."
"VFNMSUB132PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 9E /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 AE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1."
"VFNMSUB231PD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 BE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 9E /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB213PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 AE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1."
"VFNMSUB231PD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 BE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed double-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1."
"VFNMSUB132PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m64bcst and put result in xmm1."
"VFNMSUB231PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from xmm2 and xmm3/m128/m64bcst, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB213PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m64bcst and put result in ymm1."
"VFNMSUB231PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from ymm2 and ymm3/m256/m64bcst, negate the multiplication result and subtract ymm1 and put result in ymm1."
"VFNMSUB132PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 9E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm2 and put result in zmm1."
"VFNMSUB213PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 AE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m64bcst and put result in zmm1."
"VFNMSUB231PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 BE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed double-precision floating-point values from zmm2 and zmm3/m512/m64bcst, negate the multiplication result and subtract zmm1 and put result in zmm1.VFNMSUB132PD/VFNMSUB213PD/VFNMSUB231PDâ€”Fused Negative Multiply-Subtract of Packed Double-Precision Floating-Point"
"VFNMSUB132PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 9E /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 AE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1."
"VFNMSUB231PS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 BE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 9E /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm3/mem, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB213PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 AE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/mem and put result in ymm1."
"VFNMSUB231PS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 BE /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply packed single-precision floating-point values from ymm2 and ymm3/mem, negate the multiplication result and subtract ymm1 and put result in ymm1."
"VFNMSUB132PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm3/m128/m32bcst, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m128/m32bcst and put result in xmm1."
"VFNMSUB231PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from xmm2 and xmm3/m128/m32bcst, negate the multiplication result subtract add to xmm1 and put result in xmm1."
"VFNMSUB132PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm3/m256/m32bcst, negate the multiplication result and subtract ymm2 and put result in ymm1."
"VFNMSUB213PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm1 and ymm2, negate the multiplication result and subtract ymm3/m256/m32bcst and put result in ymm1."
"VFNMSUB231PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from ymm2 and ymm3/m256/m32bcst, negate the multiplication result subtract add to ymm1 and put result in ymm1."
"VFNMSUB132PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 9E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm3/m512/m32bcst, negate the multiplication result and subtract zmm2 and put result in zmm1."
"VFNMSUB213PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 AE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm1 and zmm2, negate the multiplication result and subtract zmm3/m512/m32bcst and put result in zmm1."
"VFNMSUB231PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 BE /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Multiply packed single-precision floating-point values from zmm2 and zmm3/m512/m32bcst, negate the multiplication result subtract add to zmm1 and put result in zmm1.VFNMSUB132PS/VFNMSUB213PS/VFNMSUB231PSâ€”Fused Negative Multiply-Subtract of Packed Single-Precision Floating-Point Val-"
"VFNMSUB132SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 9F /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm3/mem, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 AF /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/mem and put result in xmm1."
"VFNMSUB231SD xmm1, xmm2, xmm3/m64","VEX.DDS.LIG.66.0F38.W1 BF /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar double-precision floating-point value from xmm2 and xmm3/mem, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 9F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm3/m64, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 AF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m64 and put result in xmm1."
"VFNMSUB231SD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.DDS.LIG.66.0F38.W1 BF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar double-precision floating-point value from xmm2 and xmm3/m64, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 9F /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 AF /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1."
"VFNMSUB231SS xmm1, xmm2, xmm3/m32","VEX.DDS.LIG.66.0F38.W0 BF /r","Valid","Valid","Invalid","FMA","ModRM:reg (r, w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFNMSUB132SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 9F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm3/m32, negate the multiplication result and subtract xmm2 and put result in xmm1."
"VFNMSUB213SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 AF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm1 and xmm2, negate the multiplication result and subtract xmm3/m32 and put result in xmm1."
"VFNMSUB231SS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.DDS.LIG.66.0F38.W0 BF /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Multiply scalar single-precision floating-point value from xmm2 and xmm3/m32, negate the multiplication result and subtract xmm1 and put result in xmm1."
"VFPCLASSPD k2 {k1}, xmm2/m128/m64bcst, imm8","EVEX.128.66.0F3A.W1 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPD k2 {k1}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPD k2 {k1}, zmm2/m512/m64bcst, imm8","EVEX.512.66.0F3A.W1 66 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPS k2 {k1}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPS k2 {k1}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSPS k2 {k1}, zmm2/m512/m32bcst, imm8","EVEX.512.66.0F3A.W0 66 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSSD k2 {k1}, xmm2/m64, imm8","EVEX.LIG.66.0F3A.W1 67 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VFPCLASSSS k2 {k1}, xmm2/m32, imm8","EVEX.LIG.66.0F3A.W0 67 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Tests the input for the following categories: NaN, +0, -0, +Infinity, -Infinity, denormal, finite negative. The immediate field provides a mask bit for each of these category tests. The masked test results are OR-ed together to form a mask result."
"VGATHERDPD xmm1, vm32x, xmm2","VEX.DDS.128.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERQPD xmm1, vm64x, xmm2","VEX.DDS.128.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERDPD ymm1, vm32x, ymm2","VEX.DDS.256.66.0F38.W1 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERQPD ymm1, vm64y, ymm2","VEX.DDS.256.66.0F38.W1 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64y, gather double-pre-cision FP values from memory conditioned on mask speci-fied by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERDPS xmm1 {k1}, vm32x","EVEX.128.66.0F38.W0 92 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERDPS ymm1 {k1}, vm32y","EVEX.256.66.0F38.W0 92 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERDPS zmm1 {k1}, vm32z","EVEX.512.66.0F38.W0 92 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERDPD xmm1 {k1}, vm32x","EVEX.128.66.0F38.W1 92 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask."
"VGATHERDPD ymm1 {k1}, vm32x","EVEX.256.66.0F38.W1 92 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask."
"VGATHERDPD zmm1 {k1}, vm32y","EVEX.512.66.0F38.W1 92 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask."
"VGATHERDPS xmm1, vm32x, xmm2","VEX.DDS.128.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERQPS xmm1, vm64x, xmm2","VEX.DDS.128.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64x, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERDPS ymm1, vm32y, ymm2","VEX.DDS.256.66.0F38.W0 92 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32y, gather single-preci-sion FP values from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VGATHERQPS xmm1, vm64y, xmm2","VEX.DDS.256.66.0F38.W0 93 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64y, gather single-preci-sion FP values from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VGATHERPF0DPS vm32z {k1}","EVEX.512.66.0F38.W0 C6 /1 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T0 hint."
"VGATHERPF0QPS vm64z {k1}","EVEX.512.66.0F38.W0 C7 /1 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T0 hint."
"VGATHERPF0DPD vm32y {k1}","EVEX.512.66.0F38.W1 C6 /1 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T0 hint."
"VGATHERPF0QPD vm64z {k1}","EVEX.512.66.0F38.W1 C7 /1 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T0 hint."
"VGATHERPF1DPS vm32z {k1}","EVEX.512.66.0F38.W0 C6 /2 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T1 hint."
"VGATHERPF1QPS vm64z {k1}","EVEX.512.66.0F38.W0 C7 /2 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using opmask k1 and T1 hint."
"VGATHERPF1DPD vm32y {k1}","EVEX.512.66.0F38.W1 C6 /2 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T1 hint."
"VGATHERPF1QPD vm64z {k1}","EVEX.512.66.0F38.W1 C7 /2 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using opmask k1 and T1 hint."
"VGATHERQPS xmm1 {k1}, vm64x","EVEX.128.66.0F38.W0 93 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPS xmm1 {k1}, vm64y","EVEX.256.66.0F38.W0 93 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPS ymm1 {k1}, vm64z","EVEX.512.66.0F38.W0 93 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather single-precision floating-point values from memory using k1 as completion mask."
"VGATHERQPD xmm1 {k1}, vm64x","EVEX.128.66.0F38.W1 93 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector xmm1 using k1 as completion mask."
"VGATHERQPD ymm1 {k1}, vm64y","EVEX.256.66.0F38.W1 93 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector ymm1 using k1 as completion mask."
"VGATHERQPD zmm1 {k1}, vm64z","EVEX.512.66.0F38.W1 93 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather float64 vector into float64 vector zmm1 using k1 as completion mask."
"VGETEXPPD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}","EVEX.512.66.0F38.W1 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed double-precision floating-point values in the source operand to DP FP results representing unbiased integer exponents and stores the results in the destination under writemask k1."
"VGETEXPPS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}","EVEX.512.66.0F38.W0 42 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Convert the exponent of packed single-precision floating-point values in the source operand to SP FP results representing unbiased integer exponents and stores the results in the destination register."
"VGETEXPSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}","EVEX.NDS.LIG.66.0F38.W1 43 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Convert the biased exponent (bits 62:52) of the low double-precision floating-point value in xmm3/m64 to a DP FP value representing unbiased integer exponent. Stores the result to the low 64-bit of xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETEXPSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}","EVEX.NDS.LIG.66.0F38.W0 43 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Convert the biased exponent (bits 30:23) of the low single-precision floating-point value in xmm3/m32 to a SP FP value representing unbiased integer exponent. Stores the result to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETMANTPD xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.128.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get Normalized Mantissa from float64 vector xmm2/m128/m64bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTPD ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get Normalized Mantissa from float64 vector ymm2/m256/m64bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}, imm8","EVEX.512.66.0F3A.W1 26 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get Normalized Mantissa from float64 vector zmm2/m512/m64bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get normalized mantissa from float32 vector xmm2/m128/m32bcst and store the result in xmm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get normalized mantissa from float32 vector ymm2/m256/m32bcst and store the result in ymm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}, imm8","EVEX.512.66.0F3A.W0 26 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Get normalized mantissa from float32 vector zmm2/m512/m32bcst and store the result in zmm1, using imm8 for sign control and mantissa interval normalization, under writemask."
"VGETMANTSD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W1 27 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Extract the normalized mantissa of the low float64 element in xmm3/m64 using imm8 for sign control and mantissa interval normalization. Store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VGETMANTSS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W0 27 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Extract the normalized mantissa from the low float32 element of xmm3/m32 using imm8 for sign control and mantissa interval normalization, store the mantissa to xmm1 under the writemask k1 and merge with the other elements of xmm2."
"VINSERTF128 ymm1, ymm2, xmm3/m128, imm8","VEX.NDS.256.66.0F3A.W0 18 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Insert 128 bits of packed floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1. EVEX.NDS.256.66.0F3A.W0 18 /r ib VINSERTF32X4 ymm1 {k1}{z}, ymm2, xmm3/m128, imm8 C V/V AVX512VL AVX512F Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1. EVEX.NDS.512.66.0F3A.W0 18 /r ib VINSERTF32X4 zmm1 {k1}{z}, zmm2, xmm3/m128, imm8 C V/V AVX512F Insert 128 bits of packed single-precision floating-point values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.256.66.0F3A.W1 18 /r ib VINSERTF64X2 ymm1 {k1}{z}, ymm2, xmm3/m128, imm8 B V/V AVX512VL AVX512DQ Insert 128 bits of packed double-precision floating-point values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1. EVEX.NDS.512.66.0F3A.W1 18 /r ib VINSERTF64X2 zmm1 {k1}{z}, zmm2, xmm3/m128, imm8 B V/V AVX512DQ Insert 128 bits of packed double-precision floating-point values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.512.66.0F3A.W0 1A /r ib VINSERTF32X8 zmm1 {k1}{z}, zmm2, ymm3/m256, imm8 D V/V AVX512DQ Insert 256 bits of packed single-precision floating-point values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.512.66.0F3A.W1 1A /r ib VINSERTF64X4 zmm1 {k1}{z}, zmm2, ymm3/m256, imm8 C V/V AVX512F Insert 256 bits of packed double-precision floating-point values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1."
"VINSERTI128 ymm1, ymm2, xmm3/m128, imm8","VEX.NDS.256.66.0F3A.W0 38 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","NA","Insert 128 bits of integer data from xmm3/m128 and the remaining values from ymm2 into ymm1. EVEX.NDS.256.66.0F3A.W0 38 /r ib VINSERTI32X4 ymm1 {k1}{z}, ymm2, xmm3/m128, imm8 C V/V AVX512VL AVX512F Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1. EVEX.NDS.512.66.0F3A.W0 38 /r ib VINSERTI32X4 zmm1 {k1}{z}, zmm2, xmm3/m128, imm8 C V/V AVX512F Insert 128 bits of packed doubleword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.256.66.0F3A.W1 38 /r ib VINSERTI64X2 ymm1 {k1}{z}, ymm2, xmm3/m128, imm8 B V/V AVX512VL AVX512DQ Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from ymm2 into ymm1 under writemask k1. EVEX.NDS.512.66.0F3A.W1 38 /r ib VINSERTI64X2 zmm1 {k1}{z}, zmm2, xmm3/m128, imm8 B V/V AVX512DQ Insert 128 bits of packed quadword integer values from xmm3/m128 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.512.66.0F3A.W0 3A /r ib VINSERTI32X8 zmm1 {k1}{z}, zmm2, ymm3/m256, imm8 D V/V AVX512DQ Insert 256 bits of packed doubleword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1. EVEX.NDS.512.66.0F3A.W1 3A /r ib VINSERTI64X4 zmm1 {k1}{z}, zmm2, ymm3/m256, imm8 C V/V AVX512F Insert 256 bits of packed quadword integer values from ymm3/m256 and the remaining values from zmm2 into zmm1 under writemask k1."
"VMASKMOVPS xmm1, xmm2, m128","VEX.NDS.128.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Conditionally load packed single-precision values from m128 using mask in xmm2 and store in xmm1."
"VMASKMOVPS ymm1, ymm2, m256","VEX.NDS.256.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Conditionally load packed single-precision values from m256 using mask in ymm2 and store in ymm1."
"VMASKMOVPD xmm1, xmm2, m128","VEX.NDS.128.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Conditionally load packed double-precision values from m128 using mask in xmm2 and store in xmm1."
"VMASKMOVPD ymm1, ymm2, m256","VEX.NDS.256.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","","Conditionally load packed double-precision values from m256 using mask in ymm2 and store in ymm1."
"VMASKMOVPS m128, xmm1, xmm2","VEX.NDS.128.66.0F38.W0 2E /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","NA","","Conditionally store packed single-precision values from xmm2 using mask in xmm1."
"VMASKMOVPS m256, ymm1, ymm2","VEX.NDS.256.66.0F38.W0 2E /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","NA","","Conditionally store packed single-precision values from ymm2 using mask in ymm1."
"VMASKMOVPD m128, xmm1, xmm2","VEX.NDS.128.66.0F38.W0 2F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","NA","","Conditionally store packed double-precision values from xmm2 using mask in xmm1."
"VMASKMOVPD m256, ymm1, ymm2","VEX.NDS.256.66.0F38.W0 2F /r","Valid","Valid","Invalid","AVX","ModRM:r/m (w)","VEX.vvvv (r)","ModRM:reg (r)","NA","","Conditionally store packed double-precision values from ymm2 using mask in ymm1."
"VPBLENDD xmm1, xmm2, xmm3/m128, imm8","VEX.NDS.128.66.0F3A.W0 02 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","","Select dwords from xmm2 and xmm3/m128 from mask specified in imm8 and store the values into xmm1."
"VPBLENDD ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.W0 02 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","","Select dwords from ymm2 and ymm3/m256 from mask specified in imm8 and store the values into ymm1."
"VPBLENDMB xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend byte integer vector xmm2 and byte vector xmm3/m128 and store the result in xmm1, under control mask."
"VPBLENDMB ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend byte integer vector ymm2 and byte vector ymm3/m256 and store the result in ymm1, under control mask."
"VPBLENDMB zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W0 66 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend byte integer vector zmm2 and byte vector zmm3/m512 and store the result in zmm1, under control mask."
"VPBLENDMW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend word integer vector xmm2 and word vector xmm3/m128 and store the result in xmm1, under control mask."
"VPBLENDMW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend word integer vector ymm2 and word vector ymm3/m256 and store the result in ymm1, under control mask."
"VPBLENDMW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 66 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Blend word integer vector zmm2 and word vector zmm3/m512 and store the result in zmm1, under control mask."
"VPBLENDMD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend doubleword integer vector xmm2 and doubleword vector xmm3/m128/m32bcst and store the result in xmm1, under control mask."
"VPBLENDMD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend doubleword integer vector ymm2 and doubleword vector ymm3/m256/m32bcst and store the result in ymm1, under control mask."
"VPBLENDMD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 64 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend doubleword integer vector zmm2 and doubleword vector zmm3/m512/m32bcst and store the result in zmm1, under control mask."
"VPBLENDMQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend quadword integer vector xmm2 and quadword vector xmm3/m128/m64bcst and store the result in xmm1, under control mask."
"VPBLENDMQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend quadword integer vector ymm2 and quadword vector ymm3/m256/m64bcst and store the result in ymm1, under control mask."
"VPBLENDMQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 64 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Blend quadword integer vector zmm2 and quadword vector zmm3/m512/m64bcst and store the result in zmm1, under control mask."
"VPBROADCASTB xmm1,xmm2/m8",VEX.128.66.0F38.W0 78 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a byte integer in the source operand to sixteen locations in xmm1.
"VPBROADCASTB ymm1,xmm2/m8",VEX.256.66.0F38.W0 78 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a byte integer in the source operand to thirty-two locations in ymm1.
"VPBROADCASTB xmm1{k1}{z},xmm2/m8",EVEX.128.66.0F38.W0 78 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a byte integer in the source operand to locations in xmm1 subject to writemask k1.
"VPBROADCASTB ymm1{k1}{z},xmm2/m8",EVEX.256.66.0F38.W0 78 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a byte integer in the source operand to locations in ymm1 subject to writemask k1.
"VPBROADCASTB zmm1{k1}{z},xmm2/m8",EVEX.512.66.0F38.W0 78 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a byte integer in the source operand to 64 locations in zmm1 subject to writemask k1.
"VPBROADCASTW xmm1,xmm2/m16",VEX.128.66.0F38.W0 79 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a word integer in the source operand to eight locations in xmm1.
"VPBROADCASTW ymm1,xmm2/m16",VEX.256.66.0F38.W0 79 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a word integer in the source operand to sixteen locations in ymm1.
"VPBROADCASTW xmm1{k1}{z},xmm2/m16",EVEX.128.66.0F38.W0 79 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a word integer in the source operand to locations in xmm1 subject to writemask k1.
"VPBROADCASTW ymm1{k1}{z},xmm2/m16",EVEX.256.66.0F38.W0 79 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a word integer in the source operand to locations in ymm1 subject to writemask k1.
"VPBROADCASTW zmm1{k1}{z},xmm2/m16",EVEX.512.66.0F38.W0 79 /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a word integer in the source operand to 32 locations in zmm1 subject to writemask k1.
"VPBROADCASTD xmm1,xmm2/m32",VEX.128.66.0F38.W0 58 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a dword integer in the source operand to four locations in xmm1.
"VPBROADCASTD ymm1,xmm2/m32",VEX.256.66.0F38.W0 58 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a dword integer in the source operand to eight locations in ymm1.
"VPBROADCASTD xmm1 {k1}{z},xmm2/m32",EVEX.128.66.0F38.W0 58 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a dword integer in the source operand to locations in xmm1 subject to writemask k1.
"VPBROADCASTD ymm1 {k1}{z},xmm2/m32",EVEX.256.66.0F38.W0 58 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a dword integer in the source operand to locations in ymm1 subject to writemask k1.
"VPBROADCASTD zmm1 {k1}{z},xmm2/m32",EVEX.512.66.0F38.W0 58 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a dword integer in the source operand to locations in zmm1 subject to writemask k1.
"VPBROADCASTQ xmm1,xmm2/m64",VEX.128.66.0F38.W0 59 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a qword element in source operand to two locations in xmm1.
"VPBROADCASTQ ymm1,xmm2/m64",VEX.256.66.0F38.W0 59 /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast a qword element in source operand to four locations in ymm1.
"VPBROADCASTQ xmm1 {k1}{z},xmm2/m64",EVEX.128.66.0F38.W1 59 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a qword element in source operand to locations in xmm1 subject to writemask k1.
"VPBROADCASTQ ymm1 {k1}{z},xmm2/m64",EVEX.256.66.0F38.W1 59 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a qword element in source operand to locations in ymm1 subject to writemask k1.
"VPBROADCASTQ zmm1 {k1}{z},xmm2/m64",EVEX.512.66.0F38.W1 59 /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a qword element in source operand to locations in zmm1 subject to writemask k1.
"VBROADCASTI32x2 xmm1 {k1}{z},xmm2/m64",EVEX.128.66.0F38.W0 59 /r,Valid,Valid,Invalid,AVX512VL AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple2,Broadcast two dword elements in source operand to locations in xmm1 subject to writemask k1.
"VBROADCASTI32x2 ymm1 {k1}{z},xmm2/m64",EVEX.256.66.0F38.W0 59 /r,Valid,Valid,Invalid,AVX512VL AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple2,Broadcast two dword elements in source operand to locations in ymm1 subject to writemask k1.
"VBROADCASTI32x2 zmm1 {k1}{z},xmm2/m64",EVEX.512.66.0F38.W0 59 /r,Valid,Valid,Invalid,AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple2,Broadcast two dword elements in source operand to locations in zmm1 subject to writemask k1.
"VBROADCASTI128 ymm1,m128",VEX.256.66.0F38.W0 5A /r,Valid,Valid,Invalid,AVX2,ModRM:reg (w),ModRM:r/m (r),NA,NA,,Broadcast 128 bits of integer data in mem to low and high 128-bits in ymm1.
"VBROADCASTI32X4 ymm1 {k1}{z},m128",EVEX.256.66.0F38.W0 5A /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple4,Broadcast 128 bits of 4 doubleword integer data in mem to locations in ymm1 using writemask k1.
"VBROADCASTI32X4 zmm1 {k1}{z},m128",EVEX.512.66.0F38.W0 5A /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple4,Broadcast 128 bits of 4 doubleword integer data in mem to locations in zmm1 using writemask k1.
"VBROADCASTI64X2 ymm1 {k1}{z},m128",EVEX.256.66.0F38.W1 5A /r,Valid,Valid,Invalid,AVX512VL AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple2,Broadcast 128 bits of 2 quadword integer data in mem to locations in ymm1 using writemask k1.
"VBROADCASTI64X2 zmm1 {k1}{z},m128",EVEX.512.66.0F38.W1 5A /r,Valid,Valid,Invalid,AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple2,Broadcast 128 bits of 2 quadword integer data in mem to locations in zmm1 using writemask k1.
"VBROADCASTI32X8 zmm1 {k1}{z},m256",EVEX.512.66.0F38.W0 5B /r,Valid,Valid,Invalid,AVX512DQ,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple8,Broadcast 256 bits of 8 doubleword integer data in mem to locations in zmm1 using writemask k1.
"VBROADCASTI64X4 zmm1 {k1}{z},m256",EVEX.512.66.0F38.W1 5B /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple4,Broadcast 256 bits of 4 quadword integer data in mem to locations in zmm1 using writemask k1.
"VPBROADCASTB xmm1 {k1}{z},reg",EVEX.128.66.0F38.W0 7A /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast an 8-bit value from a GPR to all bytes in the 128-bit destination subject to writemask k1.
"VPBROADCASTB ymm1 {k1}{z},reg",EVEX.256.66.0F38.W0 7A /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast an 8-bit value from a GPR to all bytes in the 256-bit destination subject to writemask k1.
"VPBROADCASTB zmm1 {k1}{z},reg",EVEX.512.66.0F38.W0 7A /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast an 8-bit value from a GPR to all bytes in the 512-bit destination subject to writemask k1.
"VPBROADCASTW xmm1 {k1}{z},reg",EVEX.128.66.0F38.W0 7B /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 16-bit value from a GPR to all words in the 128-bit destination subject to writemask k1.
"VPBROADCASTW ymm1 {k1}{z},reg",EVEX.256.66.0F38.W0 7B /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 16-bit value from a GPR to all words in the 256-bit destination subject to writemask k1.
"VPBROADCASTW zmm1 {k1}{z},reg",EVEX.512.66.0F38.W0 7B /r,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 16-bit value from a GPR to all words in the 512-bit destination subject to writemask k1.
"VPBROADCASTD xmm1 {k1}{z},r32",EVEX.128.66.0F38.W0 7C /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 32-bit value from a GPR to all double-words in the 128-bit destination subject to writemask k1.
"VPBROADCASTD ymm1 {k1}{z},r32",EVEX.256.66.0F38.W0 7C /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 32-bit value from a GPR to all double-words in the 256-bit destination subject to writemask k1.
"VPBROADCASTD zmm1 {k1}{z},r32",EVEX.512.66.0F38.W0 7C /r,Valid,Valid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 32-bit value from a GPR to all double-words in the 512-bit destination subject to writemask k1.
"VPBROADCASTQ xmm1 {k1}{z},r64",EVEX.128.66.0F38.W1 7C /r,Valid,Invalid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 64-bit value from a GPR to all quad-words in the 128-bit destination subject to writemask k1.
"VPBROADCASTQ ymm1 {k1}{z},r64",EVEX.256.66.0F38.W1 7C /r,Valid,Invalid,Invalid,AVX512VL AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 64-bit value from a GPR to all quad-words in the 256-bit destination subject to writemask k1.
"VPBROADCASTQ zmm1 {k1}{z},r64",EVEX.512.66.0F38.W1 7C /r,Valid,Invalid,Invalid,AVX512F,ModRM:reg (w),ModRM:r/m (r),NA,NA,Tuple1 Scalar,Broadcast a 64-bit value from a GPR to all quad-words in the 512-bit destination subject to writemask k1.
"VPBROADCASTMB2Q xmm1, k1","EVEX.128.F3.0F38.W1 2A /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low byte value in k1 to two locations in xmm1."
"VPBROADCASTMB2Q ymm1, k1","EVEX.256.F3.0F38.W1 2A /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low byte value in k1 to four locations in ymm1."
"VPBROADCASTMB2Q zmm1, k1","EVEX.512.F3.0F38.W1 2A /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low byte value in k1 to eight locations in zmm1."
"VPBROADCASTMW2D xmm1, k1","EVEX.128.F3.0F38.W0 3A /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low word value in k1 to four locations in xmm1."
"VPBROADCASTMW2D ymm1, k1","EVEX.256.F3.0F38.W0 3A /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low word value in k1 to eight locations in ymm1."
"VPBROADCASTMW2D zmm1, k1","EVEX.512.F3.0F38.W0 3A /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Broadcast low word value in k1 to sixteen locations in zmm1."
"VPCMPB k1 {k2}, xmm2, xmm3/m128, imm8","EVEX.NDS.128.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPB k1 {k2}, ymm2, ymm3/m256, imm8","EVEX.NDS.256.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPB k1 {k2}, zmm2, zmm3/m512, imm8","EVEX.NDS.512.66.0F3A.W0 3F /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed signed byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUB k1 {k2}, xmm2, xmm3/m128, imm8","EVEX.NDS.128.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte values in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUB k1 {k2}, ymm2, ymm3/m256, imm8","EVEX.NDS.256.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte values in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUB k1 {k2}, zmm2, zmm3/m512, imm8","EVEX.NDS.512.66.0F3A.W0 3E /r ib","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Compare packed unsigned byte values in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPD k1 {k2}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPD k1 {k2}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPD k1 {k2}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.66.0F3A.W0 1F /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2."
"VPCMPUD k1 {k2}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned doubleword integer values in xmm3/m128/m32bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUD k1 {k2}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned doubleword integer values in ymm3/m256/m32bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUD k1 {k2}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.66.0F3A.W0 1E /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned doubleword integer values in zmm2 and zmm3/m512/m32bcst using bits 2:0 of imm8 as a comparison predicate. The comparison results are written to the destination k1 under writemask k2."
"VPCMPQ k1 {k2}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPQ k1 {k2}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPQ k1 {k2}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F3A.W1 1F /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed signed quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUQ k1 {k2}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned quadword integer values in xmm3/m128/m64bcst and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUQ k1 {k2}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned quadword integer values in ymm3/m256/m64bcst and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPUQ k1 {k2}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F3A.W1 1E /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Compare packed unsigned quadword integer values in zmm3/m512/m64bcst and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1."
"VPCMPW k1 {k2}, xmm2, xmm3/m128, imm8",EVEX.NDS.128.66.0F3A.W1 3F /r ib,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed signed word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCMPW k1 {k2}, ymm2, ymm3/m256, imm8",EVEX.NDS.256.66.0F3A.W1 3F /r ib,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed signed word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCMPW k1 {k2}, zmm2, zmm3/m512, imm8",EVEX.NDS.512.66.0F3A.W1 3F /r ib,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed signed word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCMPUW k1 {k2}, xmm2, xmm3/m128, imm8",EVEX.NDS.128.66.0F3A.W1 3E /r ib,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed unsigned word integers in xmm3/m128 and xmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCMPUW k1 {k2}, ymm2, ymm3/m256, imm8",EVEX.NDS.256.66.0F3A.W1 3E /r ib,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed unsigned word integers in ymm3/m256 and ymm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCMPUW k1 {k2}, zmm2, zmm3/m512, imm8",EVEX.NDS.512.66.0F3A.W1 3E /r ib,Valid,Valid,Invalid,AVX512BW,ModRM:reg (w),vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Compare packed unsigned word integers in zmm3/m512 and zmm2 using bits 2:0 of imm8 as a comparison predicate with writemask k2 and leave the result in mask register k1.
"VPCOMPRESSD xmm1/m128 {k1}{z}, xmm2","EVEX.128.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed doubleword integer values from xmm2 to xmm1/m128 using controlmask k1."
"VPCOMPRESSD ymm1/m256 {k1}{z}, ymm2","EVEX.256.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed doubleword integer values from ymm2 to ymm1/m256 using controlmask k1."
"VPCOMPRESSD zmm1/m512 {k1}{z}, zmm2","EVEX.512.66.0F38.W0 8B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed doubleword integer values from zmm2 to zmm1/m512 using controlmask k1."
"VPCOMPRESSQ xmm1/m128 {k1}{z}, xmm2","EVEX.128.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed quadword integer values from xmm2 to xmm1/m128 using controlmask k1."
"VPCOMPRESSQ ymm1/m256 {k1}{z}, ymm2","EVEX.256.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed quadword integer values from ymm2 to ymm1/m256 using controlmask k1."
"VPCOMPRESSQ zmm1/m512 {k1}{z}, zmm2","EVEX.512.66.0F38.W1 8B /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Tuple1 Scalar","Compress packed quadword integer values from zmm2 to zmm1/m512 using controlmask k1."
"VPCONFLICTD xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 C4 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate double-word values in xmm2/m128/m32bcst using writemask k1."
"VPCONFLICTD ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 C4 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate double-word values in ymm2/m256/m32bcst using writemask k1."
"VPCONFLICTD zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 C4 /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate double-word values in zmm2/m512/m32bcst using writemask k1."
"VPCONFLICTQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 C4 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate quad-word values in xmm2/m128/m64bcst using writemask k1."
"VPCONFLICTQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 C4 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate quad-word values in ymm2/m256/m64bcst using writemask k1."
"VPCONFLICTQ zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 C4 /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Detect duplicate quad-word values in zmm2/m512/m64bcst using writemask k1."
"VPERM2F128 ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.W0 06 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","imm8","","Permute 128-bit floating-point fields in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1."
"VPERM2I128 ymm1, ymm2, ymm3/m256, imm8","VEX.NDS.256.66.0F3A.W0 46 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","Imm8","","Permute 128-bit integer data in ymm2 and ymm3/mem using controls from imm8 and store result in ymm1."
"VPERMD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Permute doublewords in ymm3/m256 using indices in ymm2 and store the result in ymm1."
"VPERMD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Permute doublewords in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 36 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Permute doublewords in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Permute word integers in xmm3/m128 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Permute word integers in ymm3/m256 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 8D /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","Full Vector Mem","Permute word integers in zmm3/m512 using indexes in zmm2 and store the result in zmm1 using writemask k1."
"VPERMI2W xmm1 {k1}{z},xmm2,xmm3/m128",EVEX.DDS.128.66.0F38.W1 75 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Permute word integers from two tables in xmm3/m128 and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
"VPERMI2W ymm1 {k1}{z},ymm2,ymm3/m256",EVEX.DDS.256.66.0F38.W1 75 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Permute word integers from two tables in ymm3/m256 and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
"VPERMI2W zmm1 {k1}{z},zmm2,zmm3/m512",EVEX.DDS.512.66.0F38.W1 75 /r,Valid,Valid,Invalid,AVX512BW,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,Permute word integers from two tables in zmm3/m512 and zmm2 using indexes in zmm1 and store the result in zmm1 using writemask k1.
"VPERMI2D xmm1 {k1}{z},xmm2,xmm3/m128/m32bcst",EVEX.DDS.128.66.0F38.W0 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-words from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
"VPERMI2D ymm1 {k1}{z},ymm2,ymm3/m256/m32bcst",EVEX.DDS.256.66.0F38.W0 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-words from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
"VPERMI2D zmm1 {k1}{z},zmm2,zmm3/m512/m32bcst",EVEX.DDS.512.66.0F38.W0 76 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-words from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
"VPERMI2Q xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute quad-words from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
"VPERMI2Q ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 76 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute quad-words from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
"VPERMI2Q zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst",EVEX.DDS.512.66.0F38.W1 76 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute quad-words from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
"VPERMI2PS xmm1 {k1}{z},xmm2,xmm3/m128/m32bcst",EVEX.DDS.128.66.0F38.W0 77 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute single-precision FP values from two tables in xmm3/m128/m32bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
"VPERMI2PS ymm1 {k1}{z},ymm2,ymm3/m256/m32bcst",EVEX.DDS.256.66.0F38.W0 77 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute single-precision FP values from two tables in ymm3/m256/m32bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
"VPERMI2PS zmm1 {k1}{z},zmm2,zmm3/m512/m32bcst",EVEX.DDS.512.66.0F38.W0 77 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute single-precision FP values from two tables in zmm3/m512/m32bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
"VPERMI2PD xmm1 {k1}{z},xmm2,xmm3/m128/m64bcst",EVEX.DDS.128.66.0F38.W1 77 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-precision FP values from two tables in xmm3/m128/m64bcst and xmm2 using indexes in xmm1 and store the result in xmm1 using writemask k1.
"VPERMI2PD ymm1 {k1}{z},ymm2,ymm3/m256/m64bcst",EVEX.DDS.256.66.0F38.W1 77 /r,Valid,Valid,Invalid,AVX512VL AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-precision FP values from two tables in ymm3/m256/m64bcst and ymm2 using indexes in ymm1 and store the result in ymm1 using writemask k1.
"VPERMI2PD zmm1 {k1}{z},zmm2,zmm3/m512/m64bcst",EVEX.DDS.512.66.0F38.W1 77 /r,Valid,Valid,Invalid,AVX512F,"ModRM:reg (r, w)",EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,Permute double-precision FP values from two tables in zmm3/m512/m64bcst and zmm2 using indices in zmm1 and store the result in zmm1 using writemask k1.
"VPERMILPD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 0D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Permute double-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1."
"VPERMILPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 0D /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Permute double-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1."
"VPERMILPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute double-precision floating-point values in xmm2 using control from xmm3/m128/m64bcst and store the result in xmm1 using writemask k1."
"VPERMILPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute double-precision floating-point values in ymm2 using control from ymm3/m256/m64bcst and store the result in ymm1 using writemask k1."
"VPERMILPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 0D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute double-precision floating-point values in zmm2 using control from zmm3/m512/m64bcst and store the result in zmm1 using writemask k1."
"VPERMILPD xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.W0 05 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Permute double-precision floating-point values in xmm2/m128 using controls from imm8."
"VPERMILPD ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.W0 05 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Permute double-precision floating-point values in ymm2/m256 using controls from imm8."
"VPERMILPD xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.128.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute double-precision floating-point values in xmm2/m128/m64bcst using controls from imm8 and store the result in xmm1 using writemask k1."
"VPERMILPD ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute double-precision floating-point values in ymm2/m256/m64bcst using controls from imm8 and store the result in ymm1 using writemask k1."
"VPERMILPD zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8","EVEX.512.66.0F3A.W1 05 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute double-precision floating-point values in zmm2/m512/m64bcst using controls from imm8 and store the result in zmm1 using writemask k1."
"VPERMILPS xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Permute single-precision floating-point values in xmm2 using controls from xmm3/m128 and store result in xmm1."
"VPERMILPS xmm1, xmm2/m128, imm8","VEX.128.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Permute single-precision floating-point values in xmm2/m128 using controls from imm8 and store result in xmm1."
"VPERMILPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Permute single-precision floating-point values in ymm2 using controls from ymm3/m256 and store result in ymm1."
"VPERMILPS ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","NA","Permute single-precision floating-point values in ymm2/m256 using controls from imm8 and store result in ymm1."
"VPERMILPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute single-precision floating-point values xmm2 using control from xmm3/m128/m32bcst and store the result in xmm1 using writemask k1."
"VPERMILPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute single-precision floating-point values ymm2 using control from ymm3/m256/m32bcst and store the result in ymm1 using writemask k1."
"VPERMILPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 0C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute single-precision floating-point values zmm2 using control from zmm3/m512/m32bcst and store the result in zmm1 using writemask k1."
"VPERMILPS xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute single-precision floating-point values xmm2/m128/m32bcst using controls from imm8 and store the result in xmm1 using writemask k1."
"VPERMILPS ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute single-precision floating-point values ymm2/m256/m32bcst using controls from imm8 and store the result in ymm1 using writemask k1."
"VPERMILPS zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.512.66.0F3A.W0 04 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Permute single-precision floating-point values zmm2/m512/m32bcst using controls from imm8 and store the result in zmm1 using writemask k1."
"VPERMPD ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","NA","Permute double-precision floating-point elements in ymm2/m256 using indices in imm8 and store the result in ymm1."
"VPERMPD ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Permute double-precision floating-point elements in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1 subject to writemask k1."
"VPERMPD zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8","EVEX.512.66.0F3A.W1 01 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Permute double-precision floating-point elements in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1 subject to writemask k1."
"VPERMPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute double-precision floating-point elements in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1 subject to writemask k1."
"VPERMPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute double-precision floating-point elements in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1 subject to writemask k1."
"VPERMPS ymm1, ymm2, ymm3/m256","VEX.256.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Permute single-precision floating-point elements in ymm3/m256 using indices in ymm2 and store the result in ymm1."
"VPERMPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute single-precision floating-point elements in ymm3/m256/m32bcst using indexes in ymm2 and store the result in ymm1 subject to write mask k1."
"VPERMPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 16 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute single-precision floating-point values in zmm3/m512/m32bcst using indices in zmm2 and store the result in zmm1 subject to write mask k1."
"VPERMQ ymm1, ymm2/m256, imm8","VEX.256.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","NA","Permute qwords in ymm2/m256 using indices in imm8 and store the result in ymm1."
"VPERMQ ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Permute qwords in ymm2/m256/m64bcst using indexes in imm8 and store the result in ymm1."
"VPERMQ zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8","EVEX.512.66.0F3A.W1 00 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Permute qwords in zmm2/m512/m64bcst using indices in imm8 and store the result in zmm1."
"VPERMQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 36 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute qwords in ymm3/m256/m64bcst using indexes in ymm2 and store the result in ymm1."
"VPERMQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 36 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Permute qwords in zmm3/m512/m64bcst using indices in zmm2 and store the result in zmm1."
"VPERMT2W xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.DDS.128.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r,w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Mem","Permute word integers from two tables in xmm3/m128 and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2W ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.DDS.256.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (r,w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Mem","Permute word integers from two tables in ymm3/m256 and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2W zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.DDS.512.66.0F38.W1 7D /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (r,w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Mem","Permute word integers from two tables in zmm3/m512 and zmm1 using indexes in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2D xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-words from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2D ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-words from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2D zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.DDS.512.66.0F38.W0 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-words from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2Q xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.DDS.128.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute quad-words from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2Q ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.DDS.256.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute quad-words from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2Q zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.DDS.512.66.0F38.W1 7E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute quad-words from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2PS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.DDS.128.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute single-precision FP values from two tables in xmm3/m128/m32bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2PS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.DDS.256.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute single-precision FP values from two tables in ymm3/m256/m32bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2PS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.DDS.512.66.0F38.W0 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute single-precision FP values from two tables in zmm3/m512/m32bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1."
"VPERMT2PD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.DDS.128.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-precision FP values from two tables in xmm3/m128/m64bcst and xmm1 using indexes in xmm2 and store the result in xmm1 using writemask k1."
"VPERMT2PD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.DDS.256.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-precision FP values from two tables in ymm3/m256/m64bcst and ymm1 using indexes in ymm2 and store the result in ymm1 using writemask k1."
"VPERMT2PD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.DDS.512.66.0F38.W1 7F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full","Permute double-precision FP values from two tables in zmm3/m512/m64bcst and zmm1 using indices in zmm2 and store the result in zmm1 using writemask k1.VPERMT2W/D/Q/PS/PDâ€”Full Permute from Two Tables Overwriting one Table"
"VPEXPANDD xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-word integer values from xmm2/m128 to xmm1 using writemask k1."
"VPEXPANDD ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-word integer values from ymm2/m256 to ymm1 using writemask k1."
"VPEXPANDD zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.W0 89 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed double-word integer values from zmm2/m512 to zmm1 using writemask k1."
"VPEXPANDQ xmm1 {k1}{z}, xmm2/m128","EVEX.128.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed quad-word integer values from xmm2/m128 to xmm1 using writemask k1."
"VPEXPANDQ ymm1 {k1}{z}, ymm2/m256","EVEX.256.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed quad-word integer values from ymm2/m256 to ymm1 using writemask k1."
"VPEXPANDQ zmm1 {k1}{z}, zmm2/m512","EVEX.512.66.0F38.W1 89 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Tuple1 Scalar","Expand packed quad-word integer values from zmm2/m512 to zmm1 using writemask k1."
"VPGATHERDD xmm1 {k1}, vm32x","EVEX.128.66.0F38.W0 90 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDD ymm1 {k1}, vm32y","EVEX.256.66.0F38.W0 90 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDD zmm1 {k1}, vm32z","EVEX.512.66.0F38.W0 90 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ xmm1 {k1}, vm32x","EVEX.128.66.0F38.W1 90 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ ymm1 {k1}, vm32x","EVEX.256.66.0F38.W1 90 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERDQ zmm1 {k1}, vm32y","EVEX.512.66.0F38.W1 90 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed dword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERDD xmm1, vm32x, xmm2","VEX.DDS.128.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERQD xmm1, vm64x, xmm2","VEX.DDS.128.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64x, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERDD ymm1, vm32y, ymm2","VEX.DDS.256.66.0F38.W0 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32y, gather dword from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPGATHERQD xmm1, vm64y, xmm2","VEX.DDS.256.66.0F38.W0 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64y, gather dword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERDQ xmm1, vm32x, xmm2","VEX.DDS.128.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather qword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERQQ xmm1, vm64x, xmm2","VEX.DDS.128.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64x, gather qword val-ues from memory conditioned on mask specified by xmm2. Conditionally gathered elements are merged into xmm1."
"VPGATHERDQ ymm1, vm32x, ymm2","VEX.DDS.256.66.0F38.W1 90 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using dword indices specified in vm32x, gather qword val-ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPGATHERQQ ymm1, vm64y, ymm2","VEX.DDS.256.66.0F38.W1 91 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (r,w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","VEX.vvvv (r, w)","NA","","Using qword indices specified in vm64y, gather qword val-ues from memory conditioned on mask specified by ymm2. Conditionally gathered elements are merged into ymm1."
"VPGATHERQD xmm1 {k1}, vm64x","EVEX.128.66.0F38.W0 91 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQD xmm1 {k1}, vm64y","EVEX.256.66.0F38.W0 91 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQD ymm1 {k1}, vm64z","EVEX.512.66.0F38.W0 91 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather dword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ xmm1 {k1}, vm64x","EVEX.128.66.0F38.W1 91 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ ymm1 {k1}, vm64y","EVEX.256.66.0F38.W1 91 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPGATHERQQ zmm1 {k1}, vm64z","EVEX.512.66.0F38.W1 91 /vsib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","Tuple1 Scalar","Using signed qword indices, gather quadword values from memory using writemask k1 for merging-masking."
"VPLZCNTD xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 44 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each dword element of xmm2/m128/m32bcst using writemask k1."
"VPLZCNTD ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 44 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each dword element of ymm2/m256/m32bcst using writemask k1."
"VPLZCNTD zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 44 /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each dword element of zmm2/m512/m32bcst using writemask k1."
"VPLZCNTQ xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 44 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each qword element of xmm2/m128/m64bcst using writemask k1."
"VPLZCNTQ ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 44 /r","Valid","Valid","Invalid","AVX512VL AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each qword element of ymm2/m256/m64bcst using writemask k1."
"VPLZCNTQ zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 44 /r","Valid","Valid","Invalid","AVX512CD","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Count the number of leading zero bits in each qword element of zmm2/m512/m64bcst using writemask k1."
"VPMASKMOVD xmm1, xmm2, m128","VEX.NDS.128.66.0F38.W0 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","","Conditionally load dword values from m128 using mask in xmm2 and store in xmm1."
"VPMASKMOVD ymm1, ymm2, m256","VEX.NDS.256.66.0F38.W0 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","","Conditionally load dword values from m256 using mask in ymm2 and store in ymm1."
"VPMASKMOVQ xmm1, xmm2, m128","VEX.NDS.128.66.0F38.W1 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","","Conditionally load qword values from m128 using mask in xmm2 and store in xmm1."
"VPMASKMOVQ ymm1, ymm2, m256","VEX.NDS.256.66.0F38.W1 8C /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","","Conditionally load qword values from m256 using mask in ymm2 and store in ymm1."
"VPMASKMOVD m128, xmm1, xmm2","VEX.NDS.128.66.0F38.W0 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv","ModRM:reg (r)","NA","","Conditionally store dword values from xmm2 using mask in xmm1."
"VPMASKMOVD m256, ymm1, ymm2","VEX.NDS.256.66.0F38.W0 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv","ModRM:reg (r)","NA","","Conditionally store dword values from ymm2 using mask in ymm1."
"VPMASKMOVQ m128, xmm1, xmm2","VEX.NDS.128.66.0F38.W1 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv","ModRM:reg (r)","NA","","Conditionally store qword values from xmm2 using mask in xmm1."
"VPMASKMOVQ m256, ymm1, ymm2","VEX.NDS.256.66.0F38.W1 8E /r","Valid","Valid","Invalid","AVX2","ModRM:r/m (w)","VEX.vvvv","ModRM:reg (r)","NA","","Conditionally store qword values from ymm2 using mask in ymm1."
"VPMOVB2M k1, xmm1","EVEX.128.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in XMM1."
"VPMOVB2M k1, ymm1","EVEX.256.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in YMM1."
"VPMOVB2M k1, zmm1","EVEX.512.F3.0F38.W0 29 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding byte in ZMM1."
"VPMOVW2M k1, xmm1","EVEX.128.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in XMM1."
"VPMOVW2M k1, ymm1","EVEX.256.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in YMM1."
"VPMOVW2M k1, zmm1","EVEX.512.F3.0F38.W1 29 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding word in ZMM1."
"VPMOVD2M k1, xmm1","EVEX.128.F3.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in XMM1."
"VPMOVD2M k1, ymm1","EVEX.256.F3.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in YMM1."
"VPMOVD2M k1, zmm1","EVEX.512.F3.0F38.W0 39 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding doubleword in ZMM1."
"VPMOVQ2M k1, xmm1","EVEX.128.F3.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in XMM1."
"VPMOVQ2M k1, ymm1","EVEX.256.F3.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in YMM1."
"VPMOVQ2M k1, zmm1","EVEX.512.F3.0F38.W1 39 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each bit in k1 to 1 or 0 based on the value of the most significant bit of the corresponding quadword in ZMM1."
"VPMOVDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed double-word integers from xmm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1."
"VPMOVSDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed signed double-word integers from xmm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVUSDB xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed double-word integers from ymm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1."
"VPMOVSDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed signed double-word integers from ymm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVUSDB xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 31 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 16 packed double-word integers from zmm2 into 16 packed byte integers in xmm1/m128 with truncation under writemask k1."
"VPMOVSDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 21 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 16 packed signed double-word integers from zmm2 into 16 packed signed byte integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVUSDB xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 11 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned byte integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed double-word integers from xmm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1."
"VPMOVSDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed signed double-word integers from xmm2 into 4 packed signed word integers in ymm1/m64 using signed saturation under writemask k1."
"VPMOVUSDW xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed unsigned double-word integers from xmm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed double-word integers from ymm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1."
"VPMOVSDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed signed double-word integers from ymm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVUSDW xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed unsigned double-word integers from ymm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 33 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed double-word integers from zmm2 into 16 packed word integers in ymm1/m256 with truncation under writemask k1."
"VPMOVSDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 23 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed signed double-word integers from zmm2 into 16 packed signed word integers in ymm1/m256 using signed saturation under writemask k1."
"VPMOVUSDW ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 13 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed unsigned double-word integers from zmm2 into 16 packed unsigned word integers in ymm1/m256 using unsigned saturation under writemask k1."
"VPMOVM2B xmm1, k1","EVEX.128.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each byte in XMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2B ymm1, k1","EVEX.256.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each byte in YMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2B zmm1, k1","EVEX.512.F3.0F38.W0 28 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each byte in ZMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2W xmm1, k1","EVEX.128.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each word in XMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2W ymm1, k1","EVEX.256.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each word in YMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2W zmm1, k1","EVEX.512.F3.0F38.W1 28 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each word in ZMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2D xmm1, k1","EVEX.128.F3.0F38.W0 38 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each doubleword in XMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2D ymm1, k1","EVEX.256.F3.0F38.W0 38 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each doubleword in YMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2D zmm1, k1","EVEX.512.F3.0F38.W0 38 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each doubleword in ZMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2Q xmm1, k1","EVEX.128.F3.0F38.W1 38 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each quadword in XMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2Q ymm1, k1","EVEX.256.F3.0F38.W1 38 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each quadword in YMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVM2Q zmm1, k1","EVEX.512.F3.0F38.W1 38 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Sets each quadword in ZMM1 to all 1â€™s or all 0â€™s based on the value of the corresponding bit in k1."
"VPMOVQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed byte integers in xmm1/m16 with truncation under writemask k1."
"VPMOVSQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed byte integers in xmm1/m16 using signed saturation under writemask k1."
"VPMOVUSQB xmm1/m16 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned byte integers in xmm1/m16 using unsigned saturation under writemask k1."
"VPMOVQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed byte integers in xmm1/m32 with truncation under writemask k1."
"VPMOVSQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed byte integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVUSQB xmm1/m32 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned byte integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 32 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed byte integers in xmm1/m64 with truncation under writemask k1."
"VPMOVSQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 22 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed byte integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVUSQB xmm1/m64 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 12 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Oct Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned byte integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVQD xmm1/m128 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed double-word integers in xmm1/m128 with truncation subject to writemask k1."
"VPMOVSQD xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 2 packed signed quad-word integers from xmm2 into 2 packed signed double-word integers in xmm1/m64 using signed saturation subject to writemask k1."
"VPMOVUSQD xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned double-word integers in xmm1/m64 using unsigned saturation subject to writemask k1."
"VPMOVQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed double-word integers in xmm1/m128 with truncation subject to writemask k1."
"VPMOVSQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed double-word integers in xmm1/m128 using signed saturation subject to writemask k1."
"VPMOVUSQD xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned double-word integers in xmm1/m128 using unsigned saturation subject to writemask k1."
"VPMOVQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 35 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed double-word integers in ymm1/m256 with truncation subject to writemask k1."
"VPMOVSQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 25 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed double-word integers in ymm1/m256 using signed saturation subject to writemask k1."
"VPMOVUSQD ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned double-word integers in ymm1/m256 using unsigned saturation subject to writemask k1."
"VPMOVQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 2 packed quad-word integers from xmm2 into 2 packed word integers in xmm1/m32 with truncation under writemask k1."
"VPMOVSQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m32 using signed saturation under writemask k1."
"VPMOVUSQW xmm1/m32 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 2 packed unsigned quad-word integers from xmm2 into 2 packed unsigned word integers in xmm1/m32 using unsigned saturation under writemask k1."
"VPMOVQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed quad-word integers from ymm2 into 4 packed word integers in xmm1/m64 with truncation under writemask k1."
"VPMOVSQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed signed quad-word integers from ymm2 into 4 packed signed word integers in xmm1/m64 using signed saturation under writemask k1."
"VPMOVUSQW xmm1/m64 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 4 packed unsigned quad-word integers from ymm2 into 4 packed unsigned word integers in xmm1/m64 using unsigned saturation under writemask k1."
"VPMOVQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 34 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed quad-word integers from zmm2 into 8 packed word integers in xmm1/m128 with truncation under writemask k1."
"VPMOVSQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 24 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed signed quad-word integers from zmm2 into 8 packed signed word integers in xmm1/m128 using signed saturation under writemask k1."
"VPMOVUSQW xmm1/m128 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Quarter Vector Mem","Converts 8 packed unsigned quad-word integers from zmm2 into 8 packed unsigned word integers in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed word integers from xmm2 into 8 packed bytes in xmm1/m64 with truncation under writemask k1."
"VPMOVSWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed signed word integers from xmm2 into 8 packed signed bytes in xmm1/m64 using signed saturation under writemask k1."
"VPMOVUSWB xmm1/m64 {k1}{z}, xmm2","EVEX.128.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 8 packed unsigned word integers from xmm2 into 8 packed unsigned bytes in 8mm1/m64 using unsigned saturation under writemask k1."
"VPMOVWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed word integers from ymm2 into 16 packed bytes in xmm1/m128 with truncation under writemask k1."
"VPMOVSWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed signed word integers from ymm2 into 16 packed signed bytes in xmm1/m128 using signed saturation under writemask k1."
"VPMOVUSWB xmm1/m128 {k1}{z}, ymm2","EVEX.256.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 16 packed unsigned word integers from ymm2 into 16 packed unsigned bytes in xmm1/m128 using unsigned saturation under writemask k1."
"VPMOVWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 30 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 32 packed word integers from zmm2 into 32 packed bytes in ymm1/m256 with truncation under writemask k1."
"VPMOVSWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 20 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 32 packed signed word integers from zmm2 into 32 packed signed bytes in ymm1/m256 using signed saturation under writemask k1."
"VPMOVUSWB ymm1/m256 {k1}{z}, zmm2","EVEX.512.F3.0F38.W0 10 /r","Valid","Valid","Invalid","AVX512BW","ModRM:r/m (w)","ModRM:reg (r)","NA","NA","Half Vector Mem","Converts 32 packed unsigned word integers from zmm2 into 32 packed unsigned bytes in ymm1/m256 using unsigned saturation under writemask k1."
"VPROLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate doublewords in xmm2 left by count in the corresponding element of xmm3/m128/m32bcst. Result written to xmm1 under writemask k1."
"VPROLD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.NDD.128.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate doublewords in xmm2/m128/m32bcst left by imm8. Result written to xmm1 using writemask k1."
"VPROLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in xmm2 left by count in the corresponding element of xmm3/m128/m64bcst. Result written to xmm1 under writemask k1."
"VPROLQ xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.NDD.128.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in xmm2/m128/m64bcst left by imm8. Result written to xmm1 using writemask k1."
"VPROLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate doublewords in ymm2 left by count in the corresponding element of ymm3/m256/m32bcst. Result written to ymm1 under writemask k1."
"VPROLD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.NDD.256.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate doublewords in ymm2/m256/m32bcst left by imm8. Result written to ymm1 using writemask k1."
"VPROLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in ymm2 left by count in the corresponding element of ymm3/m256/m64bcst. Result written to ymm1 under writemask k1."
"VPROLQ ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.NDD.256.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in ymm2/m256/m64bcst left by imm8. Result written to ymm1 using writemask k1."
"VPROLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate left of doublewords in zmm2 by count in the corresponding element of zmm3/m512/m32bcst. Result written to zmm1 using writemask k1."
"VPROLD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.NDD.512.66.0F.W0 72 /1 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate left of doublewords in zmm3/m512/m32bcst by imm8. Result written to zmm1 using writemask k1."
"VPROLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 15 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in zmm2 left by count in the corresponding element of zmm3/m512/m64bcst. Result written to zmm1under writemask k1."
"VPROLQ zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8","EVEX.NDD.512.66.0F.W1 72 /1 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in zmm2/m512/m64bcst left by imm8. Result written to zmm1 using writemask k1."
"VPRORVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate doublewords in xmm2 right by count in the corresponding element of xmm3/m128/m32bcst, store result using writemask k1."
"VPRORD xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.NDD.128.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate doublewords in xmm2/m128/m32bcst right by imm8, store result using writemask k1."
"VPRORVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in xmm2 right by count in the corresponding element of xmm3/m128/m64bcst, store result using writemask k1."
"VPRORQ xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.NDD.128.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in xmm2/m128/m64bcst right by imm8, store result using writemask k1."
"VPRORVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate doublewords in ymm2 right by count in the corresponding element of ymm3/m256/m32bcst, store using result writemask k1."
"VPRORD ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.NDD.256.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate doublewords in ymm2/m256/m32bcst right by imm8, store result using writemask k1."
"VPRORVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in ymm2 right by count in the corresponding element of ymm3/m256/m64bcst, store result using writemask k1."
"VPRORQ ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.NDD.256.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512VL AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in ymm2/m256/m64bcst right by imm8, store result using writemask k1."
"VPRORVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate doublewords in zmm2 right by count in the corresponding element of zmm3/m512/m32bcst, store result using writemask k1."
"VPRORD zmm1 {k1}{z}, zmm2/m512/m32bcst, imm8","EVEX.NDD.512.66.0F.W0 72 /0 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate doublewords in zmm2/m512/m32bcst right by imm8, store result using writemask k1."
"VPRORVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 14 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Rotate quadwords in zmm2 right by count in the corresponding element of zmm3/m512/m64bcst, store result using writemask k1."
"VPRORQ zmm1 {k1}{z}, zmm2/m512/m64bcst, imm8","EVEX.NDD.512.66.0F.W1 72 /0 ib","Valid","Valid","Invalid","AVX512F","VEX.vvvv (w)","ModRM:r/m (R)","Imm8","NA","Full Vector","Rotate quadwords in zmm2/m512/m64bcst right by imm8, store result using writemask k1."
"VPSCATTERDD vm32x {k1}, xmm1","EVEX.128.66.0F38.W0 A0 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDD vm32y {k1}, ymm1","EVEX.256.66.0F38.W0 A0 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDD vm32z {k1}, zmm1","EVEX.512.66.0F38.W0 A0 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed dword indices, scatter dword values to memory using writemask k1."
"VPSCATTERDQ vm32x {k1}, xmm1","EVEX.128.66.0F38.W1 A0 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERDQ vm32x {k1}, ymm1","EVEX.256.66.0F38.W1 A0 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERDQ vm32y {k1}, zmm1","EVEX.512.66.0F38.W1 A0 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed dword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQD vm64x {k1}, xmm1","EVEX.128.66.0F38.W0 A1 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQD vm64y {k1}, xmm1","EVEX.256.66.0F38.W0 A1 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQD vm64z {k1}, ymm1","EVEX.512.66.0F38.W0 A1 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed qword indices, scatter dword values to memory using writemask k1."
"VPSCATTERQQ vm64x {k1}, xmm1","EVEX.128.66.0F38.W1 A1 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQQ vm64y {k1}, ymm1","EVEX.256.66.0F38.W1 A1 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSCATTERQQ vm64z {k1}, zmm1","EVEX.512.66.0F38.W1 A1 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed qword indices, scatter qword values to memory using writemask k1."
"VPSLLVD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSLLVQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSLLVD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSLLVQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSLLVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in xmm2 left by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1."
"VPSLLVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in ymm2 left by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1."
"VPSLLVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 12 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in zmm2 left by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1."
"VPSLLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1."
"VPSLLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1."
"VPSLLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1."
"VPSLLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in xmm2 left by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1."
"VPSLLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in ymm2 left by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1."
"VPSLLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 47 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in zmm2 left by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1."
"VPSRAVD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits."
"VPSRAVD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits."
"VPSRAVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in sign bits using writemask k1."
"VPSRAVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in sign bits using writemask k1."
"VPSRAVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 11 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in sign bits using writemask k1."
"VPSRAVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in sign bits using writemask k1."
"VPSRAVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in sign bits using writemask k1."
"VPSRAVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in sign bits using writemask k1."
"VPSRAVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 46 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in sign bits using writemask k1."
"VPSRLVD xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSRLVQ xmm1, xmm2, xmm3/m128","VEX.NDS.128.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s."
"VPSRLVD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSRLVQ ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX2","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s."
"VPSRLVW xmm1 {k1}{z}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in xmm2 right by amount specified in the corresponding element of xmm3/m128 while shifting in 0s using writemask k1."
"VPSRLVW ymm1 {k1}{z}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in ymm2 right by amount specified in the corresponding element of ymm3/m256 while shifting in 0s using writemask k1."
"VPSRLVW zmm1 {k1}{z}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 10 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Shift words in zmm2 right by amount specified in the corresponding element of zmm3/m512 while shifting in 0s using writemask k1."
"VPSRLVD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m32bcst while shifting in 0s using writemask k1."
"VPSRLVD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m32bcst while shifting in 0s using writemask k1."
"VPSRLVD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift doublewords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m32bcst while shifting in 0s using writemask k1."
"VPSRLVQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in xmm2 right by amount specified in the corresponding element of xmm3/m128/m64bcst while shifting in 0s using writemask k1."
"VPSRLVQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in ymm2 right by amount specified in the corresponding element of ymm3/m256/m64bcst while shifting in 0s using writemask k1."
"VPSRLVQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 45 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shift quadwords in zmm2 right by amount specified in the corresponding element of zmm3/m512/m64bcst while shifting in 0s using writemask k1."
"VPTERNLOGD xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.DDS.128.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m32bcst as source operands and writing the result to xmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGD ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.DDS.256.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m32bcst as source operands and writing the result to ymm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGD zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.DDS.512.66.0F3A.W0 25 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m32bcst as source operands and writing the result to zmm1 under writemask k1 with dword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.DDS.128.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking xmm1, xmm2 and xmm3/m128/m64bcst as source operands and writing the result to xmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.DDS.256.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking ymm1, ymm2 and ymm3/m256/m64bcst as source operands and writing the result to ymm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTERNLOGQ zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.DDS.512.66.0F3A.W1 25 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (r, w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Bitwise ternary logic taking zmm1, zmm2 and zmm3/m512/m64bcst as source operands and writing the result to zmm1 under writemask k1 with qword granularity. The immediate value determines the specific binary function being implemented."
"VPTESTMB k2 {k1}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMB k2 {k1}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMB k2 {k1}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W0 26 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, xmm2, xmm3/m128","EVEX.NDS.128.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, ymm2, ymm3/m256","EVEX.NDS.256.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512VL AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMW k2 {k1}, zmm2, zmm3/m512","EVEX.NDS.512.66.0F38.W1 26 /r","Valid","Valid","Invalid","AVX512BW","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector Mem","Bitwise AND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMD k2 {k1}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMD k2 {k1}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMD k2 {k1}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.66.0F38.W0 27 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTMQ k2 {k1}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F38.W1 27 /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Bitwise AND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result, under writemask k1."
"VPTESTNMB k2 {k1},xmm2,xmm3/m128",EVEX.NDS.128.F3.0F38.W0 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed byte integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMB k2 {k1},ymm2,ymm3/m256",EVEX.NDS.256.F3.0F38.W0 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed byte integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMB k2 {k1},zmm2,zmm3/m512",EVEX.NDS.512.F3.0F38.W0 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed byte integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1},xmm2,xmm3/m128",EVEX.NDS.128.F3.0F38.W1 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed word integers in xmm2 and xmm3/m128 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1},ymm2,ymm3/m256",EVEX.NDS.256.F3.0F38.W1 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed word integers in ymm2 and ymm3/m256 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMW k2 {k1},zmm2,zmm3/m512",EVEX.NDS.512.F3.0F38.W1 26 /r,Valid,Valid,Invalid,AVX512VL AVX512BW,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector Mem,"Bitwise NAND of packed word integers in zmm2 and zmm3/m512 and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMD k2 {k1},xmm2,xmm3/m128/m32bcst",EVEX.NDS.128.F3.0F38.W0 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed doubleword integers in xmm2 and xmm3/m128/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMD k2 {k1},ymm2,ymm3/m256/m32bcst",EVEX.NDS.256.F3.0F38.W0 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed doubleword integers in ymm2 and ymm3/m256/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMD k2 {k1},zmm2,zmm3/m512/m32bcst",EVEX.NDS.512.F3.0F38.W0 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed doubleword integers in zmm2 and zmm3/m512/m32bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1},xmm2,xmm3/m128/m64bcst",EVEX.NDS.128.F3.0F38.W1 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed quadword integers in xmm2 and xmm3/m128/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1},ymm2,ymm3/m256/m64bcst",EVEX.NDS.256.F3.0F38.W1 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed quadword integers in ymm2 and ymm3/m256/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VPTESTNMQ k2 {k1},zmm2,zmm3/m512/m64bcst",EVEX.NDS.512.F3.0F38.W1 27 /r,Valid,Valid,Invalid,AVX512VL AVX512F,ModRM:reg (w),EVEX.vvvv (r),ModRM:r/m (r),NA,Full Vector,"Bitwise NAND of packed quadword integers in zmm2 and zmm3/m512/m64bcst and set mask k2 to reflect the zero/non-zero status of each element of the result,under writemask k1."
"VRANGEPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst, imm8","EVEX.NDS.128.66.0F3A.W1 50 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate two RANGE operation output value from 2 pairs of double-precision floating-point values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGEPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst, imm8","EVEX.NDS.256.66.0F3A.W1 50 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate four RANGE operation output value from 4pairs of double-precision floating-point values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGEPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{sae}, imm8","EVEX.NDS.512.66.0F3A.W1 50 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate eight RANGE operation output value from 8 pairs of double-precision floating-point values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGEPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst, imm8","EVEX.NDS.128.66.0F3A.W0 50 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate four RANGE operation output value from 4 pairs of single-precision floating-point values in xmm2 and xmm3/m128/m32bcst, store the results to xmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGEPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst, imm8","EVEX.NDS.256.66.0F3A.W0 50 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate eight RANGE operation output value from 8 pairs of single-precision floating-point values in ymm2 and ymm3/m256/m32bcst, store the results to ymm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGEPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{sae}, imm8","EVEX.NDS.512.66.0F3A.W0 50 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Full Vector","Calculate 16 RANGE operation output value from 16 pairs of single-precision floating-point values in zmm2 and zmm3/m512/m32bcst, store the results to zmm1 under the writemask k1. Imm8 specifies the comparison and sign of the range operation."
"VRANGESD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W1 51 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Calculate a RANGE operation output value from 2 double-precision floating-point values in xmm2 and xmm3/m64, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation."
"VRANGESS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W0 51 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Calculate a RANGE operation output value from 2 single-precision floating-point values in xmm2 and xmm3/m32, store the output to xmm1 under writemask. Imm8 specifies the comparison and sign of the range operation."
"VRCP14PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask."
"VRCP14PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask."
"VRCP14PD zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 4C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask."
"VRCP14PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask."
"VRCP14PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask."
"VRCP14PS zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 4C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask."
"VRCP14SD xmm1 {k1}{z}, xmm2, xmm3/m64","EVEX.NDS.LIG.66.0F38.W1 4D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal of the scalar double-precision floating-point value in xmm3/m64 and stores the result in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VRCP14SS xmm1 {k1}{z}, xmm2, xmm3/m32","EVEX.NDS.LIG.66.0F38.W0 4D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal of the scalar single-precision floating-point value in xmm3/m32 and stores the results in xmm1 using writemask k1. Also, upper double-precision floating-point value (bits[127:32]) from xmm2 is copied to xmm1[127:32]."
"VRCP28PD zmm1 {k1}{z}, zmm2/m512/m64bcst {sae}","EVEX.512.66.0F38.W1 CA /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals ( < 2^-28 relative error) of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1. Under writemask."
"VRCP28PS zmm1 {k1}{z}, zmm2/m512/m32bcst {sae}","EVEX.512.66.0F38.W0 CA /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocals ( < 2^-28 relative error) of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask."
"VRCP28SD xmm1 {k1}{z}, xmm2, xmm3/m64 {sae}","EVEX.NDS.LIG.66.0F38.W1 CB /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal ( < 2^-28 relative error) of the scalar double-precision floating-point value in xmm3/m64 and stores the results in xmm1. Under writemask. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VRCP28SS xmm1 {k1}{z}, xmm2, xmm3/m32 {sae}","EVEX.NDS.LIG.66.0F38.W0 CB /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal ( < 2^-28 relative error) of the scalar single-precision floating-point value in xmm3/m32 and stores the results in xmm1. Under writemask. Also, upper 3 single-precision floating-point values (bits[127:32]) from xmm2 is copied to xmm1[127:32]."
"VREDUCEPD xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.128.66.0F3A.W1 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on packed double-precision floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1."
"VREDUCEPD ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on packed double-precision floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1."
"VREDUCEPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}, imm8","EVEX.512.66.0F3A.W1 56 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on double-precision floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1."
"VREDUCEPS xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on packed single-precision floating point values in xmm2/m128/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask k1."
"VREDUCEPS ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on packed single-precision floating point values in ymm2/m256/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register under writemask k1."
"VREDUCEPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}, imm8","EVEX.512.66.0F3A.W0 56 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Perform reduction transformation on packed single-precision floating point values in zmm2/m512/m32bcst by subtracting a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register under writemask k1."
"VREDUCESD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, imm8/r","EVEX.NDS.LIG.66.0F3A.W1 57","Valid","Valid","Invalid","AVX512D","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Perform a reduction transformation on a scalar double-precision floating point value in xmm3/m64 by subtracting a number of fraction bits specified by the imm8 field. Also, upper double precision floating-point value (bits[127:64]) from xmm2 are copied to xmm1[127:64]. Stores the result in xmm1 register."
"VREDUCESS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W0 57 /r ib","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Perform a reduction transformation on a scalar single-precision floating point value in xmm3/m32 by subtracting a number of fraction bits specified by the imm8 field. Also, upper single precision floating-point values (bits[127:32]) from xmm2 are copied to xmm1[127:32]. Stores the result in xmm1 register."
"VRNDSCALEPD xmm1 {k1}{z}, xmm2/m128/m64bcst, imm8","EVEX.128.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed double-precision floating point values in xmm2/m128/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask."
"VRNDSCALEPD ymm1 {k1}{z}, ymm2/m256/m64bcst, imm8","EVEX.256.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed double-precision floating point values in ymm2/m256/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask."
"VRNDSCALEPD zmm1 {k1}{z}, zmm2/m512/m64bcst{sae}, imm8","EVEX.512.66.0F3A.W1 09 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed double-precision floating-point values in zmm2/m512/m64bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask k1."
"VRNDSCALEPS xmm1 {k1}{z}, xmm2/m128/m32bcst, imm8","EVEX.128.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed single-precision floating point values in xmm2/m128/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register. Under writemask."
"VRNDSCALEPS ymm1 {k1}{z}, ymm2/m256/m32bcst, imm8","EVEX.256.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed single-precision floating point values in ymm2/m256/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in ymm1 register. Under writemask."
"VRNDSCALEPS zmm1 {k1}{z}, zmm2/m512/m32bcst{sae}, imm8","EVEX.512.66.0F3A.W0 08 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","Imm8","NA","Full Vector","Rounds packed single-precision floating-point values in zmm2/m512/m32bcst to a number of fraction bits specified by the imm8 field. Stores the result in zmm1 register using writemask."
"VRNDSCALESD xmm1 {k1}{z}, xmm2, xmm3/m64{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W1 0B /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","Imm8","Tuple1 Scalar","Rounds scalar double-precision floating-point value in xmm3/m64 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register."
"VRNDSCALESS xmm1 {k1}{z}, xmm2, xmm3/m32{sae}, imm8","EVEX.NDS.LIG.66.0F3A.W0 0A /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Rounds scalar single-precision floating-point value in xmm3/m32 to a number of fraction bits specified by the imm8 field. Stores the result in xmm1 register under writemask."
"VRSQRT14PD xmm1 {k1}{z}, xmm2/m128/m64bcst","EVEX.128.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in xmm2/m128/m64bcst and stores the results in xmm1. Under writemask."
"VRSQRT14PD ymm1 {k1}{z}, ymm2/m256/m64bcst","EVEX.256.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in ymm2/m256/m64bcst and stores the results in ymm1. Under writemask."
"VRSQRT14PD zmm1 {k1}{z}, zmm2/m512/m64bcst","EVEX.512.66.0F38.W1 4E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed double-precision floating-point values in zmm2/m512/m64bcst and stores the results in zmm1 under writemask."
"VRSQRT14PS xmm1 {k1}{z}, xmm2/m128/m32bcst","EVEX.128.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in xmm2/m128/m32bcst and stores the results in xmm1. Under writemask."
"VRSQRT14PS ymm1 {k1}{z}, ymm2/m256/m32bcst","EVEX.256.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in ymm2/m256/m32bcst and stores the results in ymm1. Under writemask."
"VRSQRT14PS zmm1 {k1}{z}, zmm2/m512/m32bcst","EVEX.512.66.0F38.W0 4E /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes the approximate reciprocal square roots of the packed single-precision floating-point values in zmm2/m512/m32bcst and stores the results in zmm1. Under writemask."
"VRSQRT14SD xmm1 {k1}{z}, xmm2, xmm3/m64","EVEX.NDS.LIG.66.0F38.W1 4F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal square root of the scalar double-precision floating-point value in xmm3/m64 and stores the result in the low quadword element of xmm1 using writemask k1. Bits[127:64] of xmm2 is copied to xmm1[127:64]."
"VRSQRT14SS xmm1 {k1}{z}, xmm2, xmm3/m32","EVEX.NDS.LIG.66.0F38.W0 4F /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes the approximate reciprocal square root of the scalar single-precision floating-point value in xmm3/m32 and stores the result in the low doubleword element of xmm1 using writemask k1. Bits[127:32] of xmm2 is copied to xmm1[127:32]."
"VRSQRT28PD zmm1 {k1}{z}, zmm2/m512/m64bcst {sae}","EVEX.512.66.0F38.W1 CC /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes approximations to the Reciprocal square root (<2^-28 relative error) of the packed double-precision floating-point values from zmm2/m512/m64bcst and stores result in zmm1with writemask k1."
"VRSQRT28PS zmm1 {k1}{z}, zmm2/m512/m32bcst {sae}","EVEX.512.66.0F38.W0 CC /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","Full Vector","Computes approximations to the Reciprocal square root (<2^-28 relative error) of the packed single-precision floating-point values from zmm2/m512/m32bcst and stores result in zmm1with writemask k1."
"VRSQRT28SD xmm1 {k1}{z}, xmm2, xmm3/m64 {sae}","EVEX.NDS.LIG.66.0F38.W1 CD /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes approximate reciprocal square root (<2^-28 relative error) of the scalar double-precision floating-point value from xmm3/m64 and stores result in xmm1with writemask k1. Also, upper double-precision floating-point value (bits[127:64]) from xmm2 is copied to xmm1[127:64]."
"VRSQRT28SS xmm1 {k1}{z}, xmm2, xmm3/m32 {sae}","EVEX.NDS.LIG.66.0F38.W0 CD /r","Valid","Valid","Invalid","AVX512ER","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Computes approximate reciprocal square root (<2^-28 relative error) of the scalar single-precision floating-point value from xmm3/m32 and stores result in xmm1with writemask k1. Also, upper 3 single-precision floating-point value (bits[127:32]) from xmm2 is copied to xmm1[127:32]."
"VSCALEFPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed double-precision floating-point values in xmm2 using values from xmm3/m128/m64bcst. Under writemask k1."
"VSCALEFPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed double-precision floating-point values in ymm2 using values from ymm3/m256/m64bcst. Under writemask k1."
"VSCALEFPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst{er}","EVEX.NDS.512.66.0F38.W1 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed double-precision floating-point values in zmm2 using values from zmm3/m512/m64bcst. Under writemask k1."
"VSCALEFPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed single-precision floating-point values in xmm2 using values from xmm3/m128/m32bcst. Under writemask k1."
"VSCALEFPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512VL AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed single-precision values in ymm2 using floating point values from ymm3/m256/m32bcst. Under writemask k1."
"VSCALEFPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst{er}","EVEX.NDS.512.66.0F38.W0 2C /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Scale the packed single-precision floating-point values in zmm2 using floating-point values from zmm3/m512/m32bcst. Under writemask k1."
"VSCALEFSD xmm1 {k1}{z}, xmm2, xmm3/m64{er}","EVEX.NDS.LIG.66.0F38.W1 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Scale the scalar double-precision floating-point values in xmm2 using the value from xmm3/m64. Under writemask k1."
"VSCALEFSS xmm1 {k1}{z}, xmm2, xmm3/m32{er}","EVEX.NDS.LIG.66.0F38.W0 2D /r","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Tuple1 Scalar","Scale the scalar single-precision floating-point value in xmm2 using floating-point value from xmm3/m32. Under writemask k1."
"VSCATTERDPS vm32x {k1}, xmm1","EVEX.128.66.0F38.W0 A2 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERDPS vm32y {k1}, ymm1","EVEX.256.66.0F38.W0 A2 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERDPS vm32z {k1}, zmm1","EVEX.512.66.0F38.W0 A2 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed dword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERDPD vm32x {k1}, xmm1","EVEX.128.66.0F38.W1 A2 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERDPD vm32x {k1}, ymm1","EVEX.256.66.0F38.W1 A2 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERDPD vm32y {k1}, zmm1","EVEX.512.66.0F38.W1 A2 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed dword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64x {k1}, xmm1","EVEX.128.66.0F38.W0 A3 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64y {k1}, xmm1","EVEX.256.66.0F38.W0 A3 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPS vm64z {k1}, ymm1","EVEX.512.66.0F38.W0 A3 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed qword indices, scatter single-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64x {k1}, xmm1","EVEX.128.66.0F38.W1 A3 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64y {k1}, ymm1","EVEX.256.66.0F38.W1 A3 /vsib","Valid","Valid","Invalid","AVX512VL AVX512F","","","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERQPD vm64z {k1}, zmm1","EVEX.512.66.0F38.W1 A3 /vsib","Valid","Valid","Invalid","AVX512F","","","","","","Using signed qword indices, scatter double-precision floating-point values to memory using writemask k1."
"VSCATTERPF0DPS vm32z {k1}","EVEX.512.66.0F38.W0 C6 /5 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T0 hint with intent to write."
"VSCATTERPF0QPS vm64z {k1}","EVEX.512.66.0F38.W0 C7 /5 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T0 hint with intent to write."
"VSCATTERPF0DPD vm32y {k1}","EVEX.512.66.0F38.W1 C6 /5 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T0 hint with intent to write."
"VSCATTERPF0QPD vm64z {k1}","EVEX.512.66.0F38.W1 C7 /5 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T0 hint with intent to write."
"VSCATTERPF1DPS vm32z {k1}","EVEX.512.66.0F38.W0 C6 /6 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T1 hint with intent to write."
"VSCATTERPF1QPS vm64z {k1}","EVEX.512.66.0F38.W0 C7 /6 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing single-precision data using writemask k1 and T1 hint with intent to write."
"VSCATTERPF1DPD vm32y {k1}","EVEX.512.66.0F38.W1 C6 /6 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed dword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T1 hint with intent to write."
"VSCATTERPF1QPD vm64z {k1}","EVEX.512.66.0F38.W1 C7 /6 /vsib","Valid","Valid","Invalid","AVX512PF","BaseReg (R): VSIB:base, VectorReg(R): VSIB:index","NA","NA","NA","Tuple1 Scalar","Using signed qword indices, prefetch sparse byte memory locations containing double-precision data using writemask k1 and T1 hint with intent to write."
"VSHUFF32x4 zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.66.0F3A.W0 23 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shuffle 128-bit packed single-precision floating-point values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1. EVEX.NDS.256.66.0F3A.W1 23 /r ib VSHUFF64X2 ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, imm8 A V/V AVX512VL AVX512F Shuffle 128-bit packed double-precision floating-point values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1."
"VSHUFF64x2 zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F3A.W1 23 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shuffle 128-bit packed double-precision floating-point values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1. EVEX.NDS.256.66.0F3A.W0 43 /r ib VSHUFI32X4 ymm1{k1}{z}, ymm2, ymm3/m256/m32bcst, imm8 A V/V AVX512VL AVX512F Shuffle 128-bit packed double-word values selected by imm8 from ymm2 and ymm3/m256/m32bcst and place results in ymm1 subject to writemask k1."
"VSHUFI32x4 zmm1{k1}{z}, zmm2, zmm3/m512/m32bcst, imm8","EVEX.NDS.512.66.0F3A.W0 43 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shuffle 128-bit packed double-word values selected by imm8 from zmm2 and zmm3/m512/m32bcst and place results in zmm1 subject to writemask k1. EVEX.NDS.256.66.0F3A.W1 43 /r ib VSHUFI64X2 ymm1{k1}{z}, ymm2, ymm3/m256/m64bcst, imm8 A V/V AVX512VL AVX512F Shuffle 128-bit packed quad-word values selected by imm8 from ymm2 and ymm3/m256/m64bcst and place results in ymm1 subject to writemask k1."
"VSHUFI64x2 zmm1{k1}{z}, zmm2, zmm3/m512/m64bcst, imm8","EVEX.NDS.512.66.0F3A.W1 43 /r ib","Valid","Valid","Invalid","AVX512F","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Shuffle 128-bit packed quad-word values selected by imm8 from zmm2 and zmm3/m512/m64bcst and place results in zmm1 subject to writemask k1."
"VTESTPS xmm1, xmm2/m128","VEX.128.66.0F38.W0 0E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources."
"VTESTPS ymm1, ymm2/m256","VEX.256.66.0F38.W0 0E /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on sign bit AND and ANDN of packed single-precision floating-point sources."
"VTESTPD xmm1, xmm2/m128","VEX.128.66.0F38.W0 0F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources."
"VTESTPD ymm1, ymm2/m256","VEX.256.66.0F38.W0 0F /r","Valid","Valid","Invalid","AVX","ModRM:reg (r)","ModRM:r/m (r)","NA","NA","","Set ZF and CF depending on sign bit AND and ANDN of packed double-precision floating-point sources."
"VZEROALL","VEX.256.0F.WIG 77","Valid","Valid","Invalid","AVX","NA","NA","NA","NA","","Zero all YMM registers."
"VZEROUPPER","VEX.128.0F.WIG 77","Valid","Valid","Invalid","AVX","NA","NA","NA","NA","","Zero upper 128 bits of all YMM registers."
"WAIT","9B","Valid","Valid","Valid","","NA","NA","NA","NA","","Check pending unmasked floating-point exceptions."
"FWAIT","9B","Valid","Valid","Valid","","NA","NA","NA","NA","","Check pending unmasked floating-point exceptions."
"WBINVD","0F 09","Valid","Valid","Valid","","NA","NA","NA","NA","","Write back and flush Internal caches; initiate writing-back and flushing of external caches."
"WRFSBASE r32","F3 0F AE /2","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (r)","NA","NA","NA","","Load the FS base address with the 32-bit value in the source register."
"WRFSBASE r64","F3 REX.W 0F AE /2","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (r)","NA","NA","NA","","Load the FS base address with the 64-bit value in the source register."
"WRGSBASE r32","F3 0F AE /3","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (r)","NA","NA","NA","","Load the GS base address with the 32-bit value in the source register."
"WRGSBASE r64","F3 REX.W 0F AE /3","Valid","Invalid","Invalid","FSGSBASE","ModRM:r/m (r)","NA","NA","NA","","Load the GS base address with the 64-bit value in the source register."
"WRMSR","0F 30","Valid","Valid","Valid","","NA","NA","NA","NA","","Write the value in EDX:EAX to MSR specified by ECX."
"WRPKRU","NP 0F 01 EF","Valid","Valid","Valid","OSPKE","NA","NA","NA","NA","","Writes EAX into PKRU."
"XABORT imm8","C6 F8 ib","Valid","Valid","Invalid","RTM","","","","","","Causes an RTM abort if in RTM execution"
XACQUIRE,F2,Valid,Valid,Invalid,HLE,,,,,,A hint used with an “XACQUIRE-enabled“ instruction to start lock elision on the instruction memory operand address.
XRELEASE,F3,Valid,Valid,Invalid,HLE,,,,,,A hint used with an “XRELEASE-enabled“ instruction to end lock elision on the instruction memory operand address.
"XADD r/m8, r8","0F C0 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r, w)","NA","NA","","Exchange r8 and r/m8; load sum into r/m8."
"XADD r/m8, r8","REX + 0F C0 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r, w)","NA","NA","","Exchange r8 and r/m8; load sum into r/m8."
"XADD r/m16, r16","0F C1 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r, w)","NA","NA","","Exchange r16 and r/m16; load sum into r/m16."
"XADD r/m32, r32","0F C1 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r, w)","NA","NA","","Exchange r32 and r/m32; load sum into r/m32."
"XADD r/m64, r64","REX.W + 0F C1 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r, w)","NA","NA","","Exchange r64 and r/m64; load sum into r/m64."
"XBEGIN rel16","C7 F8","Valid","Valid","Invalid","RTM","","","","","","Specifies the start of an RTM region. Provides a 16-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort."
"XBEGIN rel32","C7 F8","Valid","Valid","Invalid","RTM","","","","","","Specifies the start of an RTM region. Provides a 32-bit relative offset to compute the address of the fallback instruction address at which execution resumes following an RTM abort."
"XCHG AX, r16","90+rw","Valid","Valid","Valid","","AX/EAX/RAX (r, w)","opcode +rd (r, w)","NA","NA","","Exchange r16 with AX."
"XCHG r16, AX","90+rw","Valid","Valid","Valid","","opcode +rd (r, w)","AX/EAX/RAX (r, w)","NA","NA","","Exchange AX with r16."
"XCHG EAX, r32","90+rd","Valid","Valid","Valid","","AX/EAX/RAX (r, w)","opcode +rd (r, w)","NA","NA","","Exchange r32 with EAX."
"XCHG RAX, r64","REX.W + 90+rd","Valid","Invalid","Invalid","","AX/EAX/RAX (r, w)","opcode +rd (r, w)","NA","NA","","Exchange r64 with RAX."
"XCHG r32, EAX","90+rd","Valid","Valid","Valid","","opcode +rd (r, w)","AX/EAX/RAX (r, w)","NA","NA","","Exchange EAX with r32."
"XCHG r64, RAX","REX.W + 90+rd","Valid","Invalid","Invalid","","opcode +rd (r, w)","AX/EAX/RAX (r, w)","NA","NA","","Exchange RAX with r64."
"XCHG r/m8, r8","86 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Exchange r8 (byte register) with byte from r/m8."
"XCHG r/m8, r8","REX + 86 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Exchange r8 (byte register) with byte from r/m8."
"XCHG r8, r/m8","86 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Exchange byte from r/m8 with r8 (byte register)."
"XCHG r8, r/m8","REX + 86 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Exchange byte from r/m8 with r8 (byte register)."
"XCHG r/m16, r16","87 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Exchange r16 with word from r/m16."
"XCHG r16, r/m16","87 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Exchange word from r/m16 with r16."
"XCHG r/m32, r32","87 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Exchange r32 with doubleword from r/m32."
"XCHG r/m64, r64","REX.W + 87 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","Exchange r64 with quadword from r/m64."
"XCHG r32, r/m32","87 /r","Valid","Valid","Valid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Exchange doubleword from r/m32 with r32."
"XCHG r64, r/m64","REX.W + 87 /r","Valid","Invalid","Invalid","","ModRM:reg (w)","ModRM:r/m (r)","NA","NA","","Exchange quadword from r/m64 with r64."
"XEND","NP 0F 01 D5","Valid","Valid","Invalid","RTM","NA","NA","NA","NA","","Specifies the end of an RTM code region."
"XGETBV","NP 0F 01 D0","Valid","Valid","Valid","","NA","NA","NA","NA","","Reads an XCR specified by ECX into EDX:EAX."
"XLAT m8","D7","Valid","Valid","Valid","","NA","NA","NA","NA","","Set AL to memory byte DS:[(E)BX + unsigned AL]."
"XLATB","D7","Valid","Valid","Valid","","NA","NA","NA","NA","","Set AL to memory byte DS:[(E)BX + unsigned AL]."
"XLATB","REX.W + D7","Valid","Invalid","Invalid","","NA","NA","NA","NA","","Set AL to memory byte [RBX + unsigned AL]."
"XOR AL, imm8","34 ib","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AL XOR imm8."
"XOR AX, imm16","35 iw","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","AX XOR imm16."
"XOR EAX, imm32","35 id","Valid","Valid","Valid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","EAX XOR imm32."
"XOR RAX, imm32","REX.W + 35 id","Valid","Invalid","Invalid","","AL/AX/EAX/RAX","imm8/16/32","NA","NA","","RAX XOR imm32 (sign-extended)."
"XOR r/m8, imm8","80 /6 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m8 XOR imm8."
"XOR r/m8, imm8","REX + 80 /6 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m8 XOR imm8."
"XOR r/m16, imm16","81 /6 iw","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m16 XOR imm16."
"XOR r/m32, imm32","81 /6 id","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m32 XOR imm32."
"XOR r/m64, imm32","REX.W + 81 /6 id","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m64 XOR imm32 (sign-extended)."
"XOR r/m16, imm8","83 /6 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m16 XOR imm8 (sign-extended)."
"XOR r/m32, imm8","83 /6 ib","Valid","Valid","Valid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m32 XOR imm8 (sign-extended)."
"XOR r/m64, imm8","REX.W + 83 /6 ib","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","imm8/16/32","NA","NA","","r/m64 XOR imm8 (sign-extended)."
"XOR r/m8, r8","30 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m8 XOR r8."
"XOR r/m8, r8","REX + 30 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m8 XOR r8."
"XOR r/m16, r16","31 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m16 XOR r16."
"XOR r/m32, r32","31 /r","Valid","Valid","Valid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m32 XOR r32."
"XOR r/m64, r64","REX.W + 31 /r","Valid","Invalid","Invalid","","ModRM:r/m (r, w)","ModRM:reg (r)","NA","NA","","r/m64 XOR r64."
"XOR r8, r/m8","32 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r8 XOR r/m8."
"XOR r8, r/m8","REX + 32 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r8 XOR r/m8."
"XOR r16, r/m16","33 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r16 XOR r/m16."
"XOR r32, r/m32","33 /r","Valid","Valid","Valid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r32 XOR r/m32."
"XOR r64, r/m64","REX.W + 33 /r","Valid","Invalid","Invalid","","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","","r64 XOR r/m64."
"XORPD xmm1, xmm2/m128","66 0F 57/r","Valid","Valid","Invalid","SSE2","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical XOR of packed double-precision floating-point values in xmm1 and xmm2/mem."
"VXORPD xmm1,xmm2, xmm3/m128","VEX.NDS.128.66.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/mem."
"VXORPD ymm1, ymm2, ymm3/m256","VEX.NDS.256.66.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv (r)","ModRM:r/m (r)","NA","NA","Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/mem."
"VXORPD xmm1 {k1}{z}, xmm2, xmm3/m128/m64bcst","EVEX.NDS.128.66.0F.W1 57 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed double-precision floating-point values in xmm2 and xmm3/m128/m64bcst subject to writemask k1."
"VXORPD ymm1 {k1}{z}, ymm2, ymm3/m256/m64bcst","EVEX.NDS.256.66.0F.W1 57 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed double-precision floating-point values in ymm2 and ymm3/m256/m64bcst subject to writemask k1."
"VXORPD zmm1 {k1}{z}, zmm2, zmm3/m512/m64bcst","EVEX.NDS.512.66.0F.W1 57 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv (r)","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed double-precision floating-point values in zmm2 and zmm3/m512/m64bcst subject to writemask k1."
"XORPS xmm1, xmm2/m128","NP 0F 57 /r","Valid","Valid","Invalid","SSE","ModRM:reg (r, w)","ModRM:r/m (r)","NA","NA","NA","Return the bitwise logical XOR of packed single-precision floating-point values in xmm1 and xmm2/mem."
"VXORPS xmm1,xmm2, xmm3/m128","VEX.NDS.128.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/mem."
"VXORPS ymm1, ymm2, ymm3/m256","VEX.NDS.256.0F.WIG 57 /r","Valid","Valid","Invalid","AVX","ModRM:reg (w)","VEX.vvvv","ModRM:r/m (r)","NA","NA","Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/mem."
"VXORPS xmm1 {k1}{z}, xmm2, xmm3/m128/m32bcst","EVEX.NDS.128.0F.W0 57 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed single-precision floating-point values in xmm2 and xmm3/m128/m32bcst subject to writemask k1."
"VXORPS ymm1 {k1}{z}, ymm2, ymm3/m256/m32bcst","EVEX.NDS.256.0F.W0 57 /r","Valid","Valid","Invalid","AVX512VL AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed single-precision floating-point values in ymm2 and ymm3/m256/m32bcst subject to writemask k1."
"VXORPS zmm1 {k1}{z}, zmm2, zmm3/m512/m32bcst","EVEX.NDS.512.0F.W0 57 /r","Valid","Valid","Invalid","AVX512DQ","ModRM:reg (w)","EVEX.vvvv","ModRM:r/m (r)","NA","Full Vector","Return the bitwise logical XOR of packed single-precision floating-point values in zmm2 and zmm3/m512/m32bcst subject to writemask k1."
"XRSTOR mem","NP 0F AE /5","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Restore state components specified by EDX:EAX from mem."
"XRSTOR64 mem","NP REX.W + 0F AE /5","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Restore state components specified by EDX:EAX from mem."
"XRSTORS mem","NP 0F C7 /3","Valid","Valid","Valid","","ModRM:r/m (r)","NA","NA","NA","","Restore state components specified by EDX:EAX from mem."
"XRSTORS64 mem","NP REX.W + 0F C7 /3","Valid","Invalid","Invalid","","ModRM:r/m (r)","NA","NA","NA","","Restore state components specified by EDX:EAX from mem."
"XSAVE mem","NP 0F AE /4","Valid","Valid","Valid","","ModRM:r/m (w)","NA","NA","NA","","Save state components specified by EDX:EAX to mem."
"XSAVE64 mem","NP REX.W + 0F AE /4","Valid","Invalid","Invalid","","ModRM:r/m (w)","NA","NA","NA","","Save state components specified by EDX:EAX to mem."
"XSAVEC mem","NP 0F C7 /4","Valid","Valid","Valid","","","","","","","Save state components specified by EDX:EAX to mem with compaction."
"XSAVEC64 mem","NP REX.W + 0F C7 /4","Valid","Invalid","Invalid","","","","","","","Save state components specified by EDX:EAX to mem with compaction."
"XSAVEOPT mem","NP 0F AE /6","Valid","Valid","Invalid","XSAVEOPT","ModRM:r/m (w)","NA","NA","NA","","Save state components specified by EDX:EAX to mem, optimizing if possible."
"XSAVEOPT64 mem","NP REX.W + 0F AE /6","Valid","Valid","Invalid","XSAVEOPT","ModRM:r/m (w)","NA","NA","NA","","Save state components specified by EDX:EAX to mem, optimizing if possible."
"XSAVES mem","NP 0F C7 /5","Valid","Valid","Valid","","","","","","","Save state components specified by EDX:EAX to mem with compaction, optimizing if possible."
"XSAVES64 mem","NP REX.W + 0F C7 /5","Valid","Invalid","Invalid","","","","","","","Save state components specified by EDX:EAX to mem with compaction, optimizing if possible."
"XSETBV","NP 0F 01 D1","Valid","Valid","Valid","","NA","NA","NA","NA","","Write the value in EDX:EAX to the XCR specified by ECX."
"XTEST","NP 0F 01 D6","Valid","Valid","Invalid","HLE RTM","NA","NA","NA","NA","","Test if executing in a transactional region"
